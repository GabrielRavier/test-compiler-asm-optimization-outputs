//
// Generated by LLVM NVPTX Back-End
//

.version 4.2
.target sm_20
.address_size 32

	// .globl	memmove
.global .align 1 .b8 l64a_$_s[7];
.global .align 1 .b8 digits[65] = {46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122};
.global .align 8 .u64 seed;

.visible .func  (.param .b32 func_retval0) memmove(
	.param .b32 memmove_param_0,
	.param .b32 memmove_param_1,
	.param .b32 memmove_param_2
)
{
	.local .align 4 .b8 	__local_depot0[20];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<31>;

	mov.u32 	%SPL, __local_depot0;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [memmove_param_2];
	ld.param.u32 	%r2, [memmove_param_1];
	ld.param.u32 	%r1, [memmove_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+0];
	st.u32 	[%SP+12], %r4;
	ld.u32 	%r5, [%SP+4];
	st.u32 	[%SP+16], %r5;
	ld.u32 	%r6, [%SP+16];
	ld.u32 	%r7, [%SP+12];
	setp.ge.u32 	%p1, %r6, %r7;
	@%p1 bra 	$L__BB0_6;
	bra.uni 	$L__BB0_1;
$L__BB0_1:
	ld.u32 	%r17, [%SP+8];
	ld.u32 	%r18, [%SP+16];
	add.s32 	%r19, %r18, %r17;
	st.u32 	[%SP+16], %r19;
	ld.u32 	%r20, [%SP+8];
	ld.u32 	%r21, [%SP+12];
	add.s32 	%r22, %r21, %r20;
	st.u32 	[%SP+12], %r22;
	bra.uni 	$L__BB0_2;
$L__BB0_2:
	ld.u32 	%r23, [%SP+8];
	setp.eq.s32 	%p4, %r23, 0;
	@%p4 bra 	$L__BB0_5;
	bra.uni 	$L__BB0_3;
$L__BB0_3:
	ld.u32 	%r25, [%SP+16];
	add.s32 	%r26, %r25, -1;
	st.u32 	[%SP+16], %r26;
	ld.u8 	%rs2, [%r25+-1];
	ld.u32 	%r27, [%SP+12];
	add.s32 	%r28, %r27, -1;
	st.u32 	[%SP+12], %r28;
	st.u8 	[%r27+-1], %rs2;
	bra.uni 	$L__BB0_4;
$L__BB0_4:
	ld.u32 	%r29, [%SP+8];
	add.s32 	%r30, %r29, -1;
	st.u32 	[%SP+8], %r30;
	bra.uni 	$L__BB0_2;
$L__BB0_5:
	bra.uni 	$L__BB0_13;
$L__BB0_6:
	ld.u32 	%r8, [%SP+16];
	ld.u32 	%r9, [%SP+12];
	setp.eq.s32 	%p2, %r8, %r9;
	@%p2 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_7;
$L__BB0_7:
	bra.uni 	$L__BB0_8;
$L__BB0_8:
	ld.u32 	%r10, [%SP+8];
	setp.eq.s32 	%p3, %r10, 0;
	@%p3 bra 	$L__BB0_11;
	bra.uni 	$L__BB0_9;
$L__BB0_9:
	ld.u32 	%r11, [%SP+16];
	add.s32 	%r12, %r11, 1;
	st.u32 	[%SP+16], %r12;
	ld.u8 	%rs1, [%r11];
	ld.u32 	%r13, [%SP+12];
	add.s32 	%r14, %r13, 1;
	st.u32 	[%SP+12], %r14;
	st.u8 	[%r13], %rs1;
	bra.uni 	$L__BB0_10;
$L__BB0_10:
	ld.u32 	%r15, [%SP+8];
	add.s32 	%r16, %r15, -1;
	st.u32 	[%SP+8], %r16;
	bra.uni 	$L__BB0_8;
$L__BB0_11:
	bra.uni 	$L__BB0_12;
$L__BB0_12:
	bra.uni 	$L__BB0_13;
$L__BB0_13:
	ld.u32 	%r24, [%SP+0];
	st.param.b32 	[func_retval0+0], %r24;
	ret;

}
	// .globl	memccpy
.visible .func  (.param .b32 func_retval0) memccpy(
	.param .b32 memccpy_param_0,
	.param .b32 memccpy_param_1,
	.param .b32 memccpy_param_2,
	.param .b32 memccpy_param_3
)
{
	.local .align 4 .b8 	__local_depot1[28];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<25>;

	mov.u32 	%SPL, __local_depot1;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r4, [memccpy_param_3];
	ld.param.u32 	%r3, [memccpy_param_2];
	ld.param.u32 	%r2, [memccpy_param_1];
	ld.param.u32 	%r1, [memccpy_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u32 	[%SP+12], %r3;
	st.u32 	[%SP+16], %r4;
	ld.u32 	%r5, [%SP+4];
	st.u32 	[%SP+20], %r5;
	ld.u32 	%r6, [%SP+8];
	st.u32 	[%SP+24], %r6;
	ld.u8 	%r7, [%SP+12];
	st.u32 	[%SP+12], %r7;
	bra.uni 	$L__BB1_1;
$L__BB1_1:
	ld.u32 	%r8, [%SP+16];
	setp.eq.s32 	%p4, %r8, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB1_3;
	bra.uni 	$L__BB1_2;
$L__BB1_2:
	ld.u32 	%r9, [%SP+24];
	ld.u8 	%rs1, [%r9];
	ld.u32 	%r10, [%SP+20];
	st.u8 	[%r10], %rs1;
	cvt.u32.u16 	%r11, %rs1;
	and.b32  	%r12, %r11, 255;
	ld.u32 	%r13, [%SP+12];
	setp.ne.s32 	%p1, %r12, %r13;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB1_3;
$L__BB1_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB1_6;
	bra.uni 	$L__BB1_4;
$L__BB1_4:
	bra.uni 	$L__BB1_5;
$L__BB1_5:
	ld.u32 	%r19, [%SP+16];
	add.s32 	%r20, %r19, -1;
	st.u32 	[%SP+16], %r20;
	ld.u32 	%r21, [%SP+24];
	add.s32 	%r22, %r21, 1;
	st.u32 	[%SP+24], %r22;
	ld.u32 	%r23, [%SP+20];
	add.s32 	%r24, %r23, 1;
	st.u32 	[%SP+20], %r24;
	bra.uni 	$L__BB1_1;
$L__BB1_6:
	ld.u32 	%r14, [%SP+16];
	setp.eq.s32 	%p5, %r14, 0;
	@%p5 bra 	$L__BB1_8;
	bra.uni 	$L__BB1_7;
$L__BB1_7:
	ld.u32 	%r16, [%SP+20];
	add.s32 	%r17, %r16, 1;
	st.u32 	[%SP+0], %r17;
	bra.uni 	$L__BB1_9;
$L__BB1_8:
	mov.b32 	%r15, 0;
	st.u32 	[%SP+0], %r15;
	bra.uni 	$L__BB1_9;
$L__BB1_9:
	ld.u32 	%r18, [%SP+0];
	st.param.b32 	[func_retval0+0], %r18;
	ret;

}
	// .globl	memchr
.visible .func  (.param .b32 func_retval0) memchr(
	.param .b32 memchr_param_0,
	.param .b32 memchr_param_1,
	.param .b32 memchr_param_2
)
{
	.local .align 4 .b8 	__local_depot2[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<19>;

	mov.u32 	%SPL, __local_depot2;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r5, [memchr_param_2];
	ld.param.u32 	%r4, [memchr_param_1];
	ld.param.u32 	%r3, [memchr_param_0];
	st.u32 	[%SP+0], %r3;
	st.u32 	[%SP+4], %r4;
	st.u32 	[%SP+8], %r5;
	ld.u32 	%r6, [%SP+0];
	st.u32 	[%SP+12], %r6;
	ld.u8 	%r7, [%SP+4];
	st.u32 	[%SP+4], %r7;
	bra.uni 	$L__BB2_1;
$L__BB2_1:
	ld.u32 	%r8, [%SP+8];
	setp.eq.s32 	%p4, %r8, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB2_3;
	bra.uni 	$L__BB2_2;
$L__BB2_2:
	ld.u32 	%r9, [%SP+12];
	ld.u8 	%r10, [%r9];
	ld.u32 	%r11, [%SP+4];
	setp.ne.s32 	%p1, %r10, %r11;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB2_3;
$L__BB2_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB2_6;
	bra.uni 	$L__BB2_4;
$L__BB2_4:
	bra.uni 	$L__BB2_5;
$L__BB2_5:
	ld.u32 	%r14, [%SP+12];
	add.s32 	%r15, %r14, 1;
	st.u32 	[%SP+12], %r15;
	ld.u32 	%r16, [%SP+8];
	add.s32 	%r17, %r16, -1;
	st.u32 	[%SP+8], %r17;
	bra.uni 	$L__BB2_1;
$L__BB2_6:
	ld.u32 	%r12, [%SP+8];
	setp.eq.s32 	%p5, %r12, 0;
	@%p5 bra 	$L__BB2_8;
	bra.uni 	$L__BB2_7;
$L__BB2_7:
	ld.u32 	%r1, [%SP+12];
	mov.u32 	%r18, %r1;
	bra.uni 	$L__BB2_9;
$L__BB2_8:
	mov.b32 	%r13, 0;
	mov.u32 	%r18, %r13;
	bra.uni 	$L__BB2_9;
$L__BB2_9:
	mov.u32 	%r2, %r18;
	st.param.b32 	[func_retval0+0], %r2;
	ret;

}
	// .globl	memcmp
.visible .func  (.param .b32 func_retval0) memcmp(
	.param .b32 memcmp_param_0,
	.param .b32 memcmp_param_1,
	.param .b32 memcmp_param_2
)
{
	.local .align 4 .b8 	__local_depot3[20];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<26>;

	mov.u32 	%SPL, __local_depot3;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r5, [memcmp_param_2];
	ld.param.u32 	%r4, [memcmp_param_1];
	ld.param.u32 	%r3, [memcmp_param_0];
	st.u32 	[%SP+0], %r3;
	st.u32 	[%SP+4], %r4;
	st.u32 	[%SP+8], %r5;
	ld.u32 	%r6, [%SP+0];
	st.u32 	[%SP+12], %r6;
	ld.u32 	%r7, [%SP+4];
	st.u32 	[%SP+16], %r7;
	bra.uni 	$L__BB3_1;
$L__BB3_1:
	ld.u32 	%r8, [%SP+8];
	setp.eq.s32 	%p4, %r8, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB3_3;
	bra.uni 	$L__BB3_2;
$L__BB3_2:
	ld.u32 	%r9, [%SP+12];
	ld.u8 	%r10, [%r9];
	ld.u32 	%r11, [%SP+16];
	ld.u8 	%r12, [%r11];
	setp.eq.s32 	%p1, %r10, %r12;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB3_3;
$L__BB3_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB3_6;
	bra.uni 	$L__BB3_4;
$L__BB3_4:
	bra.uni 	$L__BB3_5;
$L__BB3_5:
	ld.u32 	%r19, [%SP+8];
	add.s32 	%r20, %r19, -1;
	st.u32 	[%SP+8], %r20;
	ld.u32 	%r21, [%SP+12];
	add.s32 	%r22, %r21, 1;
	st.u32 	[%SP+12], %r22;
	ld.u32 	%r23, [%SP+16];
	add.s32 	%r24, %r23, 1;
	st.u32 	[%SP+16], %r24;
	bra.uni 	$L__BB3_1;
$L__BB3_6:
	ld.u32 	%r13, [%SP+8];
	setp.eq.s32 	%p5, %r13, 0;
	@%p5 bra 	$L__BB3_8;
	bra.uni 	$L__BB3_7;
$L__BB3_7:
	ld.u32 	%r15, [%SP+12];
	ld.u8 	%r16, [%r15];
	ld.u32 	%r17, [%SP+16];
	ld.u8 	%r18, [%r17];
	sub.s32 	%r1, %r16, %r18;
	mov.u32 	%r25, %r1;
	bra.uni 	$L__BB3_9;
$L__BB3_8:
	mov.b32 	%r14, 0;
	mov.u32 	%r25, %r14;
	bra.uni 	$L__BB3_9;
$L__BB3_9:
	mov.u32 	%r2, %r25;
	st.param.b32 	[func_retval0+0], %r2;
	ret;

}
	// .globl	memcpy
.visible .func  (.param .b32 func_retval0) memcpy(
	.param .b32 memcpy_param_0,
	.param .b32 memcpy_param_1,
	.param .b32 memcpy_param_2
)
{
	.local .align 4 .b8 	__local_depot4[20];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<14>;

	mov.u32 	%SPL, __local_depot4;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [memcpy_param_2];
	ld.param.u32 	%r2, [memcpy_param_1];
	ld.param.u32 	%r1, [memcpy_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+0];
	st.u32 	[%SP+12], %r4;
	ld.u32 	%r5, [%SP+4];
	st.u32 	[%SP+16], %r5;
	bra.uni 	$L__BB4_1;
$L__BB4_1:
	ld.u32 	%r6, [%SP+8];
	setp.eq.s32 	%p1, %r6, 0;
	@%p1 bra 	$L__BB4_4;
	bra.uni 	$L__BB4_2;
$L__BB4_2:
	ld.u32 	%r8, [%SP+16];
	add.s32 	%r9, %r8, 1;
	st.u32 	[%SP+16], %r9;
	ld.u8 	%rs1, [%r8];
	ld.u32 	%r10, [%SP+12];
	add.s32 	%r11, %r10, 1;
	st.u32 	[%SP+12], %r11;
	st.u8 	[%r10], %rs1;
	bra.uni 	$L__BB4_3;
$L__BB4_3:
	ld.u32 	%r12, [%SP+8];
	add.s32 	%r13, %r12, -1;
	st.u32 	[%SP+8], %r13;
	bra.uni 	$L__BB4_1;
$L__BB4_4:
	ld.u32 	%r7, [%SP+0];
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	memrchr
.visible .func  (.param .b32 func_retval0) memrchr(
	.param .b32 memrchr_param_0,
	.param .b32 memrchr_param_1,
	.param .b32 memrchr_param_2
)
{
	.local .align 4 .b8 	__local_depot5[20];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b32 	%r<18>;

	mov.u32 	%SPL, __local_depot5;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [memrchr_param_2];
	ld.param.u32 	%r2, [memrchr_param_1];
	ld.param.u32 	%r1, [memrchr_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u32 	[%SP+12], %r3;
	ld.u32 	%r4, [%SP+4];
	st.u32 	[%SP+16], %r4;
	ld.u8 	%r5, [%SP+8];
	st.u32 	[%SP+8], %r5;
	bra.uni 	$L__BB5_1;
$L__BB5_1:
	ld.u32 	%r6, [%SP+12];
	add.s32 	%r7, %r6, -1;
	st.u32 	[%SP+12], %r7;
	setp.eq.s32 	%p1, %r6, 0;
	@%p1 bra 	$L__BB5_5;
	bra.uni 	$L__BB5_2;
$L__BB5_2:
	ld.u32 	%r9, [%SP+16];
	ld.u32 	%r10, [%SP+12];
	add.s32 	%r11, %r9, %r10;
	ld.u8 	%r12, [%r11];
	ld.u32 	%r13, [%SP+8];
	setp.ne.s32 	%p2, %r12, %r13;
	@%p2 bra 	$L__BB5_4;
	bra.uni 	$L__BB5_3;
$L__BB5_3:
	ld.u32 	%r14, [%SP+16];
	ld.u32 	%r15, [%SP+12];
	add.s32 	%r16, %r14, %r15;
	st.u32 	[%SP+0], %r16;
	bra.uni 	$L__BB5_6;
$L__BB5_4:
	bra.uni 	$L__BB5_1;
$L__BB5_5:
	mov.b32 	%r8, 0;
	st.u32 	[%SP+0], %r8;
	bra.uni 	$L__BB5_6;
$L__BB5_6:
	ld.u32 	%r17, [%SP+0];
	st.param.b32 	[func_retval0+0], %r17;
	ret;

}
	// .globl	memset
.visible .func  (.param .b32 func_retval0) memset(
	.param .b32 memset_param_0,
	.param .b32 memset_param_1,
	.param .b32 memset_param_2
)
{
	.local .align 4 .b8 	__local_depot6[20];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;

	mov.u32 	%SPL, __local_depot6;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [memset_param_2];
	ld.param.u32 	%r2, [memset_param_1];
	ld.param.u32 	%r1, [memset_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+0];
	st.u32 	[%SP+12], %r4;
	bra.uni 	$L__BB6_1;
$L__BB6_1:
	ld.u32 	%r5, [%SP+8];
	setp.eq.s32 	%p1, %r5, 0;
	@%p1 bra 	$L__BB6_4;
	bra.uni 	$L__BB6_2;
$L__BB6_2:
	ld.u32 	%r7, [%SP+4];
	ld.u32 	%r8, [%SP+12];
	st.u8 	[%r8], %r7;
	bra.uni 	$L__BB6_3;
$L__BB6_3:
	ld.u32 	%r9, [%SP+8];
	add.s32 	%r10, %r9, -1;
	st.u32 	[%SP+8], %r10;
	ld.u32 	%r11, [%SP+12];
	add.s32 	%r12, %r11, 1;
	st.u32 	[%SP+12], %r12;
	bra.uni 	$L__BB6_1;
$L__BB6_4:
	ld.u32 	%r6, [%SP+0];
	st.param.b32 	[func_retval0+0], %r6;
	ret;

}
	// .globl	stpcpy
.visible .func  (.param .b32 func_retval0) stpcpy(
	.param .b32 stpcpy_param_0,
	.param .b32 stpcpy_param_1
)
{
	.local .align 4 .b8 	__local_depot7[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;

	mov.u32 	%SPL, __local_depot7;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [stpcpy_param_1];
	ld.param.u32 	%r1, [stpcpy_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	bra.uni 	$L__BB7_1;
$L__BB7_1:
	ld.u32 	%r3, [%SP+4];
	ld.u8 	%rs1, [%r3];
	ld.u32 	%r4, [%SP+0];
	st.u8 	[%r4], %rs1;
	setp.eq.s16 	%p1, %rs1, 0;
	@%p1 bra 	$L__BB7_4;
	bra.uni 	$L__BB7_2;
$L__BB7_2:
	bra.uni 	$L__BB7_3;
$L__BB7_3:
	ld.u32 	%r6, [%SP+4];
	add.s32 	%r7, %r6, 1;
	st.u32 	[%SP+4], %r7;
	ld.u32 	%r8, [%SP+0];
	add.s32 	%r9, %r8, 1;
	st.u32 	[%SP+0], %r9;
	bra.uni 	$L__BB7_1;
$L__BB7_4:
	ld.u32 	%r5, [%SP+0];
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	strchrnul
.visible .func  (.param .b32 func_retval0) strchrnul(
	.param .b32 strchrnul_param_0,
	.param .b32 strchrnul_param_1
)
{
	.local .align 4 .b8 	__local_depot8[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<12>;

	mov.u32 	%SPL, __local_depot8;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [strchrnul_param_1];
	ld.param.u32 	%r1, [strchrnul_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u8 	%r3, [%SP+4];
	st.u32 	[%SP+4], %r3;
	bra.uni 	$L__BB8_1;
$L__BB8_1:
	ld.u32 	%r4, [%SP+0];
	ld.s8 	%r5, [%r4];
	setp.eq.s32 	%p4, %r5, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB8_3;
	bra.uni 	$L__BB8_2;
$L__BB8_2:
	ld.u32 	%r6, [%SP+0];
	ld.u8 	%r7, [%r6];
	ld.u32 	%r8, [%SP+4];
	setp.ne.s32 	%p1, %r7, %r8;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB8_3;
$L__BB8_3:
	mov.pred 	%p2, %p5;
	@!%p2 bra 	$L__BB8_6;
	bra.uni 	$L__BB8_4;
$L__BB8_4:
	bra.uni 	$L__BB8_5;
$L__BB8_5:
	ld.u32 	%r10, [%SP+0];
	add.s32 	%r11, %r10, 1;
	st.u32 	[%SP+0], %r11;
	bra.uni 	$L__BB8_1;
$L__BB8_6:
	ld.u32 	%r9, [%SP+0];
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	strchr
.visible .func  (.param .b32 func_retval0) strchr(
	.param .b32 strchr_param_0,
	.param .b32 strchr_param_1
)
{
	.local .align 4 .b8 	__local_depot9[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<11>;

	mov.u32 	%SPL, __local_depot9;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [strchr_param_1];
	ld.param.u32 	%r1, [strchr_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	bra.uni 	$L__BB9_1;
$L__BB9_1:
	ld.u32 	%r3, [%SP+4];
	ld.s8 	%r4, [%r3];
	ld.u32 	%r5, [%SP+8];
	setp.ne.s32 	%p1, %r4, %r5;
	@%p1 bra 	$L__BB9_3;
	bra.uni 	$L__BB9_2;
$L__BB9_2:
	ld.u32 	%r9, [%SP+4];
	st.u32 	[%SP+0], %r9;
	bra.uni 	$L__BB9_6;
$L__BB9_3:
	bra.uni 	$L__BB9_4;
$L__BB9_4:
	ld.u32 	%r6, [%SP+4];
	add.s32 	%r7, %r6, 1;
	st.u32 	[%SP+4], %r7;
	ld.u8 	%rs1, [%r6];
	setp.ne.s16 	%p2, %rs1, 0;
	@%p2 bra 	$L__BB9_1;
	bra.uni 	$L__BB9_5;
$L__BB9_5:
	mov.b32 	%r8, 0;
	st.u32 	[%SP+0], %r8;
	bra.uni 	$L__BB9_6;
$L__BB9_6:
	ld.u32 	%r10, [%SP+0];
	st.param.b32 	[func_retval0+0], %r10;
	ret;

}
	// .globl	strcmp
.visible .func  (.param .b32 func_retval0) strcmp(
	.param .b32 strcmp_param_0,
	.param .b32 strcmp_param_1
)
{
	.local .align 4 .b8 	__local_depot10[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<18>;

	mov.u32 	%SPL, __local_depot10;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [strcmp_param_1];
	ld.param.u32 	%r1, [strcmp_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	bra.uni 	$L__BB10_1;
$L__BB10_1:
	ld.u32 	%r3, [%SP+0];
	ld.s8 	%r4, [%r3];
	ld.u32 	%r5, [%SP+4];
	ld.s8 	%r6, [%r5];
	setp.ne.s32 	%p4, %r4, %r6;
	mov.pred 	%p3, 0;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB10_3;
	bra.uni 	$L__BB10_2;
$L__BB10_2:
	ld.u32 	%r7, [%SP+0];
	ld.s8 	%r8, [%r7];
	setp.ne.s32 	%p1, %r8, 0;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB10_3;
$L__BB10_3:
	mov.pred 	%p2, %p5;
	@!%p2 bra 	$L__BB10_6;
	bra.uni 	$L__BB10_4;
$L__BB10_4:
	bra.uni 	$L__BB10_5;
$L__BB10_5:
	ld.u32 	%r14, [%SP+0];
	add.s32 	%r15, %r14, 1;
	st.u32 	[%SP+0], %r15;
	ld.u32 	%r16, [%SP+4];
	add.s32 	%r17, %r16, 1;
	st.u32 	[%SP+4], %r17;
	bra.uni 	$L__BB10_1;
$L__BB10_6:
	ld.u32 	%r9, [%SP+0];
	ld.u8 	%r10, [%r9];
	ld.u32 	%r11, [%SP+4];
	ld.u8 	%r12, [%r11];
	sub.s32 	%r13, %r10, %r12;
	st.param.b32 	[func_retval0+0], %r13;
	ret;

}
	// .globl	strlen
.visible .func  (.param .b32 func_retval0) strlen(
	.param .b32 strlen_param_0
)
{
	.local .align 4 .b8 	__local_depot11[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<9>;

	mov.u32 	%SPL, __local_depot11;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [strlen_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+4], %r2;
	bra.uni 	$L__BB11_1;
$L__BB11_1:
	ld.u32 	%r3, [%SP+0];
	ld.u8 	%rs1, [%r3];
	setp.eq.s16 	%p1, %rs1, 0;
	@%p1 bra 	$L__BB11_4;
	bra.uni 	$L__BB11_2;
$L__BB11_2:
	bra.uni 	$L__BB11_3;
$L__BB11_3:
	ld.u32 	%r7, [%SP+0];
	add.s32 	%r8, %r7, 1;
	st.u32 	[%SP+0], %r8;
	bra.uni 	$L__BB11_1;
$L__BB11_4:
	ld.u32 	%r4, [%SP+0];
	ld.u32 	%r5, [%SP+4];
	sub.s32 	%r6, %r4, %r5;
	st.param.b32 	[func_retval0+0], %r6;
	ret;

}
	// .globl	strncmp
.visible .func  (.param .b32 func_retval0) strncmp(
	.param .b32 strncmp_param_0,
	.param .b32 strncmp_param_1,
	.param .b32 strncmp_param_2
)
{
	.local .align 4 .b8 	__local_depot12[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<28>;

	mov.u32 	%SPL, __local_depot12;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [strncmp_param_2];
	ld.param.u32 	%r2, [strncmp_param_1];
	ld.param.u32 	%r1, [strncmp_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u32 	[%SP+12], %r3;
	ld.u32 	%r4, [%SP+4];
	st.u32 	[%SP+16], %r4;
	ld.u32 	%r5, [%SP+8];
	st.u32 	[%SP+20], %r5;
	ld.u32 	%r6, [%SP+12];
	add.s32 	%r7, %r6, -1;
	st.u32 	[%SP+12], %r7;
	setp.ne.s32 	%p3, %r6, 0;
	@%p3 bra 	$L__BB12_2;
	bra.uni 	$L__BB12_1;
$L__BB12_1:
	mov.b32 	%r8, 0;
	st.u32 	[%SP+0], %r8;
	bra.uni 	$L__BB12_11;
$L__BB12_2:
	bra.uni 	$L__BB12_3;
$L__BB12_3:
	ld.u32 	%r9, [%SP+16];
	ld.u8 	%rs1, [%r9];
	setp.eq.s16 	%p5, %rs1, 0;
	mov.pred 	%p4, 0;
	mov.pred 	%p10, %p4;
	@%p5 bra 	$L__BB12_7;
	bra.uni 	$L__BB12_4;
$L__BB12_4:
	ld.u32 	%r10, [%SP+20];
	ld.u8 	%rs2, [%r10];
	setp.eq.s16 	%p7, %rs2, 0;
	mov.pred 	%p6, 0;
	mov.pred 	%p10, %p6;
	@%p7 bra 	$L__BB12_7;
	bra.uni 	$L__BB12_5;
$L__BB12_5:
	ld.u32 	%r11, [%SP+12];
	setp.eq.s32 	%p9, %r11, 0;
	mov.pred 	%p8, 0;
	mov.pred 	%p10, %p8;
	@%p9 bra 	$L__BB12_7;
	bra.uni 	$L__BB12_6;
$L__BB12_6:
	ld.u32 	%r12, [%SP+16];
	ld.u8 	%r13, [%r12];
	ld.u32 	%r14, [%SP+20];
	ld.u8 	%r15, [%r14];
	setp.eq.s32 	%p1, %r13, %r15;
	mov.pred 	%p10, %p1;
	bra.uni 	$L__BB12_7;
$L__BB12_7:
	mov.pred 	%p2, %p10;
	@!%p2 bra 	$L__BB12_10;
	bra.uni 	$L__BB12_8;
$L__BB12_8:
	bra.uni 	$L__BB12_9;
$L__BB12_9:
	ld.u32 	%r22, [%SP+16];
	add.s32 	%r23, %r22, 1;
	st.u32 	[%SP+16], %r23;
	ld.u32 	%r24, [%SP+20];
	add.s32 	%r25, %r24, 1;
	st.u32 	[%SP+20], %r25;
	ld.u32 	%r26, [%SP+12];
	add.s32 	%r27, %r26, -1;
	st.u32 	[%SP+12], %r27;
	bra.uni 	$L__BB12_3;
$L__BB12_10:
	ld.u32 	%r16, [%SP+16];
	ld.u8 	%r17, [%r16];
	ld.u32 	%r18, [%SP+20];
	ld.u8 	%r19, [%r18];
	sub.s32 	%r20, %r17, %r19;
	st.u32 	[%SP+0], %r20;
	bra.uni 	$L__BB12_11;
$L__BB12_11:
	ld.u32 	%r21, [%SP+0];
	st.param.b32 	[func_retval0+0], %r21;
	ret;

}
	// .globl	swab
.visible .func swab(
	.param .b32 swab_param_0,
	.param .b32 swab_param_1,
	.param .b32 swab_param_2
)
{
	.local .align 4 .b8 	__local_depot13[20];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<17>;

	mov.u32 	%SPL, __local_depot13;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [swab_param_2];
	ld.param.u32 	%r2, [swab_param_1];
	ld.param.u32 	%r1, [swab_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+0];
	st.u32 	[%SP+12], %r4;
	ld.u32 	%r5, [%SP+4];
	st.u32 	[%SP+16], %r5;
	bra.uni 	$L__BB13_1;
$L__BB13_1:
	ld.u32 	%r6, [%SP+8];
	setp.lt.s32 	%p1, %r6, 2;
	@%p1 bra 	$L__BB13_4;
	bra.uni 	$L__BB13_2;
$L__BB13_2:
	ld.u32 	%r7, [%SP+12];
	ld.u8 	%rs1, [%r7+1];
	ld.u32 	%r8, [%SP+16];
	st.u8 	[%r8], %rs1;
	ld.u32 	%r9, [%SP+12];
	ld.u8 	%rs2, [%r9];
	ld.u32 	%r10, [%SP+16];
	st.u8 	[%r10+1], %rs2;
	ld.u32 	%r11, [%SP+16];
	add.s32 	%r12, %r11, 2;
	st.u32 	[%SP+16], %r12;
	ld.u32 	%r13, [%SP+12];
	add.s32 	%r14, %r13, 2;
	st.u32 	[%SP+12], %r14;
	bra.uni 	$L__BB13_3;
$L__BB13_3:
	ld.u32 	%r15, [%SP+8];
	add.s32 	%r16, %r15, -2;
	st.u32 	[%SP+8], %r16;
	bra.uni 	$L__BB13_1;
$L__BB13_4:
	ret;

}
	// .globl	isalpha
.visible .func  (.param .b32 func_retval0) isalpha(
	.param .b32 isalpha_param_0
)
{
	.local .align 4 .b8 	__local_depot14[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;

	mov.u32 	%SPL, __local_depot14;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [isalpha_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	or.b32  	%r3, %r2, 32;
	add.s32 	%r4, %r3, -97;
	setp.lt.u32 	%p1, %r4, 26;
	selp.u32 	%r5, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	isascii
.visible .func  (.param .b32 func_retval0) isascii(
	.param .b32 isascii_param_0
)
{
	.local .align 4 .b8 	__local_depot15[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u32 	%SPL, __local_depot15;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [isascii_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	and.b32  	%r3, %r2, -128;
	setp.eq.s32 	%p1, %r3, 0;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	isblank
.visible .func  (.param .b32 func_retval0) isblank(
	.param .b32 isblank_param_0
)
{
	.local .align 4 .b8 	__local_depot16[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<5>;

	mov.u32 	%SPL, __local_depot16;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [isblank_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	setp.eq.s32 	%p4, %r2, 32;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB16_2;
	bra.uni 	$L__BB16_1;
$L__BB16_1:
	ld.u32 	%r3, [%SP+0];
	setp.eq.s32 	%p1, %r3, 9;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB16_2;
$L__BB16_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r4, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	iscntrl
.visible .func  (.param .b32 func_retval0) iscntrl(
	.param .b32 iscntrl_param_0
)
{
	.local .align 4 .b8 	__local_depot17[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<5>;

	mov.u32 	%SPL, __local_depot17;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [iscntrl_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	setp.lt.u32 	%p4, %r2, 32;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB17_2;
	bra.uni 	$L__BB17_1;
$L__BB17_1:
	ld.u32 	%r3, [%SP+0];
	setp.eq.s32 	%p1, %r3, 127;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB17_2;
$L__BB17_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r4, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	isdigit
.visible .func  (.param .b32 func_retval0) isdigit(
	.param .b32 isdigit_param_0
)
{
	.local .align 4 .b8 	__local_depot18[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u32 	%SPL, __local_depot18;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [isdigit_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -48;
	setp.lt.u32 	%p1, %r3, 10;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	isgraph
.visible .func  (.param .b32 func_retval0) isgraph(
	.param .b32 isgraph_param_0
)
{
	.local .align 4 .b8 	__local_depot19[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u32 	%SPL, __local_depot19;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [isgraph_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -33;
	setp.lt.u32 	%p1, %r3, 94;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	islower
.visible .func  (.param .b32 func_retval0) islower(
	.param .b32 islower_param_0
)
{
	.local .align 4 .b8 	__local_depot20[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u32 	%SPL, __local_depot20;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [islower_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -97;
	setp.lt.u32 	%p1, %r3, 26;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	isprint
.visible .func  (.param .b32 func_retval0) isprint(
	.param .b32 isprint_param_0
)
{
	.local .align 4 .b8 	__local_depot21[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u32 	%SPL, __local_depot21;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [isprint_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -32;
	setp.lt.u32 	%p1, %r3, 95;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	isspace
.visible .func  (.param .b32 func_retval0) isspace(
	.param .b32 isspace_param_0
)
{
	.local .align 4 .b8 	__local_depot22[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<6>;

	mov.u32 	%SPL, __local_depot22;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [isspace_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	setp.eq.s32 	%p4, %r2, 32;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB22_2;
	bra.uni 	$L__BB22_1;
$L__BB22_1:
	ld.u32 	%r3, [%SP+0];
	add.s32 	%r4, %r3, -9;
	setp.lt.u32 	%p1, %r4, 5;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB22_2;
$L__BB22_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r5, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	isupper
.visible .func  (.param .b32 func_retval0) isupper(
	.param .b32 isupper_param_0
)
{
	.local .align 4 .b8 	__local_depot23[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u32 	%SPL, __local_depot23;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [isupper_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -65;
	setp.lt.u32 	%p1, %r3, 26;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	iswcntrl
.visible .func  (.param .b32 func_retval0) iswcntrl(
	.param .b32 iswcntrl_param_0
)
{
	.local .align 4 .b8 	__local_depot24[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<10>;

	mov.u32 	%SPL, __local_depot24;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [iswcntrl_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	setp.lt.u32 	%p4, %r2, 32;
	mov.pred 	%p3, -1;
	mov.pred 	%p9, %p3;
	@%p4 bra 	$L__BB24_4;
	bra.uni 	$L__BB24_1;
$L__BB24_1:
	ld.u32 	%r3, [%SP+0];
	add.s32 	%r4, %r3, -127;
	setp.lt.u32 	%p6, %r4, 33;
	mov.pred 	%p5, -1;
	mov.pred 	%p9, %p5;
	@%p6 bra 	$L__BB24_4;
	bra.uni 	$L__BB24_2;
$L__BB24_2:
	ld.u32 	%r5, [%SP+0];
	add.s32 	%r6, %r5, -8232;
	setp.lt.u32 	%p8, %r6, 2;
	mov.pred 	%p7, -1;
	mov.pred 	%p9, %p7;
	@%p8 bra 	$L__BB24_4;
	bra.uni 	$L__BB24_3;
$L__BB24_3:
	ld.u32 	%r7, [%SP+0];
	add.s32 	%r8, %r7, -65529;
	setp.lt.u32 	%p1, %r8, 3;
	mov.pred 	%p9, %p1;
	bra.uni 	$L__BB24_4;
$L__BB24_4:
	mov.pred 	%p2, %p9;
	selp.u32 	%r9, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	iswdigit
.visible .func  (.param .b32 func_retval0) iswdigit(
	.param .b32 iswdigit_param_0
)
{
	.local .align 4 .b8 	__local_depot25[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u32 	%SPL, __local_depot25;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [iswdigit_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -48;
	setp.lt.u32 	%p1, %r3, 10;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	iswprint
.visible .func  (.param .b32 func_retval0) iswprint(
	.param .b32 iswprint_param_0
)
{
	.local .align 4 .b8 	__local_depot26[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<8>;
	.reg .b32 	%r<20>;

	mov.u32 	%SPL, __local_depot26;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [iswprint_param_0];
	st.u32 	[%SP+4], %r1;
	ld.u32 	%r2, [%SP+4];
	setp.gt.u32 	%p1, %r2, 254;
	@%p1 bra 	$L__BB26_2;
	bra.uni 	$L__BB26_1;
$L__BB26_1:
	ld.u32 	%r15, [%SP+4];
	add.s32 	%r16, %r15, 1;
	and.b32  	%r17, %r16, 127;
	setp.gt.s32 	%p7, %r17, 32;
	selp.u32 	%r18, 1, 0, %p7;
	st.u32 	[%SP+0], %r18;
	bra.uni 	$L__BB26_10;
$L__BB26_2:
	ld.u32 	%r3, [%SP+4];
	setp.lt.u32 	%p2, %r3, 8232;
	@%p2 bra 	$L__BB26_5;
	bra.uni 	$L__BB26_3;
$L__BB26_3:
	ld.u32 	%r4, [%SP+4];
	add.s32 	%r5, %r4, -8234;
	setp.lt.u32 	%p3, %r5, 47062;
	@%p3 bra 	$L__BB26_5;
	bra.uni 	$L__BB26_4;
$L__BB26_4:
	ld.u32 	%r6, [%SP+4];
	add.s32 	%r7, %r6, -57344;
	setp.gt.u32 	%p4, %r7, 8184;
	@%p4 bra 	$L__BB26_6;
	bra.uni 	$L__BB26_5;
$L__BB26_5:
	mov.b32 	%r14, 1;
	st.u32 	[%SP+0], %r14;
	bra.uni 	$L__BB26_10;
$L__BB26_6:
	ld.u32 	%r8, [%SP+4];
	add.s32 	%r9, %r8, -65532;
	setp.gt.u32 	%p5, %r9, 1048579;
	@%p5 bra 	$L__BB26_8;
	bra.uni 	$L__BB26_7;
$L__BB26_7:
	ld.u32 	%r10, [%SP+4];
	and.b32  	%r11, %r10, 65534;
	setp.ne.s32 	%p6, %r11, 65534;
	@%p6 bra 	$L__BB26_9;
	bra.uni 	$L__BB26_8;
$L__BB26_8:
	mov.b32 	%r13, 0;
	st.u32 	[%SP+0], %r13;
	bra.uni 	$L__BB26_10;
$L__BB26_9:
	mov.b32 	%r12, 1;
	st.u32 	[%SP+0], %r12;
	bra.uni 	$L__BB26_10;
$L__BB26_10:
	ld.u32 	%r19, [%SP+0];
	st.param.b32 	[func_retval0+0], %r19;
	ret;

}
	// .globl	iswxdigit
.visible .func  (.param .b32 func_retval0) iswxdigit(
	.param .b32 iswxdigit_param_0
)
{
	.local .align 4 .b8 	__local_depot27[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<8>;

	mov.u32 	%SPL, __local_depot27;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [iswxdigit_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -48;
	setp.lt.u32 	%p4, %r3, 10;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB27_2;
	bra.uni 	$L__BB27_1;
$L__BB27_1:
	ld.u32 	%r4, [%SP+0];
	or.b32  	%r5, %r4, 32;
	add.s32 	%r6, %r5, -97;
	setp.lt.u32 	%p1, %r6, 6;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB27_2;
$L__BB27_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r7, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	toascii
.visible .func  (.param .b32 func_retval0) toascii(
	.param .b32 toascii_param_0
)
{
	.local .align 4 .b8 	__local_depot28[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<4>;

	mov.u32 	%SPL, __local_depot28;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [toascii_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	and.b32  	%r3, %r2, 127;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	fdim
.visible .func  (.param .b64 func_retval0) fdim(
	.param .b64 fdim_param_0,
	.param .b64 fdim_param_1
)
{
	.local .align 8 .b8 	__local_depot29[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<4>;
	.reg .f64 	%fd<16>;

	mov.u32 	%SPL, __local_depot29;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f64 	%fd4, [fdim_param_1];
	ld.param.f64 	%fd3, [fdim_param_0];
	st.f64 	[%SP+8], %fd3;
	st.f64 	[%SP+16], %fd4;
	ld.f64 	%fd5, [%SP+8];
	setp.num.f64 	%p1, %fd5, %fd5;
	@%p1 bra 	$L__BB29_2;
	bra.uni 	$L__BB29_1;
$L__BB29_1:
	ld.f64 	%fd13, [%SP+8];
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB29_8;
$L__BB29_2:
	ld.f64 	%fd6, [%SP+16];
	setp.num.f64 	%p2, %fd6, %fd6;
	@%p2 bra 	$L__BB29_4;
	bra.uni 	$L__BB29_3;
$L__BB29_3:
	ld.f64 	%fd12, [%SP+16];
	st.f64 	[%SP+0], %fd12;
	bra.uni 	$L__BB29_8;
$L__BB29_4:
	ld.f64 	%fd7, [%SP+8];
	ld.f64 	%fd8, [%SP+16];
	setp.leu.f64 	%p3, %fd7, %fd8;
	@%p3 bra 	$L__BB29_6;
	bra.uni 	$L__BB29_5;
$L__BB29_5:
	ld.f64 	%fd10, [%SP+8];
	ld.f64 	%fd11, [%SP+16];
	sub.rn.f64 	%fd1, %fd10, %fd11;
	mov.f64 	%fd15, %fd1;
	bra.uni 	$L__BB29_7;
$L__BB29_6:
	mov.f64 	%fd9, 0d0000000000000000;
	mov.f64 	%fd15, %fd9;
	bra.uni 	$L__BB29_7;
$L__BB29_7:
	mov.f64 	%fd2, %fd15;
	st.f64 	[%SP+0], %fd2;
	bra.uni 	$L__BB29_8;
$L__BB29_8:
	ld.f64 	%fd14, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd14;
	ret;

}
	// .globl	fdimf
.visible .func  (.param .b32 func_retval0) fdimf(
	.param .b32 fdimf_param_0,
	.param .b32 fdimf_param_1
)
{
	.local .align 4 .b8 	__local_depot30[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<4>;
	.reg .f32 	%f<16>;

	mov.u32 	%SPL, __local_depot30;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f32 	%f4, [fdimf_param_1];
	ld.param.f32 	%f3, [fdimf_param_0];
	st.f32 	[%SP+4], %f3;
	st.f32 	[%SP+8], %f4;
	ld.f32 	%f5, [%SP+4];
	setp.num.f32 	%p1, %f5, %f5;
	@%p1 bra 	$L__BB30_2;
	bra.uni 	$L__BB30_1;
$L__BB30_1:
	ld.f32 	%f13, [%SP+4];
	st.f32 	[%SP+0], %f13;
	bra.uni 	$L__BB30_8;
$L__BB30_2:
	ld.f32 	%f6, [%SP+8];
	setp.num.f32 	%p2, %f6, %f6;
	@%p2 bra 	$L__BB30_4;
	bra.uni 	$L__BB30_3;
$L__BB30_3:
	ld.f32 	%f12, [%SP+8];
	st.f32 	[%SP+0], %f12;
	bra.uni 	$L__BB30_8;
$L__BB30_4:
	ld.f32 	%f7, [%SP+4];
	ld.f32 	%f8, [%SP+8];
	setp.leu.f32 	%p3, %f7, %f8;
	@%p3 bra 	$L__BB30_6;
	bra.uni 	$L__BB30_5;
$L__BB30_5:
	ld.f32 	%f10, [%SP+4];
	ld.f32 	%f11, [%SP+8];
	sub.rn.f32 	%f1, %f10, %f11;
	mov.f32 	%f15, %f1;
	bra.uni 	$L__BB30_7;
$L__BB30_6:
	mov.f32 	%f9, 0f00000000;
	mov.f32 	%f15, %f9;
	bra.uni 	$L__BB30_7;
$L__BB30_7:
	mov.f32 	%f2, %f15;
	st.f32 	[%SP+0], %f2;
	bra.uni 	$L__BB30_8;
$L__BB30_8:
	ld.f32 	%f14, [%SP+0];
	st.param.f32 	[func_retval0+0], %f14;
	ret;

}
	// .globl	fmax
.visible .func  (.param .b64 func_retval0) fmax(
	.param .b64 fmax_param_0,
	.param .b64 fmax_param_1
)
{
	.local .align 8 .b8 	__local_depot31[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<6>;
	.reg .f64 	%fd<18>;

	mov.u32 	%SPL, __local_depot31;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f64 	%fd8, [fmax_param_1];
	ld.param.f64 	%fd7, [fmax_param_0];
	st.f64 	[%SP+8], %fd7;
	st.f64 	[%SP+16], %fd8;
	ld.f64 	%fd9, [%SP+8];
	setp.num.f64 	%p1, %fd9, %fd9;
	@%p1 bra 	$L__BB31_2;
	bra.uni 	$L__BB31_1;
$L__BB31_1:
	ld.f64 	%fd14, [%SP+16];
	st.f64 	[%SP+0], %fd14;
	bra.uni 	$L__BB31_13;
$L__BB31_2:
	ld.f64 	%fd10, [%SP+16];
	setp.num.f64 	%p2, %fd10, %fd10;
	@%p2 bra 	$L__BB31_4;
	bra.uni 	$L__BB31_3;
$L__BB31_3:
	ld.f64 	%fd13, [%SP+8];
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB31_13;
$L__BB31_4:
	ld.u64 	%rd1, [%SP+8];
	shr.u64 	%rd2, %rd1, 63;
	cvt.u32.u64 	%r1, %rd2;
	ld.u64 	%rd3, [%SP+16];
	shr.u64 	%rd4, %rd3, 63;
	cvt.u32.u64 	%r2, %rd4;
	setp.eq.s32 	%p3, %r1, %r2;
	@%p3 bra 	$L__BB31_9;
	bra.uni 	$L__BB31_5;
$L__BB31_5:
	ld.u64 	%rd5, [%SP+8];
	setp.gt.s64 	%p5, %rd5, -1;
	@%p5 bra 	$L__BB31_7;
	bra.uni 	$L__BB31_6;
$L__BB31_6:
	ld.f64 	%fd1, [%SP+16];
	mov.f64 	%fd16, %fd1;
	bra.uni 	$L__BB31_8;
$L__BB31_7:
	ld.f64 	%fd2, [%SP+8];
	mov.f64 	%fd16, %fd2;
	bra.uni 	$L__BB31_8;
$L__BB31_8:
	mov.f64 	%fd3, %fd16;
	st.f64 	[%SP+0], %fd3;
	bra.uni 	$L__BB31_13;
$L__BB31_9:
	ld.f64 	%fd11, [%SP+8];
	ld.f64 	%fd12, [%SP+16];
	setp.geu.f64 	%p4, %fd11, %fd12;
	@%p4 bra 	$L__BB31_11;
	bra.uni 	$L__BB31_10;
$L__BB31_10:
	ld.f64 	%fd4, [%SP+16];
	mov.f64 	%fd17, %fd4;
	bra.uni 	$L__BB31_12;
$L__BB31_11:
	ld.f64 	%fd5, [%SP+8];
	mov.f64 	%fd17, %fd5;
	bra.uni 	$L__BB31_12;
$L__BB31_12:
	mov.f64 	%fd6, %fd17;
	st.f64 	[%SP+0], %fd6;
	bra.uni 	$L__BB31_13;
$L__BB31_13:
	ld.f64 	%fd15, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd15;
	ret;

}
	// .globl	fmaxf
.visible .func  (.param .b32 func_retval0) fmaxf(
	.param .b32 fmaxf_param_0,
	.param .b32 fmaxf_param_1
)
{
	.local .align 4 .b8 	__local_depot32[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<6>;
	.reg .f32 	%f<18>;

	mov.u32 	%SPL, __local_depot32;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f32 	%f8, [fmaxf_param_1];
	ld.param.f32 	%f7, [fmaxf_param_0];
	st.f32 	[%SP+4], %f7;
	st.f32 	[%SP+8], %f8;
	ld.f32 	%f9, [%SP+4];
	setp.num.f32 	%p1, %f9, %f9;
	@%p1 bra 	$L__BB32_2;
	bra.uni 	$L__BB32_1;
$L__BB32_1:
	ld.f32 	%f14, [%SP+8];
	st.f32 	[%SP+0], %f14;
	bra.uni 	$L__BB32_13;
$L__BB32_2:
	ld.f32 	%f10, [%SP+8];
	setp.num.f32 	%p2, %f10, %f10;
	@%p2 bra 	$L__BB32_4;
	bra.uni 	$L__BB32_3;
$L__BB32_3:
	ld.f32 	%f13, [%SP+4];
	st.f32 	[%SP+0], %f13;
	bra.uni 	$L__BB32_13;
$L__BB32_4:
	ld.u32 	%r1, [%SP+4];
	shr.u32 	%r2, %r1, 31;
	ld.u32 	%r3, [%SP+8];
	shr.u32 	%r4, %r3, 31;
	setp.eq.s32 	%p3, %r2, %r4;
	@%p3 bra 	$L__BB32_9;
	bra.uni 	$L__BB32_5;
$L__BB32_5:
	ld.u32 	%r5, [%SP+4];
	setp.gt.s32 	%p5, %r5, -1;
	@%p5 bra 	$L__BB32_7;
	bra.uni 	$L__BB32_6;
$L__BB32_6:
	ld.f32 	%f1, [%SP+8];
	mov.f32 	%f16, %f1;
	bra.uni 	$L__BB32_8;
$L__BB32_7:
	ld.f32 	%f2, [%SP+4];
	mov.f32 	%f16, %f2;
	bra.uni 	$L__BB32_8;
$L__BB32_8:
	mov.f32 	%f3, %f16;
	st.f32 	[%SP+0], %f3;
	bra.uni 	$L__BB32_13;
$L__BB32_9:
	ld.f32 	%f11, [%SP+4];
	ld.f32 	%f12, [%SP+8];
	setp.geu.f32 	%p4, %f11, %f12;
	@%p4 bra 	$L__BB32_11;
	bra.uni 	$L__BB32_10;
$L__BB32_10:
	ld.f32 	%f4, [%SP+8];
	mov.f32 	%f17, %f4;
	bra.uni 	$L__BB32_12;
$L__BB32_11:
	ld.f32 	%f5, [%SP+4];
	mov.f32 	%f17, %f5;
	bra.uni 	$L__BB32_12;
$L__BB32_12:
	mov.f32 	%f6, %f17;
	st.f32 	[%SP+0], %f6;
	bra.uni 	$L__BB32_13;
$L__BB32_13:
	ld.f32 	%f15, [%SP+0];
	st.param.f32 	[func_retval0+0], %f15;
	ret;

}
	// .globl	fmaxl
.visible .func  (.param .b64 func_retval0) fmaxl(
	.param .b64 fmaxl_param_0,
	.param .b64 fmaxl_param_1
)
{
	.local .align 8 .b8 	__local_depot33[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<6>;
	.reg .f64 	%fd<18>;

	mov.u32 	%SPL, __local_depot33;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f64 	%fd8, [fmaxl_param_1];
	ld.param.f64 	%fd7, [fmaxl_param_0];
	st.f64 	[%SP+8], %fd7;
	st.f64 	[%SP+16], %fd8;
	ld.f64 	%fd9, [%SP+8];
	setp.num.f64 	%p1, %fd9, %fd9;
	@%p1 bra 	$L__BB33_2;
	bra.uni 	$L__BB33_1;
$L__BB33_1:
	ld.f64 	%fd14, [%SP+16];
	st.f64 	[%SP+0], %fd14;
	bra.uni 	$L__BB33_13;
$L__BB33_2:
	ld.f64 	%fd10, [%SP+16];
	setp.num.f64 	%p2, %fd10, %fd10;
	@%p2 bra 	$L__BB33_4;
	bra.uni 	$L__BB33_3;
$L__BB33_3:
	ld.f64 	%fd13, [%SP+8];
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB33_13;
$L__BB33_4:
	ld.u64 	%rd1, [%SP+8];
	shr.u64 	%rd2, %rd1, 63;
	cvt.u32.u64 	%r1, %rd2;
	ld.u64 	%rd3, [%SP+16];
	shr.u64 	%rd4, %rd3, 63;
	cvt.u32.u64 	%r2, %rd4;
	setp.eq.s32 	%p3, %r1, %r2;
	@%p3 bra 	$L__BB33_9;
	bra.uni 	$L__BB33_5;
$L__BB33_5:
	ld.u64 	%rd5, [%SP+8];
	setp.gt.s64 	%p5, %rd5, -1;
	@%p5 bra 	$L__BB33_7;
	bra.uni 	$L__BB33_6;
$L__BB33_6:
	ld.f64 	%fd1, [%SP+16];
	mov.f64 	%fd16, %fd1;
	bra.uni 	$L__BB33_8;
$L__BB33_7:
	ld.f64 	%fd2, [%SP+8];
	mov.f64 	%fd16, %fd2;
	bra.uni 	$L__BB33_8;
$L__BB33_8:
	mov.f64 	%fd3, %fd16;
	st.f64 	[%SP+0], %fd3;
	bra.uni 	$L__BB33_13;
$L__BB33_9:
	ld.f64 	%fd11, [%SP+8];
	ld.f64 	%fd12, [%SP+16];
	setp.geu.f64 	%p4, %fd11, %fd12;
	@%p4 bra 	$L__BB33_11;
	bra.uni 	$L__BB33_10;
$L__BB33_10:
	ld.f64 	%fd4, [%SP+16];
	mov.f64 	%fd17, %fd4;
	bra.uni 	$L__BB33_12;
$L__BB33_11:
	ld.f64 	%fd5, [%SP+8];
	mov.f64 	%fd17, %fd5;
	bra.uni 	$L__BB33_12;
$L__BB33_12:
	mov.f64 	%fd6, %fd17;
	st.f64 	[%SP+0], %fd6;
	bra.uni 	$L__BB33_13;
$L__BB33_13:
	ld.f64 	%fd15, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd15;
	ret;

}
	// .globl	fmin
.visible .func  (.param .b64 func_retval0) fmin(
	.param .b64 fmin_param_0,
	.param .b64 fmin_param_1
)
{
	.local .align 8 .b8 	__local_depot34[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<6>;
	.reg .f64 	%fd<18>;

	mov.u32 	%SPL, __local_depot34;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f64 	%fd8, [fmin_param_1];
	ld.param.f64 	%fd7, [fmin_param_0];
	st.f64 	[%SP+8], %fd7;
	st.f64 	[%SP+16], %fd8;
	ld.f64 	%fd9, [%SP+8];
	setp.num.f64 	%p1, %fd9, %fd9;
	@%p1 bra 	$L__BB34_2;
	bra.uni 	$L__BB34_1;
$L__BB34_1:
	ld.f64 	%fd14, [%SP+16];
	st.f64 	[%SP+0], %fd14;
	bra.uni 	$L__BB34_13;
$L__BB34_2:
	ld.f64 	%fd10, [%SP+16];
	setp.num.f64 	%p2, %fd10, %fd10;
	@%p2 bra 	$L__BB34_4;
	bra.uni 	$L__BB34_3;
$L__BB34_3:
	ld.f64 	%fd13, [%SP+8];
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB34_13;
$L__BB34_4:
	ld.u64 	%rd1, [%SP+8];
	shr.u64 	%rd2, %rd1, 63;
	cvt.u32.u64 	%r1, %rd2;
	ld.u64 	%rd3, [%SP+16];
	shr.u64 	%rd4, %rd3, 63;
	cvt.u32.u64 	%r2, %rd4;
	setp.eq.s32 	%p3, %r1, %r2;
	@%p3 bra 	$L__BB34_9;
	bra.uni 	$L__BB34_5;
$L__BB34_5:
	ld.u64 	%rd5, [%SP+8];
	setp.gt.s64 	%p5, %rd5, -1;
	@%p5 bra 	$L__BB34_7;
	bra.uni 	$L__BB34_6;
$L__BB34_6:
	ld.f64 	%fd1, [%SP+8];
	mov.f64 	%fd16, %fd1;
	bra.uni 	$L__BB34_8;
$L__BB34_7:
	ld.f64 	%fd2, [%SP+16];
	mov.f64 	%fd16, %fd2;
	bra.uni 	$L__BB34_8;
$L__BB34_8:
	mov.f64 	%fd3, %fd16;
	st.f64 	[%SP+0], %fd3;
	bra.uni 	$L__BB34_13;
$L__BB34_9:
	ld.f64 	%fd11, [%SP+8];
	ld.f64 	%fd12, [%SP+16];
	setp.geu.f64 	%p4, %fd11, %fd12;
	@%p4 bra 	$L__BB34_11;
	bra.uni 	$L__BB34_10;
$L__BB34_10:
	ld.f64 	%fd4, [%SP+8];
	mov.f64 	%fd17, %fd4;
	bra.uni 	$L__BB34_12;
$L__BB34_11:
	ld.f64 	%fd5, [%SP+16];
	mov.f64 	%fd17, %fd5;
	bra.uni 	$L__BB34_12;
$L__BB34_12:
	mov.f64 	%fd6, %fd17;
	st.f64 	[%SP+0], %fd6;
	bra.uni 	$L__BB34_13;
$L__BB34_13:
	ld.f64 	%fd15, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd15;
	ret;

}
	// .globl	fminf
.visible .func  (.param .b32 func_retval0) fminf(
	.param .b32 fminf_param_0,
	.param .b32 fminf_param_1
)
{
	.local .align 4 .b8 	__local_depot35[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<6>;
	.reg .f32 	%f<18>;

	mov.u32 	%SPL, __local_depot35;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f32 	%f8, [fminf_param_1];
	ld.param.f32 	%f7, [fminf_param_0];
	st.f32 	[%SP+4], %f7;
	st.f32 	[%SP+8], %f8;
	ld.f32 	%f9, [%SP+4];
	setp.num.f32 	%p1, %f9, %f9;
	@%p1 bra 	$L__BB35_2;
	bra.uni 	$L__BB35_1;
$L__BB35_1:
	ld.f32 	%f14, [%SP+8];
	st.f32 	[%SP+0], %f14;
	bra.uni 	$L__BB35_13;
$L__BB35_2:
	ld.f32 	%f10, [%SP+8];
	setp.num.f32 	%p2, %f10, %f10;
	@%p2 bra 	$L__BB35_4;
	bra.uni 	$L__BB35_3;
$L__BB35_3:
	ld.f32 	%f13, [%SP+4];
	st.f32 	[%SP+0], %f13;
	bra.uni 	$L__BB35_13;
$L__BB35_4:
	ld.u32 	%r1, [%SP+4];
	shr.u32 	%r2, %r1, 31;
	ld.u32 	%r3, [%SP+8];
	shr.u32 	%r4, %r3, 31;
	setp.eq.s32 	%p3, %r2, %r4;
	@%p3 bra 	$L__BB35_9;
	bra.uni 	$L__BB35_5;
$L__BB35_5:
	ld.u32 	%r5, [%SP+4];
	setp.gt.s32 	%p5, %r5, -1;
	@%p5 bra 	$L__BB35_7;
	bra.uni 	$L__BB35_6;
$L__BB35_6:
	ld.f32 	%f1, [%SP+4];
	mov.f32 	%f16, %f1;
	bra.uni 	$L__BB35_8;
$L__BB35_7:
	ld.f32 	%f2, [%SP+8];
	mov.f32 	%f16, %f2;
	bra.uni 	$L__BB35_8;
$L__BB35_8:
	mov.f32 	%f3, %f16;
	st.f32 	[%SP+0], %f3;
	bra.uni 	$L__BB35_13;
$L__BB35_9:
	ld.f32 	%f11, [%SP+4];
	ld.f32 	%f12, [%SP+8];
	setp.geu.f32 	%p4, %f11, %f12;
	@%p4 bra 	$L__BB35_11;
	bra.uni 	$L__BB35_10;
$L__BB35_10:
	ld.f32 	%f4, [%SP+4];
	mov.f32 	%f17, %f4;
	bra.uni 	$L__BB35_12;
$L__BB35_11:
	ld.f32 	%f5, [%SP+8];
	mov.f32 	%f17, %f5;
	bra.uni 	$L__BB35_12;
$L__BB35_12:
	mov.f32 	%f6, %f17;
	st.f32 	[%SP+0], %f6;
	bra.uni 	$L__BB35_13;
$L__BB35_13:
	ld.f32 	%f15, [%SP+0];
	st.param.f32 	[func_retval0+0], %f15;
	ret;

}
	// .globl	fminl
.visible .func  (.param .b64 func_retval0) fminl(
	.param .b64 fminl_param_0,
	.param .b64 fminl_param_1
)
{
	.local .align 8 .b8 	__local_depot36[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<6>;
	.reg .f64 	%fd<18>;

	mov.u32 	%SPL, __local_depot36;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f64 	%fd8, [fminl_param_1];
	ld.param.f64 	%fd7, [fminl_param_0];
	st.f64 	[%SP+8], %fd7;
	st.f64 	[%SP+16], %fd8;
	ld.f64 	%fd9, [%SP+8];
	setp.num.f64 	%p1, %fd9, %fd9;
	@%p1 bra 	$L__BB36_2;
	bra.uni 	$L__BB36_1;
$L__BB36_1:
	ld.f64 	%fd14, [%SP+16];
	st.f64 	[%SP+0], %fd14;
	bra.uni 	$L__BB36_13;
$L__BB36_2:
	ld.f64 	%fd10, [%SP+16];
	setp.num.f64 	%p2, %fd10, %fd10;
	@%p2 bra 	$L__BB36_4;
	bra.uni 	$L__BB36_3;
$L__BB36_3:
	ld.f64 	%fd13, [%SP+8];
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB36_13;
$L__BB36_4:
	ld.u64 	%rd1, [%SP+8];
	shr.u64 	%rd2, %rd1, 63;
	cvt.u32.u64 	%r1, %rd2;
	ld.u64 	%rd3, [%SP+16];
	shr.u64 	%rd4, %rd3, 63;
	cvt.u32.u64 	%r2, %rd4;
	setp.eq.s32 	%p3, %r1, %r2;
	@%p3 bra 	$L__BB36_9;
	bra.uni 	$L__BB36_5;
$L__BB36_5:
	ld.u64 	%rd5, [%SP+8];
	setp.gt.s64 	%p5, %rd5, -1;
	@%p5 bra 	$L__BB36_7;
	bra.uni 	$L__BB36_6;
$L__BB36_6:
	ld.f64 	%fd1, [%SP+8];
	mov.f64 	%fd16, %fd1;
	bra.uni 	$L__BB36_8;
$L__BB36_7:
	ld.f64 	%fd2, [%SP+16];
	mov.f64 	%fd16, %fd2;
	bra.uni 	$L__BB36_8;
$L__BB36_8:
	mov.f64 	%fd3, %fd16;
	st.f64 	[%SP+0], %fd3;
	bra.uni 	$L__BB36_13;
$L__BB36_9:
	ld.f64 	%fd11, [%SP+8];
	ld.f64 	%fd12, [%SP+16];
	setp.geu.f64 	%p4, %fd11, %fd12;
	@%p4 bra 	$L__BB36_11;
	bra.uni 	$L__BB36_10;
$L__BB36_10:
	ld.f64 	%fd4, [%SP+8];
	mov.f64 	%fd17, %fd4;
	bra.uni 	$L__BB36_12;
$L__BB36_11:
	ld.f64 	%fd5, [%SP+16];
	mov.f64 	%fd17, %fd5;
	bra.uni 	$L__BB36_12;
$L__BB36_12:
	mov.f64 	%fd6, %fd17;
	st.f64 	[%SP+0], %fd6;
	bra.uni 	$L__BB36_13;
$L__BB36_13:
	ld.f64 	%fd15, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd15;
	ret;

}
	// .globl	l64a
.visible .func  (.param .b32 func_retval0) l64a(
	.param .b32 l64a_param_0
)
{
	.local .align 4 .b8 	__local_depot37[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<19>;

	mov.u32 	%SPL, __local_depot37;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [l64a_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+8], %r2;
	mov.u32 	%r3, l64a_$_s;
	cvta.global.u32 	%r4, %r3;
	st.u32 	[%SP+4], %r4;
	bra.uni 	$L__BB37_1;
$L__BB37_1:
	ld.u32 	%r5, [%SP+8];
	setp.eq.s32 	%p1, %r5, 0;
	@%p1 bra 	$L__BB37_4;
	bra.uni 	$L__BB37_2;
$L__BB37_2:
	ld.u32 	%r9, [%SP+8];
	and.b32  	%r10, %r9, 63;
	mov.u32 	%r11, digits;
	cvta.global.u32 	%r12, %r11;
	add.s32 	%r13, %r12, %r10;
	ld.u8 	%rs2, [%r13];
	ld.u32 	%r14, [%SP+4];
	st.u8 	[%r14], %rs2;
	bra.uni 	$L__BB37_3;
$L__BB37_3:
	ld.u32 	%r15, [%SP+4];
	add.s32 	%r16, %r15, 1;
	st.u32 	[%SP+4], %r16;
	ld.u32 	%r17, [%SP+8];
	shr.u32 	%r18, %r17, 6;
	st.u32 	[%SP+8], %r18;
	bra.uni 	$L__BB37_1;
$L__BB37_4:
	ld.u32 	%r6, [%SP+4];
	mov.u16 	%rs1, 0;
	st.u8 	[%r6], %rs1;
	mov.u32 	%r7, l64a_$_s;
	cvta.global.u32 	%r8, %r7;
	st.param.b32 	[func_retval0+0], %r8;
	ret;

}
	// .globl	srand
.visible .func srand(
	.param .b32 srand_param_0
)
{
	.local .align 4 .b8 	__local_depot38[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<2>;

	mov.u32 	%SPL, __local_depot38;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [srand_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -1;
	cvt.u64.u32 	%rd1, %r3;
	mov.u32 	%r4, seed;
	cvta.global.u32 	%r5, %r4;
	st.u64 	[%r5], %rd1;
	ret;

}
	// .globl	rand
.visible .func  (.param .b32 func_retval0) rand()
{
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<6>;

	mov.u32 	%r1, seed;
	cvta.global.u32 	%r2, %r1;
	ld.u64 	%rd1, [%r2];
	mul.lo.s64 	%rd2, %rd1, 6364136223846793005;
	add.s64 	%rd3, %rd2, 1;
	st.u64 	[%r2], %rd3;
	ld.u64 	%rd4, [%r2];
	shr.u64 	%rd5, %rd4, 33;
	cvt.u32.u64 	%r3, %rd5;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	insque
.visible .func insque(
	.param .b32 insque_param_0,
	.param .b32 insque_param_1
)
{
	.local .align 4 .b8 	__local_depot40[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b32 	%r<20>;

	mov.u32 	%SPL, __local_depot40;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [insque_param_1];
	ld.param.u32 	%r1, [insque_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+4];
	st.u32 	[%SP+12], %r4;
	ld.u32 	%r5, [%SP+12];
	setp.ne.s32 	%p1, %r5, 0;
	@%p1 bra 	$L__BB40_2;
	bra.uni 	$L__BB40_1;
$L__BB40_1:
	ld.u32 	%r6, [%SP+8];
	mov.b32 	%r7, 0;
	st.u32 	[%r6+4], %r7;
	ld.u32 	%r8, [%SP+8];
	st.u32 	[%r8], %r7;
	bra.uni 	$L__BB40_4;
$L__BB40_2:
	ld.u32 	%r9, [%SP+12];
	ld.u32 	%r10, [%r9];
	ld.u32 	%r11, [%SP+8];
	st.u32 	[%r11], %r10;
	ld.u32 	%r12, [%SP+12];
	ld.u32 	%r13, [%SP+8];
	st.u32 	[%r13+4], %r12;
	ld.u32 	%r14, [%SP+8];
	ld.u32 	%r15, [%SP+12];
	st.u32 	[%r15], %r14;
	ld.u32 	%r16, [%SP+8];
	ld.u32 	%r17, [%r16];
	setp.eq.s32 	%p2, %r17, 0;
	@%p2 bra 	$L__BB40_4;
	bra.uni 	$L__BB40_3;
$L__BB40_3:
	ld.u32 	%r18, [%SP+8];
	ld.u32 	%r19, [%r18];
	st.u32 	[%r19+4], %r18;
	bra.uni 	$L__BB40_4;
$L__BB40_4:
	ret;

}
	// .globl	remque
.visible .func remque(
	.param .b32 remque_param_0
)
{
	.local .align 4 .b8 	__local_depot41[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;

	mov.u32 	%SPL, __local_depot41;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [remque_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+4];
	ld.u32 	%r4, [%r3];
	setp.eq.s32 	%p1, %r4, 0;
	@%p1 bra 	$L__BB41_2;
	bra.uni 	$L__BB41_1;
$L__BB41_1:
	ld.u32 	%r5, [%SP+4];
	ld.u32 	%r6, [%r5+4];
	ld.u32 	%r7, [%r5];
	st.u32 	[%r7+4], %r6;
	bra.uni 	$L__BB41_2;
$L__BB41_2:
	ld.u32 	%r8, [%SP+4];
	ld.u32 	%r9, [%r8+4];
	setp.eq.s32 	%p2, %r9, 0;
	@%p2 bra 	$L__BB41_4;
	bra.uni 	$L__BB41_3;
$L__BB41_3:
	ld.u32 	%r10, [%SP+4];
	ld.u32 	%r11, [%r10];
	ld.u32 	%r12, [%r10+4];
	st.u32 	[%r12], %r11;
	bra.uni 	$L__BB41_4;
$L__BB41_4:
	ret;

}
	// .globl	abs
.visible .func  (.param .b32 func_retval0) abs(
	.param .b32 abs_param_0
)
{
	.local .align 4 .b8 	__local_depot42[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;

	mov.u32 	%SPL, __local_depot42;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r4, [abs_param_0];
	st.u32 	[%SP+0], %r4;
	ld.u32 	%r5, [%SP+0];
	setp.lt.s32 	%p1, %r5, 1;
	@%p1 bra 	$L__BB42_2;
	bra.uni 	$L__BB42_1;
$L__BB42_1:
	ld.u32 	%r1, [%SP+0];
	mov.u32 	%r7, %r1;
	bra.uni 	$L__BB42_3;
$L__BB42_2:
	ld.u32 	%r6, [%SP+0];
	neg.s32 	%r2, %r6;
	mov.u32 	%r7, %r2;
	bra.uni 	$L__BB42_3;
$L__BB42_3:
	mov.u32 	%r3, %r7;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	atoi
.visible .func  (.param .b32 func_retval0) atoi(
	.param .b32 atoi_param_0
)
{
	.local .align 4 .b8 	__local_depot43[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<31>;

	mov.u32 	%SPL, __local_depot43;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r4, [atoi_param_0];
	st.u32 	[%SP+0], %r4;
	mov.b32 	%r5, 0;
	st.u32 	[%SP+4], %r5;
	st.u32 	[%SP+8], %r5;
	bra.uni 	$L__BB43_1;
$L__BB43_1:
	ld.u32 	%r6, [%SP+0];
	ld.s8 	%r7, [%r6];
	{ // callseq 0, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r7;
	.param .b32 retval0;
	call.uni (retval0), 
	isspace, 
	(
	param0
	);
	ld.param.b32 	%r8, [retval0+0];
	} // callseq 0
	setp.eq.s32 	%p1, %r8, 0;
	@%p1 bra 	$L__BB43_3;
	bra.uni 	$L__BB43_2;
$L__BB43_2:
	ld.u32 	%r28, [%SP+0];
	add.s32 	%r29, %r28, 1;
	st.u32 	[%SP+0], %r29;
	bra.uni 	$L__BB43_1;
$L__BB43_3:
	ld.u32 	%r11, [%SP+0];
	ld.s8 	%r10, [%r11];
	setp.eq.s32 	%p2, %r10, 43;
	@%p2 bra 	$L__BB43_5;
	bra.uni 	$L__BB43_13;
$L__BB43_13:
	setp.ne.s32 	%p3, %r10, 45;
	@%p3 bra 	$L__BB43_6;
	bra.uni 	$L__BB43_4;
$L__BB43_4:
	mov.b32 	%r12, 1;
	st.u32 	[%SP+8], %r12;
	bra.uni 	$L__BB43_5;
$L__BB43_5:
	ld.u32 	%r13, [%SP+0];
	add.s32 	%r14, %r13, 1;
	st.u32 	[%SP+0], %r14;
	bra.uni 	$L__BB43_6;
$L__BB43_6:
	bra.uni 	$L__BB43_7;
$L__BB43_7:
	ld.u32 	%r15, [%SP+0];
	ld.s8 	%r16, [%r15];
	{ // callseq 1, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r16;
	.param .b32 retval0;
	call.uni (retval0), 
	isdigit, 
	(
	param0
	);
	ld.param.b32 	%r17, [retval0+0];
	} // callseq 1
	setp.eq.s32 	%p4, %r17, 0;
	@%p4 bra 	$L__BB43_9;
	bra.uni 	$L__BB43_8;
$L__BB43_8:
	ld.u32 	%r21, [%SP+4];
	mul.lo.s32 	%r22, %r21, 10;
	ld.u32 	%r23, [%SP+0];
	add.s32 	%r24, %r23, 1;
	st.u32 	[%SP+0], %r24;
	ld.s8 	%r25, [%r23];
	sub.s32 	%r26, %r22, %r25;
	add.s32 	%r27, %r26, 48;
	st.u32 	[%SP+4], %r27;
	bra.uni 	$L__BB43_7;
$L__BB43_9:
	ld.u32 	%r19, [%SP+8];
	setp.eq.s32 	%p5, %r19, 0;
	@%p5 bra 	$L__BB43_11;
	bra.uni 	$L__BB43_10;
$L__BB43_10:
	ld.u32 	%r1, [%SP+4];
	mov.u32 	%r30, %r1;
	bra.uni 	$L__BB43_12;
$L__BB43_11:
	ld.u32 	%r20, [%SP+4];
	neg.s32 	%r2, %r20;
	mov.u32 	%r30, %r2;
	bra.uni 	$L__BB43_12;
$L__BB43_12:
	mov.u32 	%r3, %r30;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	atol
.visible .func  (.param .b32 func_retval0) atol(
	.param .b32 atol_param_0
)
{
	.local .align 4 .b8 	__local_depot44[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<31>;

	mov.u32 	%SPL, __local_depot44;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r4, [atol_param_0];
	st.u32 	[%SP+0], %r4;
	mov.b32 	%r5, 0;
	st.u32 	[%SP+4], %r5;
	st.u32 	[%SP+8], %r5;
	bra.uni 	$L__BB44_1;
$L__BB44_1:
	ld.u32 	%r6, [%SP+0];
	ld.s8 	%r7, [%r6];
	{ // callseq 2, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r7;
	.param .b32 retval0;
	call.uni (retval0), 
	isspace, 
	(
	param0
	);
	ld.param.b32 	%r8, [retval0+0];
	} // callseq 2
	setp.eq.s32 	%p1, %r8, 0;
	@%p1 bra 	$L__BB44_3;
	bra.uni 	$L__BB44_2;
$L__BB44_2:
	ld.u32 	%r28, [%SP+0];
	add.s32 	%r29, %r28, 1;
	st.u32 	[%SP+0], %r29;
	bra.uni 	$L__BB44_1;
$L__BB44_3:
	ld.u32 	%r11, [%SP+0];
	ld.s8 	%r10, [%r11];
	setp.eq.s32 	%p2, %r10, 43;
	@%p2 bra 	$L__BB44_5;
	bra.uni 	$L__BB44_13;
$L__BB44_13:
	setp.ne.s32 	%p3, %r10, 45;
	@%p3 bra 	$L__BB44_6;
	bra.uni 	$L__BB44_4;
$L__BB44_4:
	mov.b32 	%r12, 1;
	st.u32 	[%SP+8], %r12;
	bra.uni 	$L__BB44_5;
$L__BB44_5:
	ld.u32 	%r13, [%SP+0];
	add.s32 	%r14, %r13, 1;
	st.u32 	[%SP+0], %r14;
	bra.uni 	$L__BB44_6;
$L__BB44_6:
	bra.uni 	$L__BB44_7;
$L__BB44_7:
	ld.u32 	%r15, [%SP+0];
	ld.s8 	%r16, [%r15];
	{ // callseq 3, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r16;
	.param .b32 retval0;
	call.uni (retval0), 
	isdigit, 
	(
	param0
	);
	ld.param.b32 	%r17, [retval0+0];
	} // callseq 3
	setp.eq.s32 	%p4, %r17, 0;
	@%p4 bra 	$L__BB44_9;
	bra.uni 	$L__BB44_8;
$L__BB44_8:
	ld.u32 	%r21, [%SP+4];
	mul.lo.s32 	%r22, %r21, 10;
	ld.u32 	%r23, [%SP+0];
	add.s32 	%r24, %r23, 1;
	st.u32 	[%SP+0], %r24;
	ld.s8 	%r25, [%r23];
	sub.s32 	%r26, %r22, %r25;
	add.s32 	%r27, %r26, 48;
	st.u32 	[%SP+4], %r27;
	bra.uni 	$L__BB44_7;
$L__BB44_9:
	ld.u32 	%r19, [%SP+8];
	setp.eq.s32 	%p5, %r19, 0;
	@%p5 bra 	$L__BB44_11;
	bra.uni 	$L__BB44_10;
$L__BB44_10:
	ld.u32 	%r1, [%SP+4];
	mov.u32 	%r30, %r1;
	bra.uni 	$L__BB44_12;
$L__BB44_11:
	ld.u32 	%r20, [%SP+4];
	neg.s32 	%r2, %r20;
	mov.u32 	%r30, %r2;
	bra.uni 	$L__BB44_12;
$L__BB44_12:
	mov.u32 	%r3, %r30;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	atoll
.visible .func  (.param .b64 func_retval0) atoll(
	.param .b32 atoll_param_0
)
{
	.local .align 8 .b8 	__local_depot45[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<11>;

	mov.u32 	%SPL, __local_depot45;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [atoll_param_0];
	st.u32 	[%SP+0], %r1;
	mov.u64 	%rd4, 0;
	st.u64 	[%SP+8], %rd4;
	mov.b32 	%r2, 0;
	st.u32 	[%SP+16], %r2;
	bra.uni 	$L__BB45_1;
$L__BB45_1:
	ld.u32 	%r3, [%SP+0];
	ld.s8 	%r4, [%r3];
	{ // callseq 4, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r4;
	.param .b32 retval0;
	call.uni (retval0), 
	isspace, 
	(
	param0
	);
	ld.param.b32 	%r5, [retval0+0];
	} // callseq 4
	setp.eq.s32 	%p1, %r5, 0;
	@%p1 bra 	$L__BB45_3;
	bra.uni 	$L__BB45_2;
$L__BB45_2:
	ld.u32 	%r21, [%SP+0];
	add.s32 	%r22, %r21, 1;
	st.u32 	[%SP+0], %r22;
	bra.uni 	$L__BB45_1;
$L__BB45_3:
	ld.u32 	%r8, [%SP+0];
	ld.s8 	%r7, [%r8];
	setp.eq.s32 	%p2, %r7, 43;
	@%p2 bra 	$L__BB45_5;
	bra.uni 	$L__BB45_13;
$L__BB45_13:
	setp.ne.s32 	%p3, %r7, 45;
	@%p3 bra 	$L__BB45_6;
	bra.uni 	$L__BB45_4;
$L__BB45_4:
	mov.b32 	%r9, 1;
	st.u32 	[%SP+16], %r9;
	bra.uni 	$L__BB45_5;
$L__BB45_5:
	ld.u32 	%r10, [%SP+0];
	add.s32 	%r11, %r10, 1;
	st.u32 	[%SP+0], %r11;
	bra.uni 	$L__BB45_6;
$L__BB45_6:
	bra.uni 	$L__BB45_7;
$L__BB45_7:
	ld.u32 	%r12, [%SP+0];
	ld.s8 	%r13, [%r12];
	{ // callseq 5, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r13;
	.param .b32 retval0;
	call.uni (retval0), 
	isdigit, 
	(
	param0
	);
	ld.param.b32 	%r14, [retval0+0];
	} // callseq 5
	setp.eq.s32 	%p4, %r14, 0;
	@%p4 bra 	$L__BB45_9;
	bra.uni 	$L__BB45_8;
$L__BB45_8:
	ld.u64 	%rd6, [%SP+8];
	mul.lo.s64 	%rd7, %rd6, 10;
	ld.u32 	%r17, [%SP+0];
	add.s32 	%r18, %r17, 1;
	st.u32 	[%SP+0], %r18;
	ld.s8 	%r19, [%r17];
	add.s32 	%r20, %r19, -48;
	cvt.s64.s32 	%rd8, %r20;
	sub.s64 	%rd9, %rd7, %rd8;
	st.u64 	[%SP+8], %rd9;
	bra.uni 	$L__BB45_7;
$L__BB45_9:
	ld.u32 	%r16, [%SP+16];
	setp.eq.s32 	%p5, %r16, 0;
	@%p5 bra 	$L__BB45_11;
	bra.uni 	$L__BB45_10;
$L__BB45_10:
	ld.u64 	%rd1, [%SP+8];
	mov.u64 	%rd10, %rd1;
	bra.uni 	$L__BB45_12;
$L__BB45_11:
	ld.u64 	%rd5, [%SP+8];
	neg.s64 	%rd2, %rd5;
	mov.u64 	%rd10, %rd2;
	bra.uni 	$L__BB45_12;
$L__BB45_12:
	mov.u64 	%rd3, %rd10;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	bsearch
.visible .func  (.param .b32 func_retval0) bsearch(
	.param .b32 bsearch_param_0,
	.param .b32 bsearch_param_1,
	.param .b32 bsearch_param_2,
	.param .b32 bsearch_param_3,
	.param .b32 bsearch_param_4
)
{
	.local .align 4 .b8 	__local_depot46[32];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<32>;

	mov.u32 	%SPL, __local_depot46;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r5, [bsearch_param_4];
	ld.param.u32 	%r4, [bsearch_param_3];
	ld.param.u32 	%r3, [bsearch_param_2];
	ld.param.u32 	%r2, [bsearch_param_1];
	ld.param.u32 	%r1, [bsearch_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u32 	[%SP+12], %r3;
	st.u32 	[%SP+16], %r4;
	st.u32 	[%SP+20], %r5;
	bra.uni 	$L__BB46_1;
$L__BB46_1:
	ld.u32 	%r6, [%SP+12];
	setp.eq.s32 	%p1, %r6, 0;
	@%p1 bra 	$L__BB46_9;
	bra.uni 	$L__BB46_2;
$L__BB46_2:
	ld.u32 	%r8, [%SP+8];
	ld.u32 	%r9, [%SP+16];
	ld.u32 	%r10, [%SP+12];
	shr.u32 	%r11, %r10, 1;
	mul.lo.s32 	%r12, %r9, %r11;
	add.s32 	%r13, %r8, %r12;
	st.u32 	[%SP+24], %r13;
	ld.u32 	%r14, [%SP+20];
	ld.u32 	%r15, [%SP+4];
	ld.u32 	%r16, [%SP+24];
	{ // callseq 6, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r15;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r16;
	.param .b32 retval0;
	prototype_6 : .callprototype (.param .b32 _) _ (.param .b32 _, .param .b32 _);
	call (retval0), 
	%r14, 
	(
	param0, 
	param1
	)
	, prototype_6;
	ld.param.b32 	%r17, [retval0+0];
	} // callseq 6
	st.u32 	[%SP+28], %r17;
	ld.u32 	%r19, [%SP+28];
	setp.gt.s32 	%p2, %r19, -1;
	@%p2 bra 	$L__BB46_4;
	bra.uni 	$L__BB46_3;
$L__BB46_3:
	ld.u32 	%r30, [%SP+12];
	shr.u32 	%r31, %r30, 1;
	st.u32 	[%SP+12], %r31;
	bra.uni 	$L__BB46_8;
$L__BB46_4:
	ld.u32 	%r20, [%SP+28];
	setp.lt.s32 	%p3, %r20, 1;
	@%p3 bra 	$L__BB46_6;
	bra.uni 	$L__BB46_5;
$L__BB46_5:
	ld.u32 	%r23, [%SP+24];
	ld.u32 	%r24, [%SP+16];
	add.s32 	%r25, %r23, %r24;
	st.u32 	[%SP+8], %r25;
	ld.u32 	%r26, [%SP+12];
	shr.u32 	%r27, %r26, 1;
	not.b32 	%r28, %r27;
	add.s32 	%r29, %r28, %r26;
	st.u32 	[%SP+12], %r29;
	bra.uni 	$L__BB46_7;
$L__BB46_6:
	ld.u32 	%r21, [%SP+24];
	st.u32 	[%SP+0], %r21;
	bra.uni 	$L__BB46_10;
$L__BB46_7:
	bra.uni 	$L__BB46_8;
$L__BB46_8:
	bra.uni 	$L__BB46_1;
$L__BB46_9:
	mov.b32 	%r7, 0;
	st.u32 	[%SP+0], %r7;
	bra.uni 	$L__BB46_10;
$L__BB46_10:
	ld.u32 	%r22, [%SP+0];
	st.param.b32 	[func_retval0+0], %r22;
	ret;

}
	// .globl	bsearch_r
.visible .func  (.param .b32 func_retval0) bsearch_r(
	.param .b32 bsearch_r_param_0,
	.param .b32 bsearch_r_param_1,
	.param .b32 bsearch_r_param_2,
	.param .b32 bsearch_r_param_3,
	.param .b32 bsearch_r_param_4,
	.param .b32 bsearch_r_param_5
)
{
	.local .align 4 .b8 	__local_depot47[44];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<34>;

	mov.u32 	%SPL, __local_depot47;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r6, [bsearch_r_param_5];
	ld.param.u32 	%r5, [bsearch_r_param_4];
	ld.param.u32 	%r4, [bsearch_r_param_3];
	ld.param.u32 	%r3, [bsearch_r_param_2];
	ld.param.u32 	%r2, [bsearch_r_param_1];
	ld.param.u32 	%r1, [bsearch_r_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u32 	[%SP+12], %r3;
	st.u32 	[%SP+16], %r4;
	st.u32 	[%SP+20], %r5;
	st.u32 	[%SP+24], %r6;
	ld.u32 	%r7, [%SP+8];
	st.u32 	[%SP+28], %r7;
	ld.u32 	%r8, [%SP+12];
	st.u32 	[%SP+32], %r8;
	bra.uni 	$L__BB47_1;
$L__BB47_1:
	ld.u32 	%r9, [%SP+32];
	setp.eq.s32 	%p1, %r9, 0;
	@%p1 bra 	$L__BB47_8;
	bra.uni 	$L__BB47_2;
$L__BB47_2:
	ld.u32 	%r11, [%SP+28];
	ld.u32 	%r12, [%SP+32];
	shr.s32 	%r13, %r12, 1;
	ld.u32 	%r14, [%SP+16];
	mul.lo.s32 	%r15, %r13, %r14;
	add.s32 	%r16, %r11, %r15;
	st.u32 	[%SP+40], %r16;
	ld.u32 	%r17, [%SP+20];
	ld.u32 	%r18, [%SP+4];
	ld.u32 	%r19, [%SP+40];
	ld.u32 	%r20, [%SP+24];
	{ // callseq 7, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r18;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r19;
	.param .b32 param2;
	st.param.b32 	[param2+0], %r20;
	.param .b32 retval0;
	prototype_7 : .callprototype (.param .b32 _) _ (.param .b32 _, .param .b32 _, .param .b32 _);
	call (retval0), 
	%r17, 
	(
	param0, 
	param1, 
	param2
	)
	, prototype_7;
	ld.param.b32 	%r21, [retval0+0];
	} // callseq 7
	st.u32 	[%SP+36], %r21;
	ld.u32 	%r23, [%SP+36];
	setp.ne.s32 	%p2, %r23, 0;
	@%p2 bra 	$L__BB47_4;
	bra.uni 	$L__BB47_3;
$L__BB47_3:
	ld.u32 	%r32, [%SP+40];
	st.u32 	[%SP+0], %r32;
	bra.uni 	$L__BB47_9;
$L__BB47_4:
	ld.u32 	%r24, [%SP+36];
	setp.lt.s32 	%p3, %r24, 1;
	@%p3 bra 	$L__BB47_6;
	bra.uni 	$L__BB47_5;
$L__BB47_5:
	ld.u32 	%r25, [%SP+40];
	ld.u32 	%r26, [%SP+16];
	add.s32 	%r27, %r25, %r26;
	st.u32 	[%SP+28], %r27;
	ld.u32 	%r28, [%SP+32];
	add.s32 	%r29, %r28, -1;
	st.u32 	[%SP+32], %r29;
	bra.uni 	$L__BB47_6;
$L__BB47_6:
	bra.uni 	$L__BB47_7;
$L__BB47_7:
	ld.u32 	%r30, [%SP+32];
	shr.s32 	%r31, %r30, 1;
	st.u32 	[%SP+32], %r31;
	bra.uni 	$L__BB47_1;
$L__BB47_8:
	mov.b32 	%r10, 0;
	st.u32 	[%SP+0], %r10;
	bra.uni 	$L__BB47_9;
$L__BB47_9:
	ld.u32 	%r33, [%SP+0];
	st.param.b32 	[func_retval0+0], %r33;
	ret;

}
	// .globl	div
.visible .func  (.param .align 4 .b8 func_retval0[8]) div(
	.param .b32 div_param_0,
	.param .b32 div_param_1
)
{
	.local .align 4 .b8 	__local_depot48[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<11>;

	mov.u32 	%SPL, __local_depot48;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [div_param_1];
	ld.param.u32 	%r1, [div_param_0];
	st.u32 	[%SP+8], %r1;
	st.u32 	[%SP+12], %r2;
	ld.u32 	%r3, [%SP+8];
	ld.u32 	%r4, [%SP+12];
	div.s32 	%r5, %r3, %r4;
	st.u32 	[%SP+0], %r5;
	ld.u32 	%r6, [%SP+8];
	ld.u32 	%r7, [%SP+12];
	rem.s32 	%r8, %r6, %r7;
	st.u32 	[%SP+4], %r8;
	ld.u32 	%r9, [%SP+4];
	ld.u32 	%r10, [%SP+0];
	st.param.b32 	[func_retval0+0], %r10;
	st.param.b32 	[func_retval0+4], %r9;
	ret;

}
	// .globl	imaxabs
.visible .func  (.param .b64 func_retval0) imaxabs(
	.param .b64 imaxabs_param_0
)
{
	.local .align 8 .b8 	__local_depot49[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b64 	%rd<8>;

	mov.u32 	%SPL, __local_depot49;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd4, [imaxabs_param_0];
	st.u64 	[%SP+0], %rd4;
	ld.u64 	%rd5, [%SP+0];
	setp.lt.s64 	%p1, %rd5, 1;
	@%p1 bra 	$L__BB49_2;
	bra.uni 	$L__BB49_1;
$L__BB49_1:
	ld.u64 	%rd1, [%SP+0];
	mov.u64 	%rd7, %rd1;
	bra.uni 	$L__BB49_3;
$L__BB49_2:
	ld.u64 	%rd6, [%SP+0];
	neg.s64 	%rd2, %rd6;
	mov.u64 	%rd7, %rd2;
	bra.uni 	$L__BB49_3;
$L__BB49_3:
	mov.u64 	%rd3, %rd7;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	imaxdiv
.visible .func  (.param .align 8 .b8 func_retval0[16]) imaxdiv(
	.param .b64 imaxdiv_param_0,
	.param .b64 imaxdiv_param_1
)
{
	.local .align 8 .b8 	__local_depot50[32];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b64 	%rd<11>;

	mov.u32 	%SPL, __local_depot50;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd2, [imaxdiv_param_1];
	ld.param.u64 	%rd1, [imaxdiv_param_0];
	st.u64 	[%SP+16], %rd1;
	st.u64 	[%SP+24], %rd2;
	ld.u64 	%rd3, [%SP+16];
	ld.u64 	%rd4, [%SP+24];
	div.s64 	%rd5, %rd3, %rd4;
	st.u64 	[%SP+0], %rd5;
	ld.u64 	%rd6, [%SP+16];
	ld.u64 	%rd7, [%SP+24];
	rem.s64 	%rd8, %rd6, %rd7;
	st.u64 	[%SP+8], %rd8;
	ld.u64 	%rd9, [%SP+8];
	ld.u64 	%rd10, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd10;
	st.param.b64 	[func_retval0+8], %rd9;
	ret;

}
	// .globl	labs
.visible .func  (.param .b32 func_retval0) labs(
	.param .b32 labs_param_0
)
{
	.local .align 4 .b8 	__local_depot51[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;

	mov.u32 	%SPL, __local_depot51;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r4, [labs_param_0];
	st.u32 	[%SP+0], %r4;
	ld.u32 	%r5, [%SP+0];
	setp.lt.s32 	%p1, %r5, 1;
	@%p1 bra 	$L__BB51_2;
	bra.uni 	$L__BB51_1;
$L__BB51_1:
	ld.u32 	%r1, [%SP+0];
	mov.u32 	%r7, %r1;
	bra.uni 	$L__BB51_3;
$L__BB51_2:
	ld.u32 	%r6, [%SP+0];
	neg.s32 	%r2, %r6;
	mov.u32 	%r7, %r2;
	bra.uni 	$L__BB51_3;
$L__BB51_3:
	mov.u32 	%r3, %r7;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	ldiv
.visible .func  (.param .align 4 .b8 func_retval0[8]) ldiv(
	.param .b32 ldiv_param_0,
	.param .b32 ldiv_param_1
)
{
	.local .align 4 .b8 	__local_depot52[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<11>;

	mov.u32 	%SPL, __local_depot52;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [ldiv_param_1];
	ld.param.u32 	%r1, [ldiv_param_0];
	st.u32 	[%SP+8], %r1;
	st.u32 	[%SP+12], %r2;
	ld.u32 	%r3, [%SP+8];
	ld.u32 	%r4, [%SP+12];
	div.s32 	%r5, %r3, %r4;
	st.u32 	[%SP+0], %r5;
	ld.u32 	%r6, [%SP+8];
	ld.u32 	%r7, [%SP+12];
	rem.s32 	%r8, %r6, %r7;
	st.u32 	[%SP+4], %r8;
	ld.u32 	%r9, [%SP+4];
	ld.u32 	%r10, [%SP+0];
	st.param.b32 	[func_retval0+0], %r10;
	st.param.b32 	[func_retval0+4], %r9;
	ret;

}
	// .globl	llabs
.visible .func  (.param .b64 func_retval0) llabs(
	.param .b64 llabs_param_0
)
{
	.local .align 8 .b8 	__local_depot53[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b64 	%rd<8>;

	mov.u32 	%SPL, __local_depot53;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd4, [llabs_param_0];
	st.u64 	[%SP+0], %rd4;
	ld.u64 	%rd5, [%SP+0];
	setp.lt.s64 	%p1, %rd5, 1;
	@%p1 bra 	$L__BB53_2;
	bra.uni 	$L__BB53_1;
$L__BB53_1:
	ld.u64 	%rd1, [%SP+0];
	mov.u64 	%rd7, %rd1;
	bra.uni 	$L__BB53_3;
$L__BB53_2:
	ld.u64 	%rd6, [%SP+0];
	neg.s64 	%rd2, %rd6;
	mov.u64 	%rd7, %rd2;
	bra.uni 	$L__BB53_3;
$L__BB53_3:
	mov.u64 	%rd3, %rd7;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	lldiv
.visible .func  (.param .align 8 .b8 func_retval0[16]) lldiv(
	.param .b64 lldiv_param_0,
	.param .b64 lldiv_param_1
)
{
	.local .align 8 .b8 	__local_depot54[32];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b64 	%rd<11>;

	mov.u32 	%SPL, __local_depot54;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd2, [lldiv_param_1];
	ld.param.u64 	%rd1, [lldiv_param_0];
	st.u64 	[%SP+16], %rd1;
	st.u64 	[%SP+24], %rd2;
	ld.u64 	%rd3, [%SP+16];
	ld.u64 	%rd4, [%SP+24];
	div.s64 	%rd5, %rd3, %rd4;
	st.u64 	[%SP+0], %rd5;
	ld.u64 	%rd6, [%SP+16];
	ld.u64 	%rd7, [%SP+24];
	rem.s64 	%rd8, %rd6, %rd7;
	st.u64 	[%SP+8], %rd8;
	ld.u64 	%rd9, [%SP+8];
	ld.u64 	%rd10, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd10;
	st.param.b64 	[func_retval0+8], %rd9;
	ret;

}
	// .globl	wcschr
.visible .func  (.param .b32 func_retval0) wcschr(
	.param .b32 wcschr_param_0,
	.param .b32 wcschr_param_1
)
{
	.local .align 4 .b8 	__local_depot55[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<16>;

	mov.u32 	%SPL, __local_depot55;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r4, [wcschr_param_1];
	ld.param.u32 	%r3, [wcschr_param_0];
	st.u32 	[%SP+0], %r3;
	st.u32 	[%SP+4], %r4;
	bra.uni 	$L__BB55_1;
$L__BB55_1:
	ld.u32 	%r5, [%SP+0];
	ld.u32 	%r6, [%r5];
	setp.eq.s32 	%p4, %r6, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB55_3;
	bra.uni 	$L__BB55_2;
$L__BB55_2:
	ld.u32 	%r7, [%SP+0];
	ld.u32 	%r8, [%r7];
	ld.u32 	%r9, [%SP+4];
	setp.ne.s32 	%p1, %r8, %r9;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB55_3;
$L__BB55_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB55_6;
	bra.uni 	$L__BB55_4;
$L__BB55_4:
	bra.uni 	$L__BB55_5;
$L__BB55_5:
	ld.u32 	%r13, [%SP+0];
	add.s32 	%r14, %r13, 4;
	st.u32 	[%SP+0], %r14;
	bra.uni 	$L__BB55_1;
$L__BB55_6:
	ld.u32 	%r10, [%SP+0];
	ld.u32 	%r11, [%r10];
	setp.eq.s32 	%p5, %r11, 0;
	@%p5 bra 	$L__BB55_8;
	bra.uni 	$L__BB55_7;
$L__BB55_7:
	ld.u32 	%r1, [%SP+0];
	mov.u32 	%r15, %r1;
	bra.uni 	$L__BB55_9;
$L__BB55_8:
	mov.b32 	%r12, 0;
	mov.u32 	%r15, %r12;
	bra.uni 	$L__BB55_9;
$L__BB55_9:
	mov.u32 	%r2, %r15;
	st.param.b32 	[func_retval0+0], %r2;
	ret;

}
	// .globl	wcscmp
.visible .func  (.param .b32 func_retval0) wcscmp(
	.param .b32 wcscmp_param_0,
	.param .b32 wcscmp_param_1
)
{
	.local .align 4 .b8 	__local_depot56[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<27>;

	mov.u32 	%SPL, __local_depot56;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r4, [wcscmp_param_1];
	ld.param.u32 	%r3, [wcscmp_param_0];
	st.u32 	[%SP+0], %r3;
	st.u32 	[%SP+4], %r4;
	bra.uni 	$L__BB56_1;
$L__BB56_1:
	ld.u32 	%r5, [%SP+0];
	ld.u32 	%r6, [%r5];
	ld.u32 	%r7, [%SP+4];
	ld.u32 	%r8, [%r7];
	setp.ne.s32 	%p4, %r6, %r8;
	mov.pred 	%p3, 0;
	mov.pred 	%p9, %p3;
	@%p4 bra 	$L__BB56_4;
	bra.uni 	$L__BB56_2;
$L__BB56_2:
	ld.u32 	%r9, [%SP+0];
	ld.u32 	%r10, [%r9];
	setp.eq.s32 	%p6, %r10, 0;
	mov.pred 	%p5, 0;
	mov.pred 	%p9, %p5;
	@%p6 bra 	$L__BB56_4;
	bra.uni 	$L__BB56_3;
$L__BB56_3:
	ld.u32 	%r11, [%SP+4];
	ld.u32 	%r12, [%r11];
	setp.ne.s32 	%p1, %r12, 0;
	mov.pred 	%p9, %p1;
	bra.uni 	$L__BB56_4;
$L__BB56_4:
	mov.pred 	%p2, %p9;
	@!%p2 bra 	$L__BB56_7;
	bra.uni 	$L__BB56_5;
$L__BB56_5:
	bra.uni 	$L__BB56_6;
$L__BB56_6:
	ld.u32 	%r22, [%SP+0];
	add.s32 	%r23, %r22, 4;
	st.u32 	[%SP+0], %r23;
	ld.u32 	%r24, [%SP+4];
	add.s32 	%r25, %r24, 4;
	st.u32 	[%SP+4], %r25;
	bra.uni 	$L__BB56_1;
$L__BB56_7:
	ld.u32 	%r13, [%SP+0];
	ld.u32 	%r14, [%r13];
	ld.u32 	%r15, [%SP+4];
	ld.u32 	%r16, [%r15];
	setp.ge.s32 	%p7, %r14, %r16;
	@%p7 bra 	$L__BB56_9;
	bra.uni 	$L__BB56_8;
$L__BB56_8:
	mov.b32 	%r21, -1;
	mov.u32 	%r26, %r21;
	bra.uni 	$L__BB56_10;
$L__BB56_9:
	ld.u32 	%r17, [%SP+0];
	ld.u32 	%r18, [%r17];
	ld.u32 	%r19, [%SP+4];
	ld.u32 	%r20, [%r19];
	setp.gt.s32 	%p8, %r18, %r20;
	selp.u32 	%r1, 1, 0, %p8;
	mov.u32 	%r26, %r1;
	bra.uni 	$L__BB56_10;
$L__BB56_10:
	mov.u32 	%r2, %r26;
	st.param.b32 	[func_retval0+0], %r2;
	ret;

}
	// .globl	wcscpy
.visible .func  (.param .b32 func_retval0) wcscpy(
	.param .b32 wcscpy_param_0,
	.param .b32 wcscpy_param_1
)
{
	.local .align 4 .b8 	__local_depot57[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<10>;

	mov.u32 	%SPL, __local_depot57;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [wcscpy_param_1];
	ld.param.u32 	%r1, [wcscpy_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	st.u32 	[%SP+8], %r3;
	bra.uni 	$L__BB57_1;
$L__BB57_1:
	ld.u32 	%r4, [%SP+4];
	add.s32 	%r5, %r4, 4;
	st.u32 	[%SP+4], %r5;
	ld.u32 	%r6, [%r4];
	ld.u32 	%r7, [%SP+0];
	add.s32 	%r8, %r7, 4;
	st.u32 	[%SP+0], %r8;
	st.u32 	[%r7], %r6;
	setp.eq.s32 	%p1, %r6, 0;
	@%p1 bra 	$L__BB57_3;
	bra.uni 	$L__BB57_2;
$L__BB57_2:
	bra.uni 	$L__BB57_1;
$L__BB57_3:
	ld.u32 	%r9, [%SP+8];
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	wcslen
.visible .func  (.param .b32 func_retval0) wcslen(
	.param .b32 wcslen_param_0
)
{
	.local .align 4 .b8 	__local_depot58[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<11>;

	mov.u32 	%SPL, __local_depot58;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [wcslen_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+4], %r2;
	bra.uni 	$L__BB58_1;
$L__BB58_1:
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%r3];
	setp.eq.s32 	%p1, %r4, 0;
	@%p1 bra 	$L__BB58_4;
	bra.uni 	$L__BB58_2;
$L__BB58_2:
	bra.uni 	$L__BB58_3;
$L__BB58_3:
	ld.u32 	%r9, [%SP+0];
	add.s32 	%r10, %r9, 4;
	st.u32 	[%SP+0], %r10;
	bra.uni 	$L__BB58_1;
$L__BB58_4:
	ld.u32 	%r5, [%SP+0];
	ld.u32 	%r6, [%SP+4];
	sub.s32 	%r7, %r5, %r6;
	shr.s32 	%r8, %r7, 2;
	st.param.b32 	[func_retval0+0], %r8;
	ret;

}
	// .globl	wcsncmp
.visible .func  (.param .b32 func_retval0) wcsncmp(
	.param .b32 wcsncmp_param_0,
	.param .b32 wcsncmp_param_1,
	.param .b32 wcsncmp_param_2
)
{
	.local .align 4 .b8 	__local_depot59[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;

	mov.u32 	%SPL, __local_depot59;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r6, [wcsncmp_param_2];
	ld.param.u32 	%r5, [wcsncmp_param_1];
	ld.param.u32 	%r4, [wcsncmp_param_0];
	st.u32 	[%SP+0], %r4;
	st.u32 	[%SP+4], %r5;
	st.u32 	[%SP+8], %r6;
	bra.uni 	$L__BB59_1;
$L__BB59_1:
	ld.u32 	%r7, [%SP+8];
	setp.eq.s32 	%p4, %r7, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p12, %p3;
	@%p4 bra 	$L__BB59_5;
	bra.uni 	$L__BB59_2;
$L__BB59_2:
	ld.u32 	%r8, [%SP+0];
	ld.u32 	%r9, [%r8];
	ld.u32 	%r10, [%SP+4];
	ld.u32 	%r11, [%r10];
	setp.ne.s32 	%p6, %r9, %r11;
	mov.pred 	%p5, 0;
	mov.pred 	%p12, %p5;
	@%p6 bra 	$L__BB59_5;
	bra.uni 	$L__BB59_3;
$L__BB59_3:
	ld.u32 	%r12, [%SP+0];
	ld.u32 	%r13, [%r12];
	setp.eq.s32 	%p8, %r13, 0;
	mov.pred 	%p7, 0;
	mov.pred 	%p12, %p7;
	@%p8 bra 	$L__BB59_5;
	bra.uni 	$L__BB59_4;
$L__BB59_4:
	ld.u32 	%r14, [%SP+4];
	ld.u32 	%r15, [%r14];
	setp.ne.s32 	%p1, %r15, 0;
	mov.pred 	%p12, %p1;
	bra.uni 	$L__BB59_5;
$L__BB59_5:
	mov.pred 	%p2, %p12;
	@!%p2 bra 	$L__BB59_8;
	bra.uni 	$L__BB59_6;
$L__BB59_6:
	bra.uni 	$L__BB59_7;
$L__BB59_7:
	ld.u32 	%r27, [%SP+8];
	add.s32 	%r28, %r27, -1;
	st.u32 	[%SP+8], %r28;
	ld.u32 	%r29, [%SP+0];
	add.s32 	%r30, %r29, 4;
	st.u32 	[%SP+0], %r30;
	ld.u32 	%r31, [%SP+4];
	add.s32 	%r32, %r31, 4;
	st.u32 	[%SP+4], %r32;
	bra.uni 	$L__BB59_1;
$L__BB59_8:
	ld.u32 	%r16, [%SP+8];
	setp.eq.s32 	%p9, %r16, 0;
	@%p9 bra 	$L__BB59_13;
	bra.uni 	$L__BB59_9;
$L__BB59_9:
	ld.u32 	%r18, [%SP+0];
	ld.u32 	%r19, [%r18];
	ld.u32 	%r20, [%SP+4];
	ld.u32 	%r21, [%r20];
	setp.ge.s32 	%p10, %r19, %r21;
	@%p10 bra 	$L__BB59_11;
	bra.uni 	$L__BB59_10;
$L__BB59_10:
	mov.b32 	%r26, -1;
	mov.u32 	%r33, %r26;
	bra.uni 	$L__BB59_12;
$L__BB59_11:
	ld.u32 	%r22, [%SP+0];
	ld.u32 	%r23, [%r22];
	ld.u32 	%r24, [%SP+4];
	ld.u32 	%r25, [%r24];
	setp.gt.s32 	%p11, %r23, %r25;
	selp.u32 	%r1, 1, 0, %p11;
	mov.u32 	%r33, %r1;
	bra.uni 	$L__BB59_12;
$L__BB59_12:
	mov.u32 	%r2, %r33;
	mov.u32 	%r34, %r2;
	bra.uni 	$L__BB59_14;
$L__BB59_13:
	mov.b32 	%r17, 0;
	mov.u32 	%r34, %r17;
	bra.uni 	$L__BB59_14;
$L__BB59_14:
	mov.u32 	%r3, %r34;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	wmemchr
.visible .func  (.param .b32 func_retval0) wmemchr(
	.param .b32 wmemchr_param_0,
	.param .b32 wmemchr_param_1,
	.param .b32 wmemchr_param_2
)
{
	.local .align 4 .b8 	__local_depot60[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<17>;

	mov.u32 	%SPL, __local_depot60;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r5, [wmemchr_param_2];
	ld.param.u32 	%r4, [wmemchr_param_1];
	ld.param.u32 	%r3, [wmemchr_param_0];
	st.u32 	[%SP+0], %r3;
	st.u32 	[%SP+4], %r4;
	st.u32 	[%SP+8], %r5;
	bra.uni 	$L__BB60_1;
$L__BB60_1:
	ld.u32 	%r6, [%SP+8];
	setp.eq.s32 	%p4, %r6, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB60_3;
	bra.uni 	$L__BB60_2;
$L__BB60_2:
	ld.u32 	%r7, [%SP+0];
	ld.u32 	%r8, [%r7];
	ld.u32 	%r9, [%SP+4];
	setp.ne.s32 	%p1, %r8, %r9;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB60_3;
$L__BB60_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB60_6;
	bra.uni 	$L__BB60_4;
$L__BB60_4:
	bra.uni 	$L__BB60_5;
$L__BB60_5:
	ld.u32 	%r12, [%SP+8];
	add.s32 	%r13, %r12, -1;
	st.u32 	[%SP+8], %r13;
	ld.u32 	%r14, [%SP+0];
	add.s32 	%r15, %r14, 4;
	st.u32 	[%SP+0], %r15;
	bra.uni 	$L__BB60_1;
$L__BB60_6:
	ld.u32 	%r10, [%SP+8];
	setp.eq.s32 	%p5, %r10, 0;
	@%p5 bra 	$L__BB60_8;
	bra.uni 	$L__BB60_7;
$L__BB60_7:
	ld.u32 	%r1, [%SP+0];
	mov.u32 	%r16, %r1;
	bra.uni 	$L__BB60_9;
$L__BB60_8:
	mov.b32 	%r11, 0;
	mov.u32 	%r16, %r11;
	bra.uni 	$L__BB60_9;
$L__BB60_9:
	mov.u32 	%r2, %r16;
	st.param.b32 	[func_retval0+0], %r2;
	ret;

}
	// .globl	wmemcmp
.visible .func  (.param .b32 func_retval0) wmemcmp(
	.param .b32 wmemcmp_param_0,
	.param .b32 wmemcmp_param_1,
	.param .b32 wmemcmp_param_2
)
{
	.local .align 4 .b8 	__local_depot61[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<31>;

	mov.u32 	%SPL, __local_depot61;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r6, [wmemcmp_param_2];
	ld.param.u32 	%r5, [wmemcmp_param_1];
	ld.param.u32 	%r4, [wmemcmp_param_0];
	st.u32 	[%SP+0], %r4;
	st.u32 	[%SP+4], %r5;
	st.u32 	[%SP+8], %r6;
	bra.uni 	$L__BB61_1;
$L__BB61_1:
	ld.u32 	%r7, [%SP+8];
	setp.eq.s32 	%p4, %r7, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p8, %p3;
	@%p4 bra 	$L__BB61_3;
	bra.uni 	$L__BB61_2;
$L__BB61_2:
	ld.u32 	%r8, [%SP+0];
	ld.u32 	%r9, [%r8];
	ld.u32 	%r10, [%SP+4];
	ld.u32 	%r11, [%r10];
	setp.eq.s32 	%p1, %r9, %r11;
	mov.pred 	%p8, %p1;
	bra.uni 	$L__BB61_3;
$L__BB61_3:
	mov.pred 	%p2, %p8;
	@!%p2 bra 	$L__BB61_6;
	bra.uni 	$L__BB61_4;
$L__BB61_4:
	bra.uni 	$L__BB61_5;
$L__BB61_5:
	ld.u32 	%r23, [%SP+8];
	add.s32 	%r24, %r23, -1;
	st.u32 	[%SP+8], %r24;
	ld.u32 	%r25, [%SP+0];
	add.s32 	%r26, %r25, 4;
	st.u32 	[%SP+0], %r26;
	ld.u32 	%r27, [%SP+4];
	add.s32 	%r28, %r27, 4;
	st.u32 	[%SP+4], %r28;
	bra.uni 	$L__BB61_1;
$L__BB61_6:
	ld.u32 	%r12, [%SP+8];
	setp.eq.s32 	%p5, %r12, 0;
	@%p5 bra 	$L__BB61_11;
	bra.uni 	$L__BB61_7;
$L__BB61_7:
	ld.u32 	%r14, [%SP+0];
	ld.u32 	%r15, [%r14];
	ld.u32 	%r16, [%SP+4];
	ld.u32 	%r17, [%r16];
	setp.ge.s32 	%p6, %r15, %r17;
	@%p6 bra 	$L__BB61_9;
	bra.uni 	$L__BB61_8;
$L__BB61_8:
	mov.b32 	%r22, -1;
	mov.u32 	%r29, %r22;
	bra.uni 	$L__BB61_10;
$L__BB61_9:
	ld.u32 	%r18, [%SP+0];
	ld.u32 	%r19, [%r18];
	ld.u32 	%r20, [%SP+4];
	ld.u32 	%r21, [%r20];
	setp.gt.s32 	%p7, %r19, %r21;
	selp.u32 	%r1, 1, 0, %p7;
	mov.u32 	%r29, %r1;
	bra.uni 	$L__BB61_10;
$L__BB61_10:
	mov.u32 	%r2, %r29;
	mov.u32 	%r30, %r2;
	bra.uni 	$L__BB61_12;
$L__BB61_11:
	mov.b32 	%r13, 0;
	mov.u32 	%r30, %r13;
	bra.uni 	$L__BB61_12;
$L__BB61_12:
	mov.u32 	%r3, %r30;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	wmemcpy
.visible .func  (.param .b32 func_retval0) wmemcpy(
	.param .b32 wmemcpy_param_0,
	.param .b32 wmemcpy_param_1,
	.param .b32 wmemcpy_param_2
)
{
	.local .align 4 .b8 	__local_depot62[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;

	mov.u32 	%SPL, __local_depot62;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [wmemcpy_param_2];
	ld.param.u32 	%r2, [wmemcpy_param_1];
	ld.param.u32 	%r1, [wmemcpy_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+0];
	st.u32 	[%SP+12], %r4;
	bra.uni 	$L__BB62_1;
$L__BB62_1:
	ld.u32 	%r5, [%SP+8];
	add.s32 	%r6, %r5, -1;
	st.u32 	[%SP+8], %r6;
	setp.eq.s32 	%p1, %r5, 0;
	@%p1 bra 	$L__BB62_3;
	bra.uni 	$L__BB62_2;
$L__BB62_2:
	ld.u32 	%r8, [%SP+4];
	add.s32 	%r9, %r8, 4;
	st.u32 	[%SP+4], %r9;
	ld.u32 	%r10, [%r8];
	ld.u32 	%r11, [%SP+0];
	add.s32 	%r12, %r11, 4;
	st.u32 	[%SP+0], %r12;
	st.u32 	[%r11], %r10;
	bra.uni 	$L__BB62_1;
$L__BB62_3:
	ld.u32 	%r7, [%SP+12];
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	wmemmove
.visible .func  (.param .b32 func_retval0) wmemmove(
	.param .b32 wmemmove_param_0,
	.param .b32 wmemmove_param_1,
	.param .b32 wmemmove_param_2
)
{
	.local .align 4 .b8 	__local_depot63[20];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<31>;

	mov.u32 	%SPL, __local_depot63;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [wmemmove_param_2];
	ld.param.u32 	%r2, [wmemmove_param_1];
	ld.param.u32 	%r1, [wmemmove_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u32 	[%SP+12], %r3;
	ld.u32 	%r4, [%SP+4];
	st.u32 	[%SP+16], %r4;
	ld.u32 	%r5, [%SP+4];
	ld.u32 	%r6, [%SP+8];
	setp.ne.s32 	%p1, %r5, %r6;
	@%p1 bra 	$L__BB63_2;
	bra.uni 	$L__BB63_1;
$L__BB63_1:
	ld.u32 	%r29, [%SP+4];
	st.u32 	[%SP+0], %r29;
	bra.uni 	$L__BB63_12;
$L__BB63_2:
	ld.u32 	%r7, [%SP+4];
	ld.u32 	%r8, [%SP+8];
	sub.s32 	%r9, %r7, %r8;
	ld.u32 	%r10, [%SP+12];
	shl.b32 	%r11, %r10, 2;
	setp.ge.u32 	%p2, %r9, %r11;
	@%p2 bra 	$L__BB63_7;
	bra.uni 	$L__BB63_3;
$L__BB63_3:
	bra.uni 	$L__BB63_4;
$L__BB63_4:
	ld.u32 	%r19, [%SP+12];
	add.s32 	%r20, %r19, -1;
	st.u32 	[%SP+12], %r20;
	setp.eq.s32 	%p4, %r19, 0;
	@%p4 bra 	$L__BB63_6;
	bra.uni 	$L__BB63_5;
$L__BB63_5:
	ld.u32 	%r22, [%SP+8];
	ld.u32 	%r23, [%SP+12];
	shl.b32 	%r24, %r23, 2;
	add.s32 	%r25, %r22, %r24;
	ld.u32 	%r26, [%r25];
	ld.u32 	%r27, [%SP+4];
	add.s32 	%r28, %r27, %r24;
	st.u32 	[%r28], %r26;
	bra.uni 	$L__BB63_4;
$L__BB63_6:
	bra.uni 	$L__BB63_11;
$L__BB63_7:
	bra.uni 	$L__BB63_8;
$L__BB63_8:
	ld.u32 	%r12, [%SP+12];
	add.s32 	%r13, %r12, -1;
	st.u32 	[%SP+12], %r13;
	setp.eq.s32 	%p3, %r12, 0;
	@%p3 bra 	$L__BB63_10;
	bra.uni 	$L__BB63_9;
$L__BB63_9:
	ld.u32 	%r14, [%SP+8];
	add.s32 	%r15, %r14, 4;
	st.u32 	[%SP+8], %r15;
	ld.u32 	%r16, [%r14];
	ld.u32 	%r17, [%SP+4];
	add.s32 	%r18, %r17, 4;
	st.u32 	[%SP+4], %r18;
	st.u32 	[%r17], %r16;
	bra.uni 	$L__BB63_8;
$L__BB63_10:
	bra.uni 	$L__BB63_11;
$L__BB63_11:
	ld.u32 	%r21, [%SP+16];
	st.u32 	[%SP+0], %r21;
	bra.uni 	$L__BB63_12;
$L__BB63_12:
	ld.u32 	%r30, [%SP+0];
	st.param.b32 	[func_retval0+0], %r30;
	ret;

}
	// .globl	wmemset
.visible .func  (.param .b32 func_retval0) wmemset(
	.param .b32 wmemset_param_0,
	.param .b32 wmemset_param_1,
	.param .b32 wmemset_param_2
)
{
	.local .align 4 .b8 	__local_depot64[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<11>;

	mov.u32 	%SPL, __local_depot64;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [wmemset_param_2];
	ld.param.u32 	%r2, [wmemset_param_1];
	ld.param.u32 	%r1, [wmemset_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+0];
	st.u32 	[%SP+12], %r4;
	bra.uni 	$L__BB64_1;
$L__BB64_1:
	ld.u32 	%r5, [%SP+8];
	add.s32 	%r6, %r5, -1;
	st.u32 	[%SP+8], %r6;
	setp.eq.s32 	%p1, %r5, 0;
	@%p1 bra 	$L__BB64_3;
	bra.uni 	$L__BB64_2;
$L__BB64_2:
	ld.u32 	%r8, [%SP+4];
	ld.u32 	%r9, [%SP+0];
	add.s32 	%r10, %r9, 4;
	st.u32 	[%SP+0], %r10;
	st.u32 	[%r9], %r8;
	bra.uni 	$L__BB64_1;
$L__BB64_3:
	ld.u32 	%r7, [%SP+12];
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	bcopy
.visible .func bcopy(
	.param .b32 bcopy_param_0,
	.param .b32 bcopy_param_1,
	.param .b32 bcopy_param_2
)
{
	.local .align 4 .b8 	__local_depot65[20];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<30>;

	mov.u32 	%SPL, __local_depot65;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [bcopy_param_2];
	ld.param.u32 	%r2, [bcopy_param_1];
	ld.param.u32 	%r1, [bcopy_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+0];
	st.u32 	[%SP+12], %r4;
	ld.u32 	%r5, [%SP+4];
	st.u32 	[%SP+16], %r5;
	ld.u32 	%r6, [%SP+12];
	ld.u32 	%r7, [%SP+16];
	setp.ge.u32 	%p1, %r6, %r7;
	@%p1 bra 	$L__BB65_6;
	bra.uni 	$L__BB65_1;
$L__BB65_1:
	ld.u32 	%r17, [%SP+8];
	ld.u32 	%r18, [%SP+12];
	add.s32 	%r19, %r18, %r17;
	st.u32 	[%SP+12], %r19;
	ld.u32 	%r20, [%SP+8];
	ld.u32 	%r21, [%SP+16];
	add.s32 	%r22, %r21, %r20;
	st.u32 	[%SP+16], %r22;
	bra.uni 	$L__BB65_2;
$L__BB65_2:
	ld.u32 	%r23, [%SP+8];
	setp.eq.s32 	%p4, %r23, 0;
	@%p4 bra 	$L__BB65_5;
	bra.uni 	$L__BB65_3;
$L__BB65_3:
	ld.u32 	%r24, [%SP+12];
	add.s32 	%r25, %r24, -1;
	st.u32 	[%SP+12], %r25;
	ld.u8 	%rs2, [%r24+-1];
	ld.u32 	%r26, [%SP+16];
	add.s32 	%r27, %r26, -1;
	st.u32 	[%SP+16], %r27;
	st.u8 	[%r26+-1], %rs2;
	bra.uni 	$L__BB65_4;
$L__BB65_4:
	ld.u32 	%r28, [%SP+8];
	add.s32 	%r29, %r28, -1;
	st.u32 	[%SP+8], %r29;
	bra.uni 	$L__BB65_2;
$L__BB65_5:
	bra.uni 	$L__BB65_13;
$L__BB65_6:
	ld.u32 	%r8, [%SP+12];
	ld.u32 	%r9, [%SP+16];
	setp.eq.s32 	%p2, %r8, %r9;
	@%p2 bra 	$L__BB65_12;
	bra.uni 	$L__BB65_7;
$L__BB65_7:
	bra.uni 	$L__BB65_8;
$L__BB65_8:
	ld.u32 	%r10, [%SP+8];
	setp.eq.s32 	%p3, %r10, 0;
	@%p3 bra 	$L__BB65_11;
	bra.uni 	$L__BB65_9;
$L__BB65_9:
	ld.u32 	%r11, [%SP+12];
	add.s32 	%r12, %r11, 1;
	st.u32 	[%SP+12], %r12;
	ld.u8 	%rs1, [%r11];
	ld.u32 	%r13, [%SP+16];
	add.s32 	%r14, %r13, 1;
	st.u32 	[%SP+16], %r14;
	st.u8 	[%r13], %rs1;
	bra.uni 	$L__BB65_10;
$L__BB65_10:
	ld.u32 	%r15, [%SP+8];
	add.s32 	%r16, %r15, -1;
	st.u32 	[%SP+8], %r16;
	bra.uni 	$L__BB65_8;
$L__BB65_11:
	bra.uni 	$L__BB65_12;
$L__BB65_12:
	bra.uni 	$L__BB65_13;
$L__BB65_13:
	ret;

}
	// .globl	rotl64
.visible .func  (.param .b64 func_retval0) rotl64(
	.param .b64 rotl64_param_0,
	.param .b32 rotl64_param_1
)
{
	.local .align 8 .b8 	__local_depot66[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<4>;

	mov.u32 	%SPL, __local_depot66;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [rotl64_param_1];
	ld.param.u64 	%rd1, [rotl64_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	ld.u64 	%rd2, [%SP+0];
	ld.u32 	%r2, [%SP+8];
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	and.b32 	%amt2, %r2, 63;
	shl.b64 	%lhs, %rd2, %amt2;
	sub.u32 	%amt2, 64, %amt2;
	shr.b64 	%rhs, %rd2, %amt2;
	add.u64 	%rd3, %lhs, %rhs;
	}
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	rotr64
.visible .func  (.param .b64 func_retval0) rotr64(
	.param .b64 rotr64_param_0,
	.param .b32 rotr64_param_1
)
{
	.local .align 8 .b8 	__local_depot67[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<4>;

	mov.u32 	%SPL, __local_depot67;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [rotr64_param_1];
	ld.param.u64 	%rd1, [rotr64_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	ld.u64 	%rd2, [%SP+0];
	ld.u32 	%r2, [%SP+8];
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	and.b32 	%amt2, %r2, 63;
	shr.b64 	%lhs, %rd2, %amt2;
	sub.u32 	%amt2, 64, %amt2;
	shl.b64 	%rhs, %rd2, %amt2;
	add.u64 	%rd3, %lhs, %rhs;
	}
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	rotl32
.visible .func  (.param .b32 func_retval0) rotl32(
	.param .b32 rotl32_param_0,
	.param .b32 rotl32_param_1
)
{
	.local .align 4 .b8 	__local_depot68[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<6>;

	mov.u32 	%SPL, __local_depot68;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [rotl32_param_1];
	ld.param.u32 	%r1, [rotl32_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	{
	.reg .b32 %lhs;
	.reg .b32 %rhs;
	.reg .b32 %amt2;
	shl.b32 	%lhs, %r3, %r4;
	sub.s32 	%amt2, 32, %r4;
	shr.b32 	%rhs, %r3, %amt2;
	add.u32 	%r5, %lhs, %rhs;
	}
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	rotr32
.visible .func  (.param .b32 func_retval0) rotr32(
	.param .b32 rotr32_param_0,
	.param .b32 rotr32_param_1
)
{
	.local .align 4 .b8 	__local_depot69[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<6>;

	mov.u32 	%SPL, __local_depot69;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [rotr32_param_1];
	ld.param.u32 	%r1, [rotr32_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	{
	.reg .b32 %lhs;
	.reg .b32 %rhs;
	.reg .b32 %amt2;
	shr.b32 	%lhs, %r3, %r4;
	sub.s32 	%amt2, 32, %r4;
	shl.b32 	%rhs, %r3, %amt2;
	add.u32 	%r5, %lhs, %rhs;
	}
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	rotl_sz
.visible .func  (.param .b32 func_retval0) rotl_sz(
	.param .b32 rotl_sz_param_0,
	.param .b32 rotl_sz_param_1
)
{
	.local .align 4 .b8 	__local_depot70[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<6>;

	mov.u32 	%SPL, __local_depot70;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [rotl_sz_param_1];
	ld.param.u32 	%r1, [rotl_sz_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	{
	.reg .b32 %lhs;
	.reg .b32 %rhs;
	.reg .b32 %amt2;
	shl.b32 	%lhs, %r3, %r4;
	sub.s32 	%amt2, 32, %r4;
	shr.b32 	%rhs, %r3, %amt2;
	add.u32 	%r5, %lhs, %rhs;
	}
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	rotr_sz
.visible .func  (.param .b32 func_retval0) rotr_sz(
	.param .b32 rotr_sz_param_0,
	.param .b32 rotr_sz_param_1
)
{
	.local .align 4 .b8 	__local_depot71[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<6>;

	mov.u32 	%SPL, __local_depot71;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [rotr_sz_param_1];
	ld.param.u32 	%r1, [rotr_sz_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	{
	.reg .b32 %lhs;
	.reg .b32 %rhs;
	.reg .b32 %amt2;
	shr.b32 	%lhs, %r3, %r4;
	sub.s32 	%amt2, 32, %r4;
	shl.b32 	%rhs, %r3, %amt2;
	add.u32 	%r5, %lhs, %rhs;
	}
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	rotl16
.visible .func  (.param .b32 func_retval0) rotl16(
	.param .b32 rotl16_param_0,
	.param .b32 rotl16_param_1
)
{
	.local .align 4 .b8 	__local_depot72[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;

	mov.u32 	%SPL, __local_depot72;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [rotl16_param_1];
	ld.param.u16 	%rs1, [rotl16_param_0];
	st.u16 	[%SP+0], %rs1;
	st.u32 	[%SP+4], %r1;
	ld.u16 	%r2, [%SP+0];
	ld.u32 	%r3, [%SP+4];
	shl.b32 	%r4, %r2, %r3;
	mov.b32 	%r5, 16;
	sub.s32 	%r6, %r5, %r3;
	shr.u32 	%r7, %r2, %r6;
	or.b32  	%r8, %r4, %r7;
	and.b32  	%r9, %r8, 65535;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	rotr16
.visible .func  (.param .b32 func_retval0) rotr16(
	.param .b32 rotr16_param_0,
	.param .b32 rotr16_param_1
)
{
	.local .align 4 .b8 	__local_depot73[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;

	mov.u32 	%SPL, __local_depot73;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [rotr16_param_1];
	ld.param.u16 	%rs1, [rotr16_param_0];
	st.u16 	[%SP+0], %rs1;
	st.u32 	[%SP+4], %r1;
	ld.u16 	%r2, [%SP+0];
	ld.u32 	%r3, [%SP+4];
	shr.u32 	%r4, %r2, %r3;
	mov.b32 	%r5, 16;
	sub.s32 	%r6, %r5, %r3;
	shl.b32 	%r7, %r2, %r6;
	or.b32  	%r8, %r4, %r7;
	and.b32  	%r9, %r8, 65535;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	rotl8
.visible .func  (.param .b32 func_retval0) rotl8(
	.param .b32 rotl8_param_0,
	.param .b32 rotl8_param_1
)
{
	.local .align 4 .b8 	__local_depot74[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;

	mov.u32 	%SPL, __local_depot74;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [rotl8_param_1];
	ld.param.u8 	%rs1, [rotl8_param_0];
	st.u8 	[%SP+0], %rs1;
	st.u32 	[%SP+4], %r1;
	ld.u8 	%r2, [%SP+0];
	ld.u32 	%r3, [%SP+4];
	shl.b32 	%r4, %r2, %r3;
	mov.b32 	%r5, 8;
	sub.s32 	%r6, %r5, %r3;
	shr.u32 	%r7, %r2, %r6;
	or.b32  	%r8, %r4, %r7;
	and.b32  	%r9, %r8, 255;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	rotr8
.visible .func  (.param .b32 func_retval0) rotr8(
	.param .b32 rotr8_param_0,
	.param .b32 rotr8_param_1
)
{
	.local .align 4 .b8 	__local_depot75[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;

	mov.u32 	%SPL, __local_depot75;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [rotr8_param_1];
	ld.param.u8 	%rs1, [rotr8_param_0];
	st.u8 	[%SP+0], %rs1;
	st.u32 	[%SP+4], %r1;
	ld.u8 	%r2, [%SP+0];
	ld.u32 	%r3, [%SP+4];
	shr.u32 	%r4, %r2, %r3;
	mov.b32 	%r5, 8;
	sub.s32 	%r6, %r5, %r3;
	shl.b32 	%r7, %r2, %r6;
	or.b32  	%r8, %r4, %r7;
	and.b32  	%r9, %r8, 255;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	bswap_16
.visible .func  (.param .b32 func_retval0) bswap_16(
	.param .b32 bswap_16_param_0
)
{
	.local .align 2 .b8 	__local_depot76[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<10>;

	mov.u32 	%SPL, __local_depot76;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u16 	%rs1, [bswap_16_param_0];
	st.u16 	[%SP+0], %rs1;
	mov.u16 	%rs2, 255;
	st.u16 	[%SP+2], %rs2;
	ld.u16 	%r1, [%SP+0];
	ld.u16 	%r2, [%SP+2];
	shl.b32 	%r3, %r2, 8;
	and.b32  	%r4, %r1, %r3;
	shr.u32 	%r5, %r4, 8;
	and.b32  	%r6, %r1, %r2;
	shl.b32 	%r7, %r6, 8;
	or.b32  	%r8, %r5, %r7;
	and.b32  	%r9, %r8, 65535;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	bswap_32
.visible .func  (.param .b32 func_retval0) bswap_32(
	.param .b32 bswap_32_param_0
)
{
	.local .align 4 .b8 	__local_depot77[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<18>;

	mov.u32 	%SPL, __local_depot77;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [bswap_32_param_0];
	st.u32 	[%SP+0], %r1;
	mov.b32 	%r2, 255;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	shl.b32 	%r5, %r4, 24;
	and.b32  	%r6, %r3, %r5;
	shr.u32 	%r7, %r6, 24;
	shl.b32 	%r8, %r4, 16;
	and.b32  	%r9, %r3, %r8;
	shr.u32 	%r10, %r9, 8;
	or.b32  	%r11, %r7, %r10;
	shl.b32 	%r12, %r3, 8;
	and.b32  	%r13, %r8, %r12;
	or.b32  	%r14, %r11, %r13;
	and.b32  	%r15, %r3, %r4;
	shl.b32 	%r16, %r15, 24;
	or.b32  	%r17, %r14, %r16;
	st.param.b32 	[func_retval0+0], %r17;
	ret;

}
	// .globl	bswap_64
.visible .func  (.param .b64 func_retval0) bswap_64(
	.param .b64 bswap_64_param_0
)
{
	.local .align 8 .b8 	__local_depot78[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b64 	%rd<32>;

	mov.u32 	%SPL, __local_depot78;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd1, [bswap_64_param_0];
	st.u64 	[%SP+0], %rd1;
	mov.u64 	%rd2, 255;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	ld.u64 	%rd4, [%SP+8];
	shl.b64 	%rd5, %rd4, 56;
	and.b64  	%rd6, %rd3, %rd5;
	shr.u64 	%rd7, %rd6, 56;
	shl.b64 	%rd8, %rd4, 48;
	and.b64  	%rd9, %rd3, %rd8;
	shr.u64 	%rd10, %rd9, 40;
	or.b64  	%rd11, %rd7, %rd10;
	shl.b64 	%rd12, %rd4, 40;
	and.b64  	%rd13, %rd3, %rd12;
	shr.u64 	%rd14, %rd13, 24;
	or.b64  	%rd15, %rd11, %rd14;
	shl.b64 	%rd16, %rd4, 32;
	and.b64  	%rd17, %rd3, %rd16;
	shr.u64 	%rd18, %rd17, 8;
	or.b64  	%rd19, %rd15, %rd18;
	shl.b64 	%rd20, %rd3, 8;
	and.b64  	%rd21, %rd16, %rd20;
	or.b64  	%rd22, %rd19, %rd21;
	shl.b64 	%rd23, %rd3, 24;
	and.b64  	%rd24, %rd12, %rd23;
	or.b64  	%rd25, %rd22, %rd24;
	shl.b64 	%rd26, %rd3, 40;
	and.b64  	%rd27, %rd8, %rd26;
	or.b64  	%rd28, %rd25, %rd27;
	and.b64  	%rd29, %rd3, %rd4;
	shl.b64 	%rd30, %rd29, 56;
	or.b64  	%rd31, %rd28, %rd30;
	st.param.b64 	[func_retval0+0], %rd31;
	ret;

}
	// .globl	ffs
.visible .func  (.param .b32 func_retval0) ffs(
	.param .b32 ffs_param_0
)
{
	.local .align 4 .b8 	__local_depot79[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<14>;

	mov.u32 	%SPL, __local_depot79;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [ffs_param_0];
	st.u32 	[%SP+4], %r1;
	mov.b32 	%r2, 0;
	st.u32 	[%SP+8], %r2;
	bra.uni 	$L__BB79_1;
$L__BB79_1:
	ld.u32 	%r3, [%SP+8];
	setp.gt.u32 	%p1, %r3, 31;
	@%p1 bra 	$L__BB79_6;
	bra.uni 	$L__BB79_2;
$L__BB79_2:
	ld.u32 	%r5, [%SP+4];
	ld.u32 	%r6, [%SP+8];
	shr.u32 	%r7, %r5, %r6;
	and.b32  	%r8, %r7, 1;
	setp.eq.b32 	%p2, %r8, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB79_4;
	bra.uni 	$L__BB79_3;
$L__BB79_3:
	ld.u32 	%r11, [%SP+8];
	add.s32 	%r12, %r11, 1;
	st.u32 	[%SP+0], %r12;
	bra.uni 	$L__BB79_7;
$L__BB79_4:
	bra.uni 	$L__BB79_5;
$L__BB79_5:
	ld.u32 	%r9, [%SP+8];
	add.s32 	%r10, %r9, 1;
	st.u32 	[%SP+8], %r10;
	bra.uni 	$L__BB79_1;
$L__BB79_6:
	mov.b32 	%r4, 0;
	st.u32 	[%SP+0], %r4;
	bra.uni 	$L__BB79_7;
$L__BB79_7:
	ld.u32 	%r13, [%SP+0];
	st.param.b32 	[func_retval0+0], %r13;
	ret;

}
	// .globl	libiberty_ffs
.visible .func  (.param .b32 func_retval0) libiberty_ffs(
	.param .b32 libiberty_ffs_param_0
)
{
	.local .align 4 .b8 	__local_depot80[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<13>;

	mov.u32 	%SPL, __local_depot80;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [libiberty_ffs_param_0];
	st.u32 	[%SP+4], %r1;
	ld.u32 	%r2, [%SP+4];
	setp.ne.s32 	%p1, %r2, 0;
	@%p1 bra 	$L__BB80_2;
	bra.uni 	$L__BB80_1;
$L__BB80_1:
	mov.b32 	%r11, 0;
	st.u32 	[%SP+0], %r11;
	bra.uni 	$L__BB80_7;
$L__BB80_2:
	mov.b32 	%r3, 1;
	st.u32 	[%SP+8], %r3;
	bra.uni 	$L__BB80_3;
$L__BB80_3:
	ld.u32 	%r4, [%SP+4];
	and.b32  	%r5, %r4, 1;
	setp.eq.b32 	%p2, %r5, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB80_6;
	bra.uni 	$L__BB80_4;
$L__BB80_4:
	ld.u32 	%r7, [%SP+4];
	shr.s32 	%r8, %r7, 1;
	st.u32 	[%SP+4], %r8;
	bra.uni 	$L__BB80_5;
$L__BB80_5:
	ld.u32 	%r9, [%SP+8];
	add.s32 	%r10, %r9, 1;
	st.u32 	[%SP+8], %r10;
	bra.uni 	$L__BB80_3;
$L__BB80_6:
	ld.u32 	%r6, [%SP+8];
	st.u32 	[%SP+0], %r6;
	bra.uni 	$L__BB80_7;
$L__BB80_7:
	ld.u32 	%r12, [%SP+0];
	st.param.b32 	[func_retval0+0], %r12;
	ret;

}
	// .globl	gl_isinff
.visible .func  (.param .b32 func_retval0) gl_isinff(
	.param .b32 gl_isinff_param_0
)
{
	.local .align 4 .b8 	__local_depot81[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<2>;
	.reg .f32 	%f<4>;

	mov.u32 	%SPL, __local_depot81;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f32 	%f1, [gl_isinff_param_0];
	st.f32 	[%SP+0], %f1;
	ld.f32 	%f2, [%SP+0];
	setp.lt.f32 	%p4, %f2, 0fFF7FFFFF;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB81_2;
	bra.uni 	$L__BB81_1;
$L__BB81_1:
	ld.f32 	%f3, [%SP+0];
	setp.gt.f32 	%p1, %f3, 0f7F7FFFFF;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB81_2;
$L__BB81_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r1, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r1;
	ret;

}
	// .globl	gl_isinfd
.visible .func  (.param .b32 func_retval0) gl_isinfd(
	.param .b64 gl_isinfd_param_0
)
{
	.local .align 8 .b8 	__local_depot82[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<2>;
	.reg .f64 	%fd<4>;

	mov.u32 	%SPL, __local_depot82;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f64 	%fd1, [gl_isinfd_param_0];
	st.f64 	[%SP+0], %fd1;
	ld.f64 	%fd2, [%SP+0];
	setp.lt.f64 	%p4, %fd2, 0dFFEFFFFFFFFFFFFF;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB82_2;
	bra.uni 	$L__BB82_1;
$L__BB82_1:
	ld.f64 	%fd3, [%SP+0];
	setp.gt.f64 	%p1, %fd3, 0d7FEFFFFFFFFFFFFF;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB82_2;
$L__BB82_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r1, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r1;
	ret;

}
	// .globl	gl_isinfl
.visible .func  (.param .b32 func_retval0) gl_isinfl(
	.param .b64 gl_isinfl_param_0
)
{
	.local .align 8 .b8 	__local_depot83[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<2>;
	.reg .f64 	%fd<4>;

	mov.u32 	%SPL, __local_depot83;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f64 	%fd1, [gl_isinfl_param_0];
	st.f64 	[%SP+0], %fd1;
	ld.f64 	%fd2, [%SP+0];
	setp.lt.f64 	%p4, %fd2, 0dFFEFFFFFFFFFFFFF;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB83_2;
	bra.uni 	$L__BB83_1;
$L__BB83_1:
	ld.f64 	%fd3, [%SP+0];
	setp.gt.f64 	%p1, %fd3, 0d7FEFFFFFFFFFFFFF;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB83_2;
$L__BB83_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r1, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r1;
	ret;

}
	// .globl	_Qp_itoq
.visible .func _Qp_itoq(
	.param .b32 _Qp_itoq_param_0,
	.param .b32 _Qp_itoq_param_1
)
{
	.local .align 4 .b8 	__local_depot84[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<2>;

	mov.u32 	%SPL, __local_depot84;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [_Qp_itoq_param_1];
	ld.param.u32 	%r1, [_Qp_itoq_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+4];
	cvt.rn.f64.s32 	%fd1, %r3;
	ld.u32 	%r4, [%SP+0];
	st.f64 	[%r4], %fd1;
	ret;

}
	// .globl	ldexpf
.visible .func  (.param .b32 func_retval0) ldexpf(
	.param .b32 ldexpf_param_0,
	.param .b32 ldexpf_param_1
)
{
	.local .align 4 .b8 	__local_depot85[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<12>;

	mov.u32 	%SPL, __local_depot85;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [ldexpf_param_1];
	ld.param.f32 	%f1, [ldexpf_param_0];
	st.f32 	[%SP+0], %f1;
	st.u32 	[%SP+4], %r1;
	ld.f32 	%f2, [%SP+0];
	setp.nan.f32 	%p1, %f2, %f2;
	@%p1 bra 	$L__BB85_9;
	bra.uni 	$L__BB85_1;
$L__BB85_1:
	ld.f32 	%f3, [%SP+0];
	add.rn.f32 	%f4, %f3, %f3;
	setp.eq.f32 	%p2, %f4, %f3;
	@%p2 bra 	$L__BB85_9;
	bra.uni 	$L__BB85_2;
$L__BB85_2:
	ld.u32 	%r2, [%SP+4];
	setp.lt.s32 	%p3, %r2, 0;
	selp.f32 	%f5, 0f3F000000, 0f40000000, %p3;
	st.f32 	[%SP+8], %f5;
	bra.uni 	$L__BB85_3;
$L__BB85_3:
	ld.u32 	%r3, [%SP+4];
	shr.u32 	%r4, %r3, 31;
	add.s32 	%r5, %r3, %r4;
	and.b32  	%r6, %r5, -2;
	sub.s32 	%r7, %r3, %r6;
	setp.eq.s32 	%p4, %r7, 0;
	@%p4 bra 	$L__BB85_5;
	bra.uni 	$L__BB85_4;
$L__BB85_4:
	ld.f32 	%f6, [%SP+8];
	ld.f32 	%f7, [%SP+0];
	mul.rn.f32 	%f8, %f7, %f6;
	st.f32 	[%SP+0], %f8;
	bra.uni 	$L__BB85_5;
$L__BB85_5:
	ld.u32 	%r8, [%SP+4];
	shr.u32 	%r9, %r8, 31;
	add.s32 	%r10, %r8, %r9;
	shr.s32 	%r11, %r10, 1;
	st.u32 	[%SP+4], %r11;
	ld.u32 	%r12, [%SP+4];
	setp.ne.s32 	%p5, %r12, 0;
	@%p5 bra 	$L__BB85_7;
	bra.uni 	$L__BB85_6;
$L__BB85_6:
	bra.uni 	$L__BB85_8;
$L__BB85_7:
	ld.f32 	%f9, [%SP+8];
	mul.rn.f32 	%f10, %f9, %f9;
	st.f32 	[%SP+8], %f10;
	bra.uni 	$L__BB85_3;
$L__BB85_8:
	bra.uni 	$L__BB85_9;
$L__BB85_9:
	ld.f32 	%f11, [%SP+0];
	st.param.f32 	[func_retval0+0], %f11;
	ret;

}
	// .globl	ldexp
.visible .func  (.param .b64 func_retval0) ldexp(
	.param .b64 ldexp_param_0,
	.param .b32 ldexp_param_1
)
{
	.local .align 8 .b8 	__local_depot86[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<12>;

	mov.u32 	%SPL, __local_depot86;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [ldexp_param_1];
	ld.param.f64 	%fd1, [ldexp_param_0];
	st.f64 	[%SP+0], %fd1;
	st.u32 	[%SP+8], %r1;
	ld.f64 	%fd2, [%SP+0];
	setp.nan.f64 	%p1, %fd2, %fd2;
	@%p1 bra 	$L__BB86_9;
	bra.uni 	$L__BB86_1;
$L__BB86_1:
	ld.f64 	%fd3, [%SP+0];
	add.rn.f64 	%fd4, %fd3, %fd3;
	setp.eq.f64 	%p2, %fd4, %fd3;
	@%p2 bra 	$L__BB86_9;
	bra.uni 	$L__BB86_2;
$L__BB86_2:
	ld.u32 	%r2, [%SP+8];
	setp.lt.s32 	%p3, %r2, 0;
	selp.f64 	%fd5, 0d3FE0000000000000, 0d4000000000000000, %p3;
	st.f64 	[%SP+16], %fd5;
	bra.uni 	$L__BB86_3;
$L__BB86_3:
	ld.u32 	%r3, [%SP+8];
	shr.u32 	%r4, %r3, 31;
	add.s32 	%r5, %r3, %r4;
	and.b32  	%r6, %r5, -2;
	sub.s32 	%r7, %r3, %r6;
	setp.eq.s32 	%p4, %r7, 0;
	@%p4 bra 	$L__BB86_5;
	bra.uni 	$L__BB86_4;
$L__BB86_4:
	ld.f64 	%fd6, [%SP+16];
	ld.f64 	%fd7, [%SP+0];
	mul.rn.f64 	%fd8, %fd7, %fd6;
	st.f64 	[%SP+0], %fd8;
	bra.uni 	$L__BB86_5;
$L__BB86_5:
	ld.u32 	%r8, [%SP+8];
	shr.u32 	%r9, %r8, 31;
	add.s32 	%r10, %r8, %r9;
	shr.s32 	%r11, %r10, 1;
	st.u32 	[%SP+8], %r11;
	ld.u32 	%r12, [%SP+8];
	setp.ne.s32 	%p5, %r12, 0;
	@%p5 bra 	$L__BB86_7;
	bra.uni 	$L__BB86_6;
$L__BB86_6:
	bra.uni 	$L__BB86_8;
$L__BB86_7:
	ld.f64 	%fd9, [%SP+16];
	mul.rn.f64 	%fd10, %fd9, %fd9;
	st.f64 	[%SP+16], %fd10;
	bra.uni 	$L__BB86_3;
$L__BB86_8:
	bra.uni 	$L__BB86_9;
$L__BB86_9:
	ld.f64 	%fd11, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd11;
	ret;

}
	// .globl	ldexpl
.visible .func  (.param .b64 func_retval0) ldexpl(
	.param .b64 ldexpl_param_0,
	.param .b32 ldexpl_param_1
)
{
	.local .align 8 .b8 	__local_depot87[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<12>;

	mov.u32 	%SPL, __local_depot87;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [ldexpl_param_1];
	ld.param.f64 	%fd1, [ldexpl_param_0];
	st.f64 	[%SP+0], %fd1;
	st.u32 	[%SP+8], %r1;
	ld.f64 	%fd2, [%SP+0];
	setp.nan.f64 	%p1, %fd2, %fd2;
	@%p1 bra 	$L__BB87_9;
	bra.uni 	$L__BB87_1;
$L__BB87_1:
	ld.f64 	%fd3, [%SP+0];
	add.rn.f64 	%fd4, %fd3, %fd3;
	setp.eq.f64 	%p2, %fd4, %fd3;
	@%p2 bra 	$L__BB87_9;
	bra.uni 	$L__BB87_2;
$L__BB87_2:
	ld.u32 	%r2, [%SP+8];
	setp.lt.s32 	%p3, %r2, 0;
	selp.f64 	%fd5, 0d3FE0000000000000, 0d4000000000000000, %p3;
	st.f64 	[%SP+16], %fd5;
	bra.uni 	$L__BB87_3;
$L__BB87_3:
	ld.u32 	%r3, [%SP+8];
	shr.u32 	%r4, %r3, 31;
	add.s32 	%r5, %r3, %r4;
	and.b32  	%r6, %r5, -2;
	sub.s32 	%r7, %r3, %r6;
	setp.eq.s32 	%p4, %r7, 0;
	@%p4 bra 	$L__BB87_5;
	bra.uni 	$L__BB87_4;
$L__BB87_4:
	ld.f64 	%fd6, [%SP+16];
	ld.f64 	%fd7, [%SP+0];
	mul.rn.f64 	%fd8, %fd7, %fd6;
	st.f64 	[%SP+0], %fd8;
	bra.uni 	$L__BB87_5;
$L__BB87_5:
	ld.u32 	%r8, [%SP+8];
	shr.u32 	%r9, %r8, 31;
	add.s32 	%r10, %r8, %r9;
	shr.s32 	%r11, %r10, 1;
	st.u32 	[%SP+8], %r11;
	ld.u32 	%r12, [%SP+8];
	setp.ne.s32 	%p5, %r12, 0;
	@%p5 bra 	$L__BB87_7;
	bra.uni 	$L__BB87_6;
$L__BB87_6:
	bra.uni 	$L__BB87_8;
$L__BB87_7:
	ld.f64 	%fd9, [%SP+16];
	mul.rn.f64 	%fd10, %fd9, %fd9;
	st.f64 	[%SP+16], %fd10;
	bra.uni 	$L__BB87_3;
$L__BB87_8:
	bra.uni 	$L__BB87_9;
$L__BB87_9:
	ld.f64 	%fd11, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd11;
	ret;

}
	// .globl	memxor
.visible .func  (.param .b32 func_retval0) memxor(
	.param .b32 memxor_param_0,
	.param .b32 memxor_param_1,
	.param .b32 memxor_param_2
)
{
	.local .align 4 .b8 	__local_depot88[20];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<17>;

	mov.u32 	%SPL, __local_depot88;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [memxor_param_2];
	ld.param.u32 	%r2, [memxor_param_1];
	ld.param.u32 	%r1, [memxor_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+4];
	st.u32 	[%SP+12], %r4;
	ld.u32 	%r5, [%SP+0];
	st.u32 	[%SP+16], %r5;
	bra.uni 	$L__BB88_1;
$L__BB88_1:
	ld.u32 	%r6, [%SP+8];
	setp.eq.s32 	%p1, %r6, 0;
	@%p1 bra 	$L__BB88_4;
	bra.uni 	$L__BB88_2;
$L__BB88_2:
	ld.u32 	%r8, [%SP+12];
	add.s32 	%r9, %r8, 1;
	st.u32 	[%SP+12], %r9;
	ld.u8 	%r10, [%r8];
	ld.u32 	%r11, [%SP+16];
	add.s32 	%r12, %r11, 1;
	st.u32 	[%SP+16], %r12;
	ld.u8 	%r13, [%r11];
	xor.b32  	%r14, %r13, %r10;
	st.u8 	[%r11], %r14;
	bra.uni 	$L__BB88_3;
$L__BB88_3:
	ld.u32 	%r15, [%SP+8];
	add.s32 	%r16, %r15, -1;
	st.u32 	[%SP+8], %r16;
	bra.uni 	$L__BB88_1;
$L__BB88_4:
	ld.u32 	%r7, [%SP+0];
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	strncat
.visible .func  (.param .b32 func_retval0) strncat(
	.param .b32 strncat_param_0,
	.param .b32 strncat_param_1,
	.param .b32 strncat_param_2
)
{
	.local .align 4 .b8 	__local_depot89[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<22>;

	mov.u32 	%SPL, __local_depot89;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [strncat_param_2];
	ld.param.u32 	%r2, [strncat_param_1];
	ld.param.u32 	%r1, [strncat_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+0];
	{ // callseq 8, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r4;
	.param .b32 retval0;
	call.uni (retval0), 
	strlen, 
	(
	param0
	);
	ld.param.b32 	%r5, [retval0+0];
	} // callseq 8
	add.s32 	%r7, %r4, %r5;
	st.u32 	[%SP+12], %r7;
	bra.uni 	$L__BB89_1;
$L__BB89_1:
	ld.u32 	%r8, [%SP+8];
	setp.eq.s32 	%p4, %r8, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB89_3;
	bra.uni 	$L__BB89_2;
$L__BB89_2:
	ld.u32 	%r9, [%SP+4];
	ld.u8 	%rs1, [%r9];
	ld.u32 	%r10, [%SP+12];
	st.u8 	[%r10], %rs1;
	cvt.u32.u16 	%r11, %rs1;
	and.b32  	%r12, %r11, 255;
	setp.ne.s32 	%p1, %r12, 0;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB89_3;
$L__BB89_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB89_6;
	bra.uni 	$L__BB89_4;
$L__BB89_4:
	bra.uni 	$L__BB89_5;
$L__BB89_5:
	ld.u32 	%r16, [%SP+4];
	add.s32 	%r17, %r16, 1;
	st.u32 	[%SP+4], %r17;
	ld.u32 	%r18, [%SP+12];
	add.s32 	%r19, %r18, 1;
	st.u32 	[%SP+12], %r19;
	ld.u32 	%r20, [%SP+8];
	add.s32 	%r21, %r20, -1;
	st.u32 	[%SP+8], %r21;
	bra.uni 	$L__BB89_1;
$L__BB89_6:
	ld.u32 	%r13, [%SP+8];
	setp.ne.s32 	%p5, %r13, 0;
	@%p5 bra 	$L__BB89_8;
	bra.uni 	$L__BB89_7;
$L__BB89_7:
	ld.u32 	%r14, [%SP+12];
	mov.u16 	%rs2, 0;
	st.u8 	[%r14], %rs2;
	bra.uni 	$L__BB89_8;
$L__BB89_8:
	ld.u32 	%r15, [%SP+0];
	st.param.b32 	[func_retval0+0], %r15;
	ret;

}
	// .globl	strnlen
.visible .func  (.param .b32 func_retval0) strnlen(
	.param .b32 strnlen_param_0,
	.param .b32 strnlen_param_1
)
{
	.local .align 4 .b8 	__local_depot90[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<13>;

	mov.u32 	%SPL, __local_depot90;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [strnlen_param_1];
	ld.param.u32 	%r1, [strnlen_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	mov.b32 	%r3, 0;
	st.u32 	[%SP+8], %r3;
	bra.uni 	$L__BB90_1;
$L__BB90_1:
	ld.u32 	%r4, [%SP+8];
	ld.u32 	%r5, [%SP+4];
	setp.ge.u32 	%p4, %r4, %r5;
	mov.pred 	%p3, 0;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB90_3;
	bra.uni 	$L__BB90_2;
$L__BB90_2:
	ld.u32 	%r6, [%SP+0];
	ld.u32 	%r7, [%SP+8];
	add.s32 	%r8, %r6, %r7;
	ld.s8 	%r9, [%r8];
	setp.ne.s32 	%p1, %r9, 0;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB90_3;
$L__BB90_3:
	mov.pred 	%p2, %p5;
	@!%p2 bra 	$L__BB90_6;
	bra.uni 	$L__BB90_4;
$L__BB90_4:
	bra.uni 	$L__BB90_5;
$L__BB90_5:
	ld.u32 	%r11, [%SP+8];
	add.s32 	%r12, %r11, 1;
	st.u32 	[%SP+8], %r12;
	bra.uni 	$L__BB90_1;
$L__BB90_6:
	ld.u32 	%r10, [%SP+8];
	st.param.b32 	[func_retval0+0], %r10;
	ret;

}
	// .globl	strpbrk
.visible .func  (.param .b32 func_retval0) strpbrk(
	.param .b32 strpbrk_param_0,
	.param .b32 strpbrk_param_1
)
{
	.local .align 4 .b8 	__local_depot91[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<18>;

	mov.u32 	%SPL, __local_depot91;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [strpbrk_param_1];
	ld.param.u32 	%r1, [strpbrk_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	bra.uni 	$L__BB91_1;
$L__BB91_1:
	ld.u32 	%r3, [%SP+4];
	ld.s8 	%r4, [%r3];
	setp.eq.s32 	%p1, %r4, 0;
	@%p1 bra 	$L__BB91_8;
	bra.uni 	$L__BB91_2;
$L__BB91_2:
	ld.u32 	%r6, [%SP+8];
	st.u32 	[%SP+12], %r6;
	bra.uni 	$L__BB91_3;
$L__BB91_3:
	ld.u32 	%r7, [%SP+12];
	ld.s8 	%r8, [%r7];
	setp.eq.s32 	%p2, %r8, 0;
	@%p2 bra 	$L__BB91_7;
	bra.uni 	$L__BB91_4;
$L__BB91_4:
	ld.u32 	%r11, [%SP+12];
	add.s32 	%r12, %r11, 1;
	st.u32 	[%SP+12], %r12;
	ld.s8 	%r13, [%r11];
	ld.u32 	%r14, [%SP+4];
	ld.s8 	%r15, [%r14];
	setp.ne.s32 	%p3, %r13, %r15;
	@%p3 bra 	$L__BB91_6;
	bra.uni 	$L__BB91_5;
$L__BB91_5:
	ld.u32 	%r16, [%SP+4];
	st.u32 	[%SP+0], %r16;
	bra.uni 	$L__BB91_9;
$L__BB91_6:
	bra.uni 	$L__BB91_3;
$L__BB91_7:
	ld.u32 	%r9, [%SP+4];
	add.s32 	%r10, %r9, 1;
	st.u32 	[%SP+4], %r10;
	bra.uni 	$L__BB91_1;
$L__BB91_8:
	mov.b32 	%r5, 0;
	st.u32 	[%SP+0], %r5;
	bra.uni 	$L__BB91_9;
$L__BB91_9:
	ld.u32 	%r17, [%SP+0];
	st.param.b32 	[func_retval0+0], %r17;
	ret;

}
	// .globl	strrchr
.visible .func  (.param .b32 func_retval0) strrchr(
	.param .b32 strrchr_param_0,
	.param .b32 strrchr_param_1
)
{
	.local .align 4 .b8 	__local_depot92[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<11>;

	mov.u32 	%SPL, __local_depot92;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [strrchr_param_1];
	ld.param.u32 	%r1, [strrchr_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	mov.b32 	%r3, 0;
	st.u32 	[%SP+8], %r3;
	bra.uni 	$L__BB92_1;
$L__BB92_1:
	ld.u32 	%r4, [%SP+0];
	ld.s8 	%r5, [%r4];
	ld.u32 	%r6, [%SP+4];
	setp.ne.s32 	%p1, %r5, %r6;
	@%p1 bra 	$L__BB92_3;
	bra.uni 	$L__BB92_2;
$L__BB92_2:
	ld.u32 	%r7, [%SP+0];
	st.u32 	[%SP+8], %r7;
	bra.uni 	$L__BB92_3;
$L__BB92_3:
	bra.uni 	$L__BB92_4;
$L__BB92_4:
	ld.u32 	%r8, [%SP+0];
	add.s32 	%r9, %r8, 1;
	st.u32 	[%SP+0], %r9;
	ld.u8 	%rs1, [%r8];
	setp.ne.s16 	%p2, %rs1, 0;
	@%p2 bra 	$L__BB92_1;
	bra.uni 	$L__BB92_5;
$L__BB92_5:
	ld.u32 	%r10, [%SP+8];
	st.param.b32 	[func_retval0+0], %r10;
	ret;

}
	// .globl	strstr
.visible .func  (.param .b32 func_retval0) strstr(
	.param .b32 strstr_param_0,
	.param .b32 strstr_param_1
)
{
	.local .align 4 .b8 	__local_depot93[20];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<24>;

	mov.u32 	%SPL, __local_depot93;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [strstr_param_1];
	ld.param.u32 	%r1, [strstr_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	ld.u32 	%r3, [%SP+4];
	st.u32 	[%SP+12], %r3;
	ld.u32 	%r4, [%SP+8];
	{ // callseq 9, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r4;
	.param .b32 retval0;
	call.uni (retval0), 
	strlen, 
	(
	param0
	);
	ld.param.b32 	%r5, [retval0+0];
	} // callseq 9
	st.u32 	[%SP+16], %r5;
	ld.u32 	%r7, [%SP+16];
	setp.ne.s32 	%p1, %r7, 0;
	@%p1 bra 	$L__BB93_2;
	bra.uni 	$L__BB93_1;
$L__BB93_1:
	ld.u32 	%r8, [%SP+4];
	st.u32 	[%SP+0], %r8;
	bra.uni 	$L__BB93_9;
$L__BB93_2:
	bra.uni 	$L__BB93_3;
$L__BB93_3:
	ld.u32 	%r9, [%SP+12];
	ld.u32 	%r10, [%SP+8];
	ld.s8 	%r11, [%r10];
	{ // callseq 10, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r9;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r11;
	.param .b32 retval0;
	call.uni (retval0), 
	strchr, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r12, [retval0+0];
	} // callseq 10
	st.u32 	[%SP+12], %r12;
	setp.eq.s32 	%p2, %r12, 0;
	@%p2 bra 	$L__BB93_8;
	bra.uni 	$L__BB93_4;
$L__BB93_4:
	ld.u32 	%r15, [%SP+12];
	ld.u32 	%r16, [%SP+8];
	ld.u32 	%r17, [%SP+16];
	{ // callseq 11, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r15;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r16;
	.param .b32 param2;
	st.param.b32 	[param2+0], %r17;
	.param .b32 retval0;
	call.uni (retval0), 
	strncmp, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r18, [retval0+0];
	} // callseq 11
	setp.ne.s32 	%p3, %r18, 0;
	@%p3 bra 	$L__BB93_6;
	bra.uni 	$L__BB93_5;
$L__BB93_5:
	ld.u32 	%r22, [%SP+12];
	st.u32 	[%SP+0], %r22;
	bra.uni 	$L__BB93_9;
$L__BB93_6:
	bra.uni 	$L__BB93_7;
$L__BB93_7:
	ld.u32 	%r20, [%SP+12];
	add.s32 	%r21, %r20, 1;
	st.u32 	[%SP+12], %r21;
	bra.uni 	$L__BB93_3;
$L__BB93_8:
	mov.b32 	%r14, 0;
	st.u32 	[%SP+0], %r14;
	bra.uni 	$L__BB93_9;
$L__BB93_9:
	ld.u32 	%r23, [%SP+0];
	st.param.b32 	[func_retval0+0], %r23;
	ret;

}
	// .globl	copysign
.visible .func  (.param .b64 func_retval0) copysign(
	.param .b64 copysign_param_0,
	.param .b64 copysign_param_1
)
{
	.local .align 8 .b8 	__local_depot94[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<5>;
	.reg .f64 	%fd<11>;

	mov.u32 	%SPL, __local_depot94;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f64 	%fd2, [copysign_param_1];
	ld.param.f64 	%fd1, [copysign_param_0];
	st.f64 	[%SP+8], %fd1;
	st.f64 	[%SP+16], %fd2;
	ld.f64 	%fd3, [%SP+8];
	setp.geu.f64 	%p1, %fd3, 0d0000000000000000;
	@%p1 bra 	$L__BB94_2;
	bra.uni 	$L__BB94_1;
$L__BB94_1:
	ld.f64 	%fd4, [%SP+16];
	setp.gt.f64 	%p2, %fd4, 0d0000000000000000;
	@%p2 bra 	$L__BB94_4;
	bra.uni 	$L__BB94_2;
$L__BB94_2:
	ld.f64 	%fd5, [%SP+8];
	setp.leu.f64 	%p3, %fd5, 0d0000000000000000;
	@%p3 bra 	$L__BB94_5;
	bra.uni 	$L__BB94_3;
$L__BB94_3:
	ld.f64 	%fd6, [%SP+16];
	setp.geu.f64 	%p4, %fd6, 0d0000000000000000;
	@%p4 bra 	$L__BB94_5;
	bra.uni 	$L__BB94_4;
$L__BB94_4:
	ld.f64 	%fd8, [%SP+8];
	neg.f64 	%fd9, %fd8;
	st.f64 	[%SP+0], %fd9;
	bra.uni 	$L__BB94_6;
$L__BB94_5:
	ld.f64 	%fd7, [%SP+8];
	st.f64 	[%SP+0], %fd7;
	bra.uni 	$L__BB94_6;
$L__BB94_6:
	ld.f64 	%fd10, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd10;
	ret;

}
	// .globl	memmem
.visible .func  (.param .b32 func_retval0) memmem(
	.param .b32 memmem_param_0,
	.param .b32 memmem_param_1,
	.param .b32 memmem_param_2,
	.param .b32 memmem_param_3
)
{
	.local .align 4 .b8 	__local_depot95[28];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<35>;

	mov.u32 	%SPL, __local_depot95;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r4, [memmem_param_3];
	ld.param.u32 	%r3, [memmem_param_2];
	ld.param.u32 	%r2, [memmem_param_1];
	ld.param.u32 	%r1, [memmem_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u32 	[%SP+12], %r3;
	st.u32 	[%SP+16], %r4;
	ld.u32 	%r5, [%SP+4];
	ld.u32 	%r6, [%SP+8];
	add.s32 	%r7, %r5, %r6;
	ld.u32 	%r8, [%SP+16];
	sub.s32 	%r9, %r7, %r8;
	st.u32 	[%SP+24], %r9;
	ld.u32 	%r10, [%SP+16];
	setp.ne.s32 	%p1, %r10, 0;
	@%p1 bra 	$L__BB95_2;
	bra.uni 	$L__BB95_1;
$L__BB95_1:
	ld.u32 	%r33, [%SP+4];
	st.u32 	[%SP+0], %r33;
	bra.uni 	$L__BB95_12;
$L__BB95_2:
	ld.u32 	%r11, [%SP+8];
	ld.u32 	%r12, [%SP+16];
	setp.ge.u32 	%p2, %r11, %r12;
	@%p2 bra 	$L__BB95_4;
	bra.uni 	$L__BB95_3;
$L__BB95_3:
	mov.b32 	%r32, 0;
	st.u32 	[%SP+0], %r32;
	bra.uni 	$L__BB95_12;
$L__BB95_4:
	ld.u32 	%r13, [%SP+4];
	st.u32 	[%SP+20], %r13;
	bra.uni 	$L__BB95_5;
$L__BB95_5:
	ld.u32 	%r14, [%SP+20];
	ld.u32 	%r15, [%SP+24];
	setp.gt.u32 	%p3, %r14, %r15;
	@%p3 bra 	$L__BB95_11;
	bra.uni 	$L__BB95_6;
$L__BB95_6:
	ld.u32 	%r17, [%SP+20];
	ld.s8 	%r18, [%r17];
	ld.u32 	%r19, [%SP+12];
	ld.s8 	%r20, [%r19];
	setp.ne.s32 	%p4, %r18, %r20;
	@%p4 bra 	$L__BB95_9;
	bra.uni 	$L__BB95_7;
$L__BB95_7:
	ld.u32 	%r21, [%SP+20];
	add.s32 	%r22, %r21, 1;
	ld.u32 	%r23, [%SP+12];
	add.s32 	%r24, %r23, 1;
	ld.u32 	%r25, [%SP+16];
	add.s32 	%r26, %r25, -1;
	{ // callseq 12, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r22;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r24;
	.param .b32 param2;
	st.param.b32 	[param2+0], %r26;
	.param .b32 retval0;
	call.uni (retval0), 
	memcmp, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r27, [retval0+0];
	} // callseq 12
	setp.ne.s32 	%p5, %r27, 0;
	@%p5 bra 	$L__BB95_9;
	bra.uni 	$L__BB95_8;
$L__BB95_8:
	ld.u32 	%r29, [%SP+20];
	st.u32 	[%SP+0], %r29;
	bra.uni 	$L__BB95_12;
$L__BB95_9:
	bra.uni 	$L__BB95_10;
$L__BB95_10:
	ld.u32 	%r30, [%SP+20];
	add.s32 	%r31, %r30, 1;
	st.u32 	[%SP+20], %r31;
	bra.uni 	$L__BB95_5;
$L__BB95_11:
	mov.b32 	%r16, 0;
	st.u32 	[%SP+0], %r16;
	bra.uni 	$L__BB95_12;
$L__BB95_12:
	ld.u32 	%r34, [%SP+0];
	st.param.b32 	[func_retval0+0], %r34;
	ret;

}
	// .globl	mempcpy
.visible .func  (.param .b32 func_retval0) mempcpy(
	.param .b32 mempcpy_param_0,
	.param .b32 mempcpy_param_1,
	.param .b32 mempcpy_param_2
)
{
	.local .align 4 .b8 	__local_depot96[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<15>;

	mov.u32 	%SPL, __local_depot96;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r8, [mempcpy_param_2];
	ld.param.u32 	%r7, [mempcpy_param_1];
	ld.param.u32 	%r6, [mempcpy_param_0];
	st.u32 	[%SP+0], %r6;
	st.u32 	[%SP+4], %r7;
	st.u32 	[%SP+8], %r8;
	ld.u32 	%r1, [%SP+0];
	ld.u32 	%r2, [%SP+4];
	ld.u32 	%r3, [%SP+8];
	setp.eq.s32 	%p1, %r3, 0;
	mov.b32 	%r9, 0;
	mov.u32 	%r14, %r9;
	@%p1 bra 	$L__BB96_2;
	bra.uni 	$L__BB96_1;
$L__BB96_1:
	mov.u32 	%r4, %r14;
	add.s32 	%r10, %r2, %r4;
	ld.u8 	%rs1, [%r10];
	add.s32 	%r11, %r1, %r4;
	st.u8 	[%r11], %rs1;
	add.s32 	%r5, %r4, 1;
	setp.lt.u32 	%p2, %r5, %r3;
	mov.u32 	%r14, %r5;
	@%p2 bra 	$L__BB96_1;
	bra.uni 	$L__BB96_2;
$L__BB96_2:
	ld.u32 	%r12, [%SP+8];
	add.s32 	%r13, %r1, %r12;
	st.param.b32 	[func_retval0+0], %r13;
	ret;

}
	// .globl	frexp
.visible .func  (.param .b64 func_retval0) frexp(
	.param .b64 frexp_param_0,
	.param .b32 frexp_param_1
)
{
	.local .align 8 .b8 	__local_depot97[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f64 	%fd<17>;

	mov.u32 	%SPL, __local_depot97;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [frexp_param_1];
	ld.param.f64 	%fd1, [frexp_param_0];
	st.f64 	[%SP+0], %fd1;
	st.u32 	[%SP+8], %r1;
	mov.b32 	%r2, 0;
	st.u32 	[%SP+16], %r2;
	st.u32 	[%SP+12], %r2;
	ld.f64 	%fd2, [%SP+0];
	setp.geu.f64 	%p1, %fd2, 0d0000000000000000;
	@%p1 bra 	$L__BB97_2;
	bra.uni 	$L__BB97_1;
$L__BB97_1:
	ld.f64 	%fd3, [%SP+0];
	neg.f64 	%fd4, %fd3;
	st.f64 	[%SP+0], %fd4;
	mov.b32 	%r3, 1;
	st.u32 	[%SP+12], %r3;
	bra.uni 	$L__BB97_2;
$L__BB97_2:
	ld.f64 	%fd5, [%SP+0];
	setp.ltu.f64 	%p2, %fd5, 0d3FF0000000000000;
	@%p2 bra 	$L__BB97_7;
	bra.uni 	$L__BB97_3;
$L__BB97_3:
	bra.uni 	$L__BB97_4;
$L__BB97_4:
	ld.f64 	%fd11, [%SP+0];
	setp.ltu.f64 	%p6, %fd11, 0d3FF0000000000000;
	@%p6 bra 	$L__BB97_6;
	bra.uni 	$L__BB97_5;
$L__BB97_5:
	ld.u32 	%r9, [%SP+16];
	add.s32 	%r10, %r9, 1;
	st.u32 	[%SP+16], %r10;
	ld.f64 	%fd15, [%SP+0];
	mul.rn.f64 	%fd16, %fd15, 0d3FE0000000000000;
	st.f64 	[%SP+0], %fd16;
	bra.uni 	$L__BB97_4;
$L__BB97_6:
	bra.uni 	$L__BB97_14;
$L__BB97_7:
	ld.f64 	%fd6, [%SP+0];
	setp.geu.f64 	%p3, %fd6, 0d3FE0000000000000;
	@%p3 bra 	$L__BB97_13;
	bra.uni 	$L__BB97_8;
$L__BB97_8:
	ld.f64 	%fd7, [%SP+0];
	setp.eq.f64 	%p4, %fd7, 0d0000000000000000;
	@%p4 bra 	$L__BB97_13;
	bra.uni 	$L__BB97_9;
$L__BB97_9:
	bra.uni 	$L__BB97_10;
$L__BB97_10:
	ld.f64 	%fd8, [%SP+0];
	setp.geu.f64 	%p5, %fd8, 0d3FE0000000000000;
	@%p5 bra 	$L__BB97_12;
	bra.uni 	$L__BB97_11;
$L__BB97_11:
	ld.u32 	%r4, [%SP+16];
	add.s32 	%r5, %r4, -1;
	st.u32 	[%SP+16], %r5;
	ld.f64 	%fd9, [%SP+0];
	add.rn.f64 	%fd10, %fd9, %fd9;
	st.f64 	[%SP+0], %fd10;
	bra.uni 	$L__BB97_10;
$L__BB97_12:
	bra.uni 	$L__BB97_13;
$L__BB97_13:
	bra.uni 	$L__BB97_14;
$L__BB97_14:
	ld.u32 	%r6, [%SP+16];
	ld.u32 	%r7, [%SP+8];
	st.u32 	[%r7], %r6;
	ld.u32 	%r8, [%SP+12];
	setp.eq.s32 	%p7, %r8, 0;
	@%p7 bra 	$L__BB97_16;
	bra.uni 	$L__BB97_15;
$L__BB97_15:
	ld.f64 	%fd12, [%SP+0];
	neg.f64 	%fd13, %fd12;
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB97_16;
$L__BB97_16:
	ld.f64 	%fd14, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd14;
	ret;

}
	// .globl	__muldi3
.visible .func  (.param .b64 func_retval0) __muldi3(
	.param .b64 __muldi3_param_0,
	.param .b64 __muldi3_param_1
)
{
	.local .align 8 .b8 	__local_depot98[32];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b64 	%rd<16>;

	mov.u32 	%SPL, __local_depot98;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd2, [__muldi3_param_1];
	ld.param.u64 	%rd1, [__muldi3_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	mov.u64 	%rd3, 0;
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+0];
	st.u64 	[%SP+24], %rd4;
	bra.uni 	$L__BB98_1;
$L__BB98_1:
	ld.u64 	%rd5, [%SP+24];
	setp.eq.s64 	%p1, %rd5, 0;
	@%p1 bra 	$L__BB98_5;
	bra.uni 	$L__BB98_2;
$L__BB98_2:
	ld.u64 	%rd7, [%SP+24];
	and.b64  	%rd8, %rd7, 1;
	setp.eq.b64 	%p2, %rd8, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB98_4;
	bra.uni 	$L__BB98_3;
$L__BB98_3:
	ld.u64 	%rd9, [%SP+8];
	ld.u64 	%rd10, [%SP+16];
	add.s64 	%rd11, %rd10, %rd9;
	st.u64 	[%SP+16], %rd11;
	bra.uni 	$L__BB98_4;
$L__BB98_4:
	ld.u64 	%rd12, [%SP+8];
	shl.b64 	%rd13, %rd12, 1;
	st.u64 	[%SP+8], %rd13;
	ld.u64 	%rd14, [%SP+24];
	shr.u64 	%rd15, %rd14, 1;
	st.u64 	[%SP+24], %rd15;
	bra.uni 	$L__BB98_1;
$L__BB98_5:
	ld.u64 	%rd6, [%SP+16];
	st.param.b64 	[func_retval0+0], %rd6;
	ret;

}
	// .globl	udivmodsi4
.visible .func  (.param .b32 func_retval0) udivmodsi4(
	.param .b32 udivmodsi4_param_0,
	.param .b32 udivmodsi4_param_1,
	.param .b32 udivmodsi4_param_2
)
{
	.local .align 4 .b8 	__local_depot99[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<32>;

	mov.u32 	%SPL, __local_depot99;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [udivmodsi4_param_2];
	ld.param.u32 	%r2, [udivmodsi4_param_1];
	ld.param.u32 	%r1, [udivmodsi4_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u32 	[%SP+12], %r3;
	mov.b32 	%r4, 1;
	st.u32 	[%SP+16], %r4;
	mov.b32 	%r5, 0;
	st.u32 	[%SP+20], %r5;
	bra.uni 	$L__BB99_1;
$L__BB99_1:
	ld.u32 	%r6, [%SP+8];
	ld.u32 	%r7, [%SP+4];
	setp.ge.u32 	%p4, %r6, %r7;
	mov.pred 	%p3, 0;
	mov.pred 	%p10, %p3;
	@%p4 bra 	$L__BB99_4;
	bra.uni 	$L__BB99_2;
$L__BB99_2:
	ld.u32 	%r8, [%SP+16];
	setp.eq.s32 	%p6, %r8, 0;
	mov.pred 	%p5, 0;
	mov.pred 	%p10, %p5;
	@%p6 bra 	$L__BB99_4;
	bra.uni 	$L__BB99_3;
$L__BB99_3:
	add.u32 	%r9, %SP, 8;
	or.b32  	%r10, %r9, 3;
	ld.u8 	%rs1, [%r10];
	and.b16  	%rs2, %rs1, 128;
	setp.eq.s16 	%p1, %rs2, 0;
	mov.pred 	%p10, %p1;
	bra.uni 	$L__BB99_4;
$L__BB99_4:
	mov.pred 	%p2, %p10;
	@!%p2 bra 	$L__BB99_6;
	bra.uni 	$L__BB99_5;
$L__BB99_5:
	ld.u32 	%r28, [%SP+8];
	shl.b32 	%r29, %r28, 1;
	st.u32 	[%SP+8], %r29;
	ld.u32 	%r30, [%SP+16];
	shl.b32 	%r31, %r30, 1;
	st.u32 	[%SP+16], %r31;
	bra.uni 	$L__BB99_1;
$L__BB99_6:
	bra.uni 	$L__BB99_7;
$L__BB99_7:
	ld.u32 	%r11, [%SP+16];
	setp.eq.s32 	%p7, %r11, 0;
	@%p7 bra 	$L__BB99_11;
	bra.uni 	$L__BB99_8;
$L__BB99_8:
	ld.u32 	%r16, [%SP+4];
	ld.u32 	%r17, [%SP+8];
	setp.lt.u32 	%p9, %r16, %r17;
	@%p9 bra 	$L__BB99_10;
	bra.uni 	$L__BB99_9;
$L__BB99_9:
	ld.u32 	%r18, [%SP+8];
	ld.u32 	%r19, [%SP+4];
	sub.s32 	%r20, %r19, %r18;
	st.u32 	[%SP+4], %r20;
	ld.u32 	%r21, [%SP+16];
	ld.u32 	%r22, [%SP+20];
	or.b32  	%r23, %r22, %r21;
	st.u32 	[%SP+20], %r23;
	bra.uni 	$L__BB99_10;
$L__BB99_10:
	ld.u32 	%r24, [%SP+16];
	shr.u32 	%r25, %r24, 1;
	st.u32 	[%SP+16], %r25;
	ld.u32 	%r26, [%SP+8];
	shr.u32 	%r27, %r26, 1;
	st.u32 	[%SP+8], %r27;
	bra.uni 	$L__BB99_7;
$L__BB99_11:
	ld.u32 	%r12, [%SP+12];
	setp.eq.s32 	%p8, %r12, 0;
	@%p8 bra 	$L__BB99_13;
	bra.uni 	$L__BB99_12;
$L__BB99_12:
	ld.u32 	%r14, [%SP+4];
	st.u32 	[%SP+0], %r14;
	bra.uni 	$L__BB99_14;
$L__BB99_13:
	ld.u32 	%r13, [%SP+20];
	st.u32 	[%SP+0], %r13;
	bra.uni 	$L__BB99_14;
$L__BB99_14:
	ld.u32 	%r15, [%SP+0];
	st.param.b32 	[func_retval0+0], %r15;
	ret;

}
	// .globl	__clrsbqi2
.visible .func  (.param .b32 func_retval0) __clrsbqi2(
	.param .b32 __clrsbqi2_param_0
)
{
	.local .align 4 .b8 	__local_depot100[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<12>;

	mov.u32 	%SPL, __local_depot100;
	cvta.local.u32 	%SP, %SPL;
	ld.param.s8 	%rs1, [__clrsbqi2_param_0];
	st.u8 	[%SP+4], %rs1;
	ld.s8 	%r1, [%SP+4];
	setp.gt.s32 	%p1, %r1, -1;
	@%p1 bra 	$L__BB100_2;
	bra.uni 	$L__BB100_1;
$L__BB100_1:
	ld.u8 	%r2, [%SP+4];
	not.b32 	%r3, %r2;
	st.u8 	[%SP+4], %r3;
	bra.uni 	$L__BB100_2;
$L__BB100_2:
	ld.s8 	%r4, [%SP+4];
	setp.ne.s32 	%p2, %r4, 0;
	@%p2 bra 	$L__BB100_4;
	bra.uni 	$L__BB100_3;
$L__BB100_3:
	mov.b32 	%r10, 7;
	st.u32 	[%SP+0], %r10;
	bra.uni 	$L__BB100_5;
$L__BB100_4:
	ld.s8 	%r5, [%SP+4];
	shl.b32 	%r6, %r5, 8;
	clz.b32 	%r7, %r6;
	st.u32 	[%SP+8], %r7;
	ld.u32 	%r8, [%SP+8];
	add.s32 	%r9, %r8, -1;
	st.u32 	[%SP+0], %r9;
	bra.uni 	$L__BB100_5;
$L__BB100_5:
	ld.u32 	%r11, [%SP+0];
	st.param.b32 	[func_retval0+0], %r11;
	ret;

}
	// .globl	__clrsbdi2
.visible .func  (.param .b32 func_retval0) __clrsbdi2(
	.param .b64 __clrsbdi2_param_0
)
{
	.local .align 8 .b8 	__local_depot101[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;

	mov.u32 	%SPL, __local_depot101;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd1, [__clrsbdi2_param_0];
	st.u64 	[%SP+8], %rd1;
	ld.u64 	%rd2, [%SP+8];
	setp.gt.s64 	%p1, %rd2, -1;
	@%p1 bra 	$L__BB101_2;
	bra.uni 	$L__BB101_1;
$L__BB101_1:
	ld.u64 	%rd3, [%SP+8];
	not.b64 	%rd4, %rd3;
	st.u64 	[%SP+8], %rd4;
	bra.uni 	$L__BB101_2;
$L__BB101_2:
	ld.u64 	%rd5, [%SP+8];
	setp.ne.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB101_4;
	bra.uni 	$L__BB101_3;
$L__BB101_3:
	mov.b32 	%r4, 63;
	st.u32 	[%SP+0], %r4;
	bra.uni 	$L__BB101_5;
$L__BB101_4:
	ld.u64 	%rd6, [%SP+8];
	clz.b64 	%r1, %rd6;
	cvt.u64.u32 	%rd7, %r1;
	st.u32 	[%SP+16], %rd7;
	ld.u32 	%r2, [%SP+16];
	add.s32 	%r3, %r2, -1;
	st.u32 	[%SP+0], %r3;
	bra.uni 	$L__BB101_5;
$L__BB101_5:
	ld.u32 	%r5, [%SP+0];
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	__mulsi3
.visible .func  (.param .b32 func_retval0) __mulsi3(
	.param .b32 __mulsi3_param_0,
	.param .b32 __mulsi3_param_1
)
{
	.local .align 4 .b8 	__local_depot102[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<15>;

	mov.u32 	%SPL, __local_depot102;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [__mulsi3_param_1];
	ld.param.u32 	%r1, [__mulsi3_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	mov.b32 	%r3, 0;
	st.u32 	[%SP+8], %r3;
	bra.uni 	$L__BB102_1;
$L__BB102_1:
	ld.u32 	%r4, [%SP+0];
	setp.eq.s32 	%p1, %r4, 0;
	@%p1 bra 	$L__BB102_5;
	bra.uni 	$L__BB102_2;
$L__BB102_2:
	ld.u32 	%r6, [%SP+0];
	and.b32  	%r7, %r6, 1;
	setp.eq.b32 	%p2, %r7, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB102_4;
	bra.uni 	$L__BB102_3;
$L__BB102_3:
	ld.u32 	%r8, [%SP+4];
	ld.u32 	%r9, [%SP+8];
	add.s32 	%r10, %r9, %r8;
	st.u32 	[%SP+8], %r10;
	bra.uni 	$L__BB102_4;
$L__BB102_4:
	ld.u32 	%r11, [%SP+0];
	shr.u32 	%r12, %r11, 1;
	st.u32 	[%SP+0], %r12;
	ld.u32 	%r13, [%SP+4];
	shl.b32 	%r14, %r13, 1;
	st.u32 	[%SP+4], %r14;
	bra.uni 	$L__BB102_1;
$L__BB102_5:
	ld.u32 	%r5, [%SP+8];
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	__cmovd
.visible .func __cmovd(
	.param .b32 __cmovd_param_0,
	.param .b32 __cmovd_param_1,
	.param .b32 __cmovd_param_2
)
{
	.local .align 4 .b8 	__local_depot103[32];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<2>;

	mov.u32 	%SPL, __local_depot103;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [__cmovd_param_2];
	ld.param.u32 	%r2, [__cmovd_param_1];
	ld.param.u32 	%r1, [__cmovd_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+8];
	shr.u32 	%r5, %r4, 3;
	st.u32 	[%SP+16], %r5;
	ld.u32 	%r6, [%SP+8];
	and.b32  	%r7, %r6, -8;
	st.u32 	[%SP+20], %r7;
	ld.u32 	%r8, [%SP+0];
	st.u32 	[%SP+24], %r8;
	ld.u32 	%r9, [%SP+4];
	st.u32 	[%SP+28], %r9;
	ld.u32 	%r10, [%SP+24];
	ld.u32 	%r11, [%SP+28];
	setp.lt.u32 	%p1, %r10, %r11;
	@%p1 bra 	$L__BB103_2;
	bra.uni 	$L__BB103_1;
$L__BB103_1:
	ld.u32 	%r12, [%SP+24];
	ld.u32 	%r13, [%SP+28];
	ld.u32 	%r14, [%SP+8];
	add.s32 	%r15, %r13, %r14;
	setp.le.u32 	%p2, %r12, %r15;
	@%p2 bra 	$L__BB103_10;
	bra.uni 	$L__BB103_2;
$L__BB103_2:
	mov.b32 	%r23, 0;
	st.u32 	[%SP+12], %r23;
	bra.uni 	$L__BB103_3;
$L__BB103_3:
	ld.u32 	%r24, [%SP+12];
	ld.u32 	%r25, [%SP+16];
	setp.ge.u32 	%p4, %r24, %r25;
	@%p4 bra 	$L__BB103_6;
	bra.uni 	$L__BB103_4;
$L__BB103_4:
	ld.u32 	%r35, [%SP+4];
	ld.u32 	%r36, [%SP+12];
	shl.b32 	%r37, %r36, 3;
	add.s32 	%r38, %r35, %r37;
	ld.u64 	%rd1, [%r38];
	ld.u32 	%r39, [%SP+0];
	add.s32 	%r40, %r39, %r37;
	st.u64 	[%r40], %rd1;
	bra.uni 	$L__BB103_5;
$L__BB103_5:
	ld.u32 	%r41, [%SP+12];
	add.s32 	%r42, %r41, 1;
	st.u32 	[%SP+12], %r42;
	bra.uni 	$L__BB103_3;
$L__BB103_6:
	bra.uni 	$L__BB103_7;
$L__BB103_7:
	ld.u32 	%r26, [%SP+8];
	ld.u32 	%r27, [%SP+20];
	setp.le.u32 	%p5, %r26, %r27;
	@%p5 bra 	$L__BB103_9;
	bra.uni 	$L__BB103_8;
$L__BB103_8:
	ld.u32 	%r28, [%SP+28];
	ld.u32 	%r29, [%SP+20];
	add.s32 	%r30, %r28, %r29;
	ld.u8 	%rs2, [%r30];
	ld.u32 	%r31, [%SP+24];
	add.s32 	%r32, %r31, %r29;
	st.u8 	[%r32], %rs2;
	ld.u32 	%r33, [%SP+20];
	add.s32 	%r34, %r33, 1;
	st.u32 	[%SP+20], %r34;
	bra.uni 	$L__BB103_7;
$L__BB103_9:
	bra.uni 	$L__BB103_14;
$L__BB103_10:
	bra.uni 	$L__BB103_11;
$L__BB103_11:
	ld.u32 	%r16, [%SP+8];
	add.s32 	%r17, %r16, -1;
	st.u32 	[%SP+8], %r17;
	setp.eq.s32 	%p3, %r16, 0;
	@%p3 bra 	$L__BB103_13;
	bra.uni 	$L__BB103_12;
$L__BB103_12:
	ld.u32 	%r18, [%SP+28];
	ld.u32 	%r19, [%SP+8];
	add.s32 	%r20, %r18, %r19;
	ld.u8 	%rs1, [%r20];
	ld.u32 	%r21, [%SP+24];
	add.s32 	%r22, %r21, %r19;
	st.u8 	[%r22], %rs1;
	bra.uni 	$L__BB103_11;
$L__BB103_13:
	bra.uni 	$L__BB103_14;
$L__BB103_14:
	ret;

}
	// .globl	__cmovh
.visible .func __cmovh(
	.param .b32 __cmovh_param_0,
	.param .b32 __cmovh_param_1,
	.param .b32 __cmovh_param_2
)
{
	.local .align 4 .b8 	__local_depot104[28];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<40>;

	mov.u32 	%SPL, __local_depot104;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [__cmovh_param_2];
	ld.param.u32 	%r2, [__cmovh_param_1];
	ld.param.u32 	%r1, [__cmovh_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+8];
	shr.u32 	%r5, %r4, 1;
	st.u32 	[%SP+16], %r5;
	ld.u32 	%r6, [%SP+0];
	st.u32 	[%SP+20], %r6;
	ld.u32 	%r7, [%SP+4];
	st.u32 	[%SP+24], %r7;
	ld.u32 	%r8, [%SP+20];
	ld.u32 	%r9, [%SP+24];
	setp.lt.u32 	%p1, %r8, %r9;
	@%p1 bra 	$L__BB104_2;
	bra.uni 	$L__BB104_1;
$L__BB104_1:
	ld.u32 	%r10, [%SP+20];
	ld.u32 	%r11, [%SP+24];
	ld.u32 	%r12, [%SP+8];
	add.s32 	%r13, %r11, %r12;
	setp.le.u32 	%p2, %r10, %r13;
	@%p2 bra 	$L__BB104_9;
	bra.uni 	$L__BB104_2;
$L__BB104_2:
	mov.b32 	%r21, 0;
	st.u32 	[%SP+12], %r21;
	bra.uni 	$L__BB104_3;
$L__BB104_3:
	ld.u32 	%r22, [%SP+12];
	ld.u32 	%r23, [%SP+16];
	setp.ge.u32 	%p4, %r22, %r23;
	@%p4 bra 	$L__BB104_6;
	bra.uni 	$L__BB104_4;
$L__BB104_4:
	ld.u32 	%r32, [%SP+4];
	ld.u32 	%r33, [%SP+12];
	shl.b32 	%r34, %r33, 1;
	add.s32 	%r35, %r32, %r34;
	ld.u16 	%rs3, [%r35];
	ld.u32 	%r36, [%SP+0];
	add.s32 	%r37, %r36, %r34;
	st.u16 	[%r37], %rs3;
	bra.uni 	$L__BB104_5;
$L__BB104_5:
	ld.u32 	%r38, [%SP+12];
	add.s32 	%r39, %r38, 1;
	st.u32 	[%SP+12], %r39;
	bra.uni 	$L__BB104_3;
$L__BB104_6:
	ld.u32 	%r24, [%SP+8];
	and.b32  	%r25, %r24, 1;
	setp.eq.b32 	%p5, %r25, 1;
	mov.pred 	%p6, 0;
	xor.pred  	%p7, %p5, %p6;
	not.pred 	%p8, %p7;
	@%p8 bra 	$L__BB104_8;
	bra.uni 	$L__BB104_7;
$L__BB104_7:
	ld.u32 	%r26, [%SP+24];
	ld.u32 	%r27, [%SP+8];
	add.s32 	%r28, %r27, -1;
	add.s32 	%r29, %r26, %r28;
	ld.u8 	%rs2, [%r29];
	ld.u32 	%r30, [%SP+20];
	add.s32 	%r31, %r30, %r28;
	st.u8 	[%r31], %rs2;
	bra.uni 	$L__BB104_8;
$L__BB104_8:
	bra.uni 	$L__BB104_13;
$L__BB104_9:
	bra.uni 	$L__BB104_10;
$L__BB104_10:
	ld.u32 	%r14, [%SP+8];
	add.s32 	%r15, %r14, -1;
	st.u32 	[%SP+8], %r15;
	setp.eq.s32 	%p3, %r14, 0;
	@%p3 bra 	$L__BB104_12;
	bra.uni 	$L__BB104_11;
$L__BB104_11:
	ld.u32 	%r16, [%SP+24];
	ld.u32 	%r17, [%SP+8];
	add.s32 	%r18, %r16, %r17;
	ld.u8 	%rs1, [%r18];
	ld.u32 	%r19, [%SP+20];
	add.s32 	%r20, %r19, %r17;
	st.u8 	[%r20], %rs1;
	bra.uni 	$L__BB104_10;
$L__BB104_12:
	bra.uni 	$L__BB104_13;
$L__BB104_13:
	ret;

}
	// .globl	__cmovw
.visible .func __cmovw(
	.param .b32 __cmovw_param_0,
	.param .b32 __cmovw_param_1,
	.param .b32 __cmovw_param_2
)
{
	.local .align 4 .b8 	__local_depot105[32];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<44>;

	mov.u32 	%SPL, __local_depot105;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [__cmovw_param_2];
	ld.param.u32 	%r2, [__cmovw_param_1];
	ld.param.u32 	%r1, [__cmovw_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+8];
	shr.u32 	%r5, %r4, 2;
	st.u32 	[%SP+16], %r5;
	ld.u32 	%r6, [%SP+8];
	and.b32  	%r7, %r6, -4;
	st.u32 	[%SP+20], %r7;
	ld.u32 	%r8, [%SP+0];
	st.u32 	[%SP+24], %r8;
	ld.u32 	%r9, [%SP+4];
	st.u32 	[%SP+28], %r9;
	ld.u32 	%r10, [%SP+24];
	ld.u32 	%r11, [%SP+28];
	setp.lt.u32 	%p1, %r10, %r11;
	@%p1 bra 	$L__BB105_2;
	bra.uni 	$L__BB105_1;
$L__BB105_1:
	ld.u32 	%r12, [%SP+24];
	ld.u32 	%r13, [%SP+28];
	ld.u32 	%r14, [%SP+8];
	add.s32 	%r15, %r13, %r14;
	setp.le.u32 	%p2, %r12, %r15;
	@%p2 bra 	$L__BB105_10;
	bra.uni 	$L__BB105_2;
$L__BB105_2:
	mov.b32 	%r23, 0;
	st.u32 	[%SP+12], %r23;
	bra.uni 	$L__BB105_3;
$L__BB105_3:
	ld.u32 	%r24, [%SP+12];
	ld.u32 	%r25, [%SP+16];
	setp.ge.u32 	%p4, %r24, %r25;
	@%p4 bra 	$L__BB105_6;
	bra.uni 	$L__BB105_4;
$L__BB105_4:
	ld.u32 	%r35, [%SP+4];
	ld.u32 	%r36, [%SP+12];
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r38, %r35, %r37;
	ld.u32 	%r39, [%r38];
	ld.u32 	%r40, [%SP+0];
	add.s32 	%r41, %r40, %r37;
	st.u32 	[%r41], %r39;
	bra.uni 	$L__BB105_5;
$L__BB105_5:
	ld.u32 	%r42, [%SP+12];
	add.s32 	%r43, %r42, 1;
	st.u32 	[%SP+12], %r43;
	bra.uni 	$L__BB105_3;
$L__BB105_6:
	bra.uni 	$L__BB105_7;
$L__BB105_7:
	ld.u32 	%r26, [%SP+8];
	ld.u32 	%r27, [%SP+20];
	setp.le.u32 	%p5, %r26, %r27;
	@%p5 bra 	$L__BB105_9;
	bra.uni 	$L__BB105_8;
$L__BB105_8:
	ld.u32 	%r28, [%SP+28];
	ld.u32 	%r29, [%SP+20];
	add.s32 	%r30, %r28, %r29;
	ld.u8 	%rs2, [%r30];
	ld.u32 	%r31, [%SP+24];
	add.s32 	%r32, %r31, %r29;
	st.u8 	[%r32], %rs2;
	ld.u32 	%r33, [%SP+20];
	add.s32 	%r34, %r33, 1;
	st.u32 	[%SP+20], %r34;
	bra.uni 	$L__BB105_7;
$L__BB105_9:
	bra.uni 	$L__BB105_14;
$L__BB105_10:
	bra.uni 	$L__BB105_11;
$L__BB105_11:
	ld.u32 	%r16, [%SP+8];
	add.s32 	%r17, %r16, -1;
	st.u32 	[%SP+8], %r17;
	setp.eq.s32 	%p3, %r16, 0;
	@%p3 bra 	$L__BB105_13;
	bra.uni 	$L__BB105_12;
$L__BB105_12:
	ld.u32 	%r18, [%SP+28];
	ld.u32 	%r19, [%SP+8];
	add.s32 	%r20, %r18, %r19;
	ld.u8 	%rs1, [%r20];
	ld.u32 	%r21, [%SP+24];
	add.s32 	%r22, %r21, %r19;
	st.u8 	[%r22], %rs1;
	bra.uni 	$L__BB105_11;
$L__BB105_13:
	bra.uni 	$L__BB105_14;
$L__BB105_14:
	ret;

}
	// .globl	__modi
.visible .func  (.param .b32 func_retval0) __modi(
	.param .b32 __modi_param_0,
	.param .b32 __modi_param_1
)
{
	.local .align 4 .b8 	__local_depot106[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<6>;

	mov.u32 	%SPL, __local_depot106;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [__modi_param_1];
	ld.param.u32 	%r1, [__modi_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	rem.s32 	%r5, %r3, %r4;
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	__uitod
.visible .func  (.param .b64 func_retval0) __uitod(
	.param .b32 __uitod_param_0
)
{
	.local .align 4 .b8 	__local_depot107[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<3>;
	.reg .f64 	%fd<2>;

	mov.u32 	%SPL, __local_depot107;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__uitod_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	cvt.rn.f64.u32 	%fd1, %r2;
	st.param.f64 	[func_retval0+0], %fd1;
	ret;

}
	// .globl	__uitof
.visible .func  (.param .b32 func_retval0) __uitof(
	.param .b32 __uitof_param_0
)
{
	.local .align 4 .b8 	__local_depot108[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<3>;
	.reg .f32 	%f<2>;

	mov.u32 	%SPL, __local_depot108;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__uitof_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	cvt.rn.f32.u32 	%f1, %r2;
	st.param.f32 	[func_retval0+0], %f1;
	ret;

}
	// .globl	__ulltod
.visible .func  (.param .b64 func_retval0) __ulltod(
	.param .b64 __ulltod_param_0
)
{
	.local .align 8 .b8 	__local_depot109[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b64 	%rd<3>;
	.reg .f64 	%fd<2>;

	mov.u32 	%SPL, __local_depot109;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd1, [__ulltod_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	cvt.rn.f64.u64 	%fd1, %rd2;
	st.param.f64 	[func_retval0+0], %fd1;
	ret;

}
	// .globl	__ulltof
.visible .func  (.param .b32 func_retval0) __ulltof(
	.param .b64 __ulltof_param_0
)
{
	.local .align 8 .b8 	__local_depot110[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<3>;

	mov.u32 	%SPL, __local_depot110;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd1, [__ulltof_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	cvt.rn.f32.u64 	%f1, %rd2;
	st.param.f32 	[func_retval0+0], %f1;
	ret;

}
	// .globl	__umodi
.visible .func  (.param .b32 func_retval0) __umodi(
	.param .b32 __umodi_param_0,
	.param .b32 __umodi_param_1
)
{
	.local .align 4 .b8 	__local_depot111[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<6>;

	mov.u32 	%SPL, __local_depot111;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [__umodi_param_1];
	ld.param.u32 	%r1, [__umodi_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	rem.u32 	%r5, %r3, %r4;
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	__clzhi2
.visible .func  (.param .b32 func_retval0) __clzhi2(
	.param .b32 __clzhi2_param_0
)
{
	.local .align 4 .b8 	__local_depot112[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<12>;

	mov.u32 	%SPL, __local_depot112;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u16 	%rs1, [__clzhi2_param_0];
	st.u16 	[%SP+0], %rs1;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+4], %r1;
	bra.uni 	$L__BB112_1;
$L__BB112_1:
	ld.u32 	%r2, [%SP+4];
	setp.gt.s32 	%p1, %r2, 15;
	@%p1 bra 	$L__BB112_6;
	bra.uni 	$L__BB112_2;
$L__BB112_2:
	ld.u16 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	mov.b32 	%r5, 15;
	sub.s32 	%r6, %r5, %r4;
	shr.u32 	%r7, %r3, %r6;
	and.b32  	%r8, %r7, 1;
	setp.eq.b32 	%p2, %r8, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB112_4;
	bra.uni 	$L__BB112_3;
$L__BB112_3:
	bra.uni 	$L__BB112_6;
$L__BB112_4:
	bra.uni 	$L__BB112_5;
$L__BB112_5:
	ld.u32 	%r9, [%SP+4];
	add.s32 	%r10, %r9, 1;
	st.u32 	[%SP+4], %r10;
	bra.uni 	$L__BB112_1;
$L__BB112_6:
	ld.u32 	%r11, [%SP+4];
	st.param.b32 	[func_retval0+0], %r11;
	ret;

}
	// .globl	__ctzhi2
.visible .func  (.param .b32 func_retval0) __ctzhi2(
	.param .b32 __ctzhi2_param_0
)
{
	.local .align 4 .b8 	__local_depot113[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;

	mov.u32 	%SPL, __local_depot113;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u16 	%rs1, [__ctzhi2_param_0];
	st.u16 	[%SP+0], %rs1;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+4], %r1;
	bra.uni 	$L__BB113_1;
$L__BB113_1:
	ld.u32 	%r2, [%SP+4];
	setp.gt.s32 	%p1, %r2, 15;
	@%p1 bra 	$L__BB113_6;
	bra.uni 	$L__BB113_2;
$L__BB113_2:
	ld.u16 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	shr.u32 	%r5, %r3, %r4;
	and.b32  	%r6, %r5, 1;
	setp.eq.b32 	%p2, %r6, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB113_4;
	bra.uni 	$L__BB113_3;
$L__BB113_3:
	bra.uni 	$L__BB113_6;
$L__BB113_4:
	bra.uni 	$L__BB113_5;
$L__BB113_5:
	ld.u32 	%r7, [%SP+4];
	add.s32 	%r8, %r7, 1;
	st.u32 	[%SP+4], %r8;
	bra.uni 	$L__BB113_1;
$L__BB113_6:
	ld.u32 	%r9, [%SP+4];
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	__fixunssfsi
.visible .func  (.param .b32 func_retval0) __fixunssfsi(
	.param .b32 __fixunssfsi_param_0
)
{
	.local .align 4 .b8 	__local_depot114[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .f32 	%f<6>;

	mov.u32 	%SPL, __local_depot114;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f32 	%f1, [__fixunssfsi_param_0];
	st.f32 	[%SP+4], %f1;
	ld.f32 	%f2, [%SP+4];
	setp.ltu.f32 	%p1, %f2, 0f47000000;
	@%p1 bra 	$L__BB114_2;
	bra.uni 	$L__BB114_1;
$L__BB114_1:
	ld.f32 	%f4, [%SP+4];
	add.rn.f32 	%f5, %f4, 0fC7000000;
	cvt.rzi.s32.f32 	%r2, %f5;
	add.s32 	%r3, %r2, 32768;
	st.u32 	[%SP+0], %r3;
	bra.uni 	$L__BB114_3;
$L__BB114_2:
	ld.f32 	%f3, [%SP+4];
	cvt.rzi.s32.f32 	%r1, %f3;
	st.u32 	[%SP+0], %r1;
	bra.uni 	$L__BB114_3;
$L__BB114_3:
	ld.u32 	%r4, [%SP+0];
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	__parityhi2
.visible .func  (.param .b32 func_retval0) __parityhi2(
	.param .b32 __parityhi2_param_0
)
{
	.local .align 4 .b8 	__local_depot115[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<13>;

	mov.u32 	%SPL, __local_depot115;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u16 	%rs1, [__parityhi2_param_0];
	st.u16 	[%SP+0], %rs1;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+8], %r1;
	st.u32 	[%SP+4], %r1;
	bra.uni 	$L__BB115_1;
$L__BB115_1:
	ld.u32 	%r2, [%SP+4];
	setp.gt.s32 	%p1, %r2, 15;
	@%p1 bra 	$L__BB115_6;
	bra.uni 	$L__BB115_2;
$L__BB115_2:
	ld.u16 	%r5, [%SP+0];
	ld.u32 	%r6, [%SP+4];
	shr.u32 	%r7, %r5, %r6;
	and.b32  	%r8, %r7, 1;
	setp.eq.b32 	%p2, %r8, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB115_4;
	bra.uni 	$L__BB115_3;
$L__BB115_3:
	ld.u32 	%r9, [%SP+8];
	add.s32 	%r10, %r9, 1;
	st.u32 	[%SP+8], %r10;
	bra.uni 	$L__BB115_4;
$L__BB115_4:
	bra.uni 	$L__BB115_5;
$L__BB115_5:
	ld.u32 	%r11, [%SP+4];
	add.s32 	%r12, %r11, 1;
	st.u32 	[%SP+4], %r12;
	bra.uni 	$L__BB115_1;
$L__BB115_6:
	ld.u32 	%r3, [%SP+8];
	and.b32  	%r4, %r3, 1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	__popcounthi2
.visible .func  (.param .b32 func_retval0) __popcounthi2(
	.param .b32 __popcounthi2_param_0
)
{
	.local .align 4 .b8 	__local_depot116[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<12>;

	mov.u32 	%SPL, __local_depot116;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u16 	%rs1, [__popcounthi2_param_0];
	st.u16 	[%SP+0], %rs1;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+8], %r1;
	st.u32 	[%SP+4], %r1;
	bra.uni 	$L__BB116_1;
$L__BB116_1:
	ld.u32 	%r2, [%SP+4];
	setp.gt.s32 	%p1, %r2, 15;
	@%p1 bra 	$L__BB116_6;
	bra.uni 	$L__BB116_2;
$L__BB116_2:
	ld.u16 	%r4, [%SP+0];
	ld.u32 	%r5, [%SP+4];
	shr.u32 	%r6, %r4, %r5;
	and.b32  	%r7, %r6, 1;
	setp.eq.b32 	%p2, %r7, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB116_4;
	bra.uni 	$L__BB116_3;
$L__BB116_3:
	ld.u32 	%r8, [%SP+8];
	add.s32 	%r9, %r8, 1;
	st.u32 	[%SP+8], %r9;
	bra.uni 	$L__BB116_4;
$L__BB116_4:
	bra.uni 	$L__BB116_5;
$L__BB116_5:
	ld.u32 	%r10, [%SP+4];
	add.s32 	%r11, %r10, 1;
	st.u32 	[%SP+4], %r11;
	bra.uni 	$L__BB116_1;
$L__BB116_6:
	ld.u32 	%r3, [%SP+8];
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	__mulsi3_iq2000
.visible .func  (.param .b32 func_retval0) __mulsi3_iq2000(
	.param .b32 __mulsi3_iq2000_param_0,
	.param .b32 __mulsi3_iq2000_param_1
)
{
	.local .align 4 .b8 	__local_depot117[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<15>;

	mov.u32 	%SPL, __local_depot117;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [__mulsi3_iq2000_param_1];
	ld.param.u32 	%r1, [__mulsi3_iq2000_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	mov.b32 	%r3, 0;
	st.u32 	[%SP+8], %r3;
	bra.uni 	$L__BB117_1;
$L__BB117_1:
	ld.u32 	%r4, [%SP+0];
	setp.eq.s32 	%p1, %r4, 0;
	@%p1 bra 	$L__BB117_5;
	bra.uni 	$L__BB117_2;
$L__BB117_2:
	ld.u32 	%r6, [%SP+0];
	and.b32  	%r7, %r6, 1;
	setp.eq.b32 	%p2, %r7, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB117_4;
	bra.uni 	$L__BB117_3;
$L__BB117_3:
	ld.u32 	%r8, [%SP+4];
	ld.u32 	%r9, [%SP+8];
	add.s32 	%r10, %r9, %r8;
	st.u32 	[%SP+8], %r10;
	bra.uni 	$L__BB117_4;
$L__BB117_4:
	ld.u32 	%r11, [%SP+0];
	shr.u32 	%r12, %r11, 1;
	st.u32 	[%SP+0], %r12;
	ld.u32 	%r13, [%SP+4];
	shl.b32 	%r14, %r13, 1;
	st.u32 	[%SP+4], %r14;
	bra.uni 	$L__BB117_1;
$L__BB117_5:
	ld.u32 	%r5, [%SP+8];
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	__mulsi3_lm32
.visible .func  (.param .b32 func_retval0) __mulsi3_lm32(
	.param .b32 __mulsi3_lm32_param_0,
	.param .b32 __mulsi3_lm32_param_1
)
{
	.local .align 4 .b8 	__local_depot118[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<18>;

	mov.u32 	%SPL, __local_depot118;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [__mulsi3_lm32_param_1];
	ld.param.u32 	%r1, [__mulsi3_lm32_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	mov.b32 	%r3, 0;
	st.u32 	[%SP+12], %r3;
	ld.u32 	%r4, [%SP+4];
	setp.ne.s32 	%p1, %r4, 0;
	@%p1 bra 	$L__BB118_2;
	bra.uni 	$L__BB118_1;
$L__BB118_1:
	mov.b32 	%r16, 0;
	st.u32 	[%SP+0], %r16;
	bra.uni 	$L__BB118_8;
$L__BB118_2:
	bra.uni 	$L__BB118_3;
$L__BB118_3:
	ld.u32 	%r5, [%SP+8];
	setp.eq.s32 	%p2, %r5, 0;
	@%p2 bra 	$L__BB118_7;
	bra.uni 	$L__BB118_4;
$L__BB118_4:
	ld.u32 	%r7, [%SP+8];
	and.b32  	%r8, %r7, 1;
	setp.eq.b32 	%p3, %r8, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	not.pred 	%p6, %p5;
	@%p6 bra 	$L__BB118_6;
	bra.uni 	$L__BB118_5;
$L__BB118_5:
	ld.u32 	%r9, [%SP+4];
	ld.u32 	%r10, [%SP+12];
	add.s32 	%r11, %r10, %r9;
	st.u32 	[%SP+12], %r11;
	bra.uni 	$L__BB118_6;
$L__BB118_6:
	ld.u32 	%r12, [%SP+4];
	shl.b32 	%r13, %r12, 1;
	st.u32 	[%SP+4], %r13;
	ld.u32 	%r14, [%SP+8];
	shr.u32 	%r15, %r14, 1;
	st.u32 	[%SP+8], %r15;
	bra.uni 	$L__BB118_3;
$L__BB118_7:
	ld.u32 	%r6, [%SP+12];
	st.u32 	[%SP+0], %r6;
	bra.uni 	$L__BB118_8;
$L__BB118_8:
	ld.u32 	%r17, [%SP+0];
	st.param.b32 	[func_retval0+0], %r17;
	ret;

}
	// .globl	__udivmodsi4
.visible .func  (.param .b32 func_retval0) __udivmodsi4(
	.param .b32 __udivmodsi4_param_0,
	.param .b32 __udivmodsi4_param_1,
	.param .b32 __udivmodsi4_param_2
)
{
	.local .align 4 .b8 	__local_depot119[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<32>;

	mov.u32 	%SPL, __local_depot119;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [__udivmodsi4_param_2];
	ld.param.u32 	%r2, [__udivmodsi4_param_1];
	ld.param.u32 	%r1, [__udivmodsi4_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u32 	[%SP+12], %r3;
	mov.b32 	%r4, 1;
	st.u32 	[%SP+16], %r4;
	mov.b32 	%r5, 0;
	st.u32 	[%SP+20], %r5;
	bra.uni 	$L__BB119_1;
$L__BB119_1:
	ld.u32 	%r6, [%SP+8];
	ld.u32 	%r7, [%SP+4];
	setp.ge.u32 	%p4, %r6, %r7;
	mov.pred 	%p3, 0;
	mov.pred 	%p10, %p3;
	@%p4 bra 	$L__BB119_4;
	bra.uni 	$L__BB119_2;
$L__BB119_2:
	ld.u32 	%r8, [%SP+16];
	setp.eq.s32 	%p6, %r8, 0;
	mov.pred 	%p5, 0;
	mov.pred 	%p10, %p5;
	@%p6 bra 	$L__BB119_4;
	bra.uni 	$L__BB119_3;
$L__BB119_3:
	add.u32 	%r9, %SP, 8;
	or.b32  	%r10, %r9, 3;
	ld.u8 	%rs1, [%r10];
	and.b16  	%rs2, %rs1, 128;
	setp.eq.s16 	%p1, %rs2, 0;
	mov.pred 	%p10, %p1;
	bra.uni 	$L__BB119_4;
$L__BB119_4:
	mov.pred 	%p2, %p10;
	@!%p2 bra 	$L__BB119_6;
	bra.uni 	$L__BB119_5;
$L__BB119_5:
	ld.u32 	%r28, [%SP+8];
	shl.b32 	%r29, %r28, 1;
	st.u32 	[%SP+8], %r29;
	ld.u32 	%r30, [%SP+16];
	shl.b32 	%r31, %r30, 1;
	st.u32 	[%SP+16], %r31;
	bra.uni 	$L__BB119_1;
$L__BB119_6:
	bra.uni 	$L__BB119_7;
$L__BB119_7:
	ld.u32 	%r11, [%SP+16];
	setp.eq.s32 	%p7, %r11, 0;
	@%p7 bra 	$L__BB119_11;
	bra.uni 	$L__BB119_8;
$L__BB119_8:
	ld.u32 	%r16, [%SP+4];
	ld.u32 	%r17, [%SP+8];
	setp.lt.u32 	%p9, %r16, %r17;
	@%p9 bra 	$L__BB119_10;
	bra.uni 	$L__BB119_9;
$L__BB119_9:
	ld.u32 	%r18, [%SP+8];
	ld.u32 	%r19, [%SP+4];
	sub.s32 	%r20, %r19, %r18;
	st.u32 	[%SP+4], %r20;
	ld.u32 	%r21, [%SP+16];
	ld.u32 	%r22, [%SP+20];
	or.b32  	%r23, %r22, %r21;
	st.u32 	[%SP+20], %r23;
	bra.uni 	$L__BB119_10;
$L__BB119_10:
	ld.u32 	%r24, [%SP+16];
	shr.u32 	%r25, %r24, 1;
	st.u32 	[%SP+16], %r25;
	ld.u32 	%r26, [%SP+8];
	shr.u32 	%r27, %r26, 1;
	st.u32 	[%SP+8], %r27;
	bra.uni 	$L__BB119_7;
$L__BB119_11:
	ld.u32 	%r12, [%SP+12];
	setp.eq.s32 	%p8, %r12, 0;
	@%p8 bra 	$L__BB119_13;
	bra.uni 	$L__BB119_12;
$L__BB119_12:
	ld.u32 	%r14, [%SP+4];
	st.u32 	[%SP+0], %r14;
	bra.uni 	$L__BB119_14;
$L__BB119_13:
	ld.u32 	%r13, [%SP+20];
	st.u32 	[%SP+0], %r13;
	bra.uni 	$L__BB119_14;
$L__BB119_14:
	ld.u32 	%r15, [%SP+0];
	st.param.b32 	[func_retval0+0], %r15;
	ret;

}
	// .globl	__mspabi_cmpf
.visible .func  (.param .b32 func_retval0) __mspabi_cmpf(
	.param .b32 __mspabi_cmpf_param_0,
	.param .b32 __mspabi_cmpf_param_1
)
{
	.local .align 4 .b8 	__local_depot120[12];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b32 	%r<5>;
	.reg .f32 	%f<7>;

	mov.u32 	%SPL, __local_depot120;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f32 	%f2, [__mspabi_cmpf_param_1];
	ld.param.f32 	%f1, [__mspabi_cmpf_param_0];
	st.f32 	[%SP+4], %f1;
	st.f32 	[%SP+8], %f2;
	ld.f32 	%f3, [%SP+4];
	ld.f32 	%f4, [%SP+8];
	setp.geu.f32 	%p1, %f3, %f4;
	@%p1 bra 	$L__BB120_2;
	bra.uni 	$L__BB120_1;
$L__BB120_1:
	mov.b32 	%r3, -1;
	st.u32 	[%SP+0], %r3;
	bra.uni 	$L__BB120_5;
$L__BB120_2:
	ld.f32 	%f5, [%SP+4];
	ld.f32 	%f6, [%SP+8];
	setp.leu.f32 	%p2, %f5, %f6;
	@%p2 bra 	$L__BB120_4;
	bra.uni 	$L__BB120_3;
$L__BB120_3:
	mov.b32 	%r2, 1;
	st.u32 	[%SP+0], %r2;
	bra.uni 	$L__BB120_5;
$L__BB120_4:
	mov.b32 	%r1, 0;
	st.u32 	[%SP+0], %r1;
	bra.uni 	$L__BB120_5;
$L__BB120_5:
	ld.u32 	%r4, [%SP+0];
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	__mspabi_cmpd
.visible .func  (.param .b32 func_retval0) __mspabi_cmpd(
	.param .b64 __mspabi_cmpd_param_0,
	.param .b64 __mspabi_cmpd_param_1
)
{
	.local .align 8 .b8 	__local_depot121[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<7>;

	mov.u32 	%SPL, __local_depot121;
	cvta.local.u32 	%SP, %SPL;
	ld.param.f64 	%fd2, [__mspabi_cmpd_param_1];
	ld.param.f64 	%fd1, [__mspabi_cmpd_param_0];
	st.f64 	[%SP+8], %fd1;
	st.f64 	[%SP+16], %fd2;
	ld.f64 	%fd3, [%SP+8];
	ld.f64 	%fd4, [%SP+16];
	setp.geu.f64 	%p1, %fd3, %fd4;
	@%p1 bra 	$L__BB121_2;
	bra.uni 	$L__BB121_1;
$L__BB121_1:
	mov.b32 	%r3, -1;
	st.u32 	[%SP+0], %r3;
	bra.uni 	$L__BB121_5;
$L__BB121_2:
	ld.f64 	%fd5, [%SP+8];
	ld.f64 	%fd6, [%SP+16];
	setp.leu.f64 	%p2, %fd5, %fd6;
	@%p2 bra 	$L__BB121_4;
	bra.uni 	$L__BB121_3;
$L__BB121_3:
	mov.b32 	%r2, 1;
	st.u32 	[%SP+0], %r2;
	bra.uni 	$L__BB121_5;
$L__BB121_4:
	mov.b32 	%r1, 0;
	st.u32 	[%SP+0], %r1;
	bra.uni 	$L__BB121_5;
$L__BB121_5:
	ld.u32 	%r4, [%SP+0];
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	__mspabi_mpysll
.visible .func  (.param .b64 func_retval0) __mspabi_mpysll(
	.param .b32 __mspabi_mpysll_param_0,
	.param .b32 __mspabi_mpysll_param_1
)
{
	.local .align 4 .b8 	__local_depot122[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<4>;

	mov.u32 	%SPL, __local_depot122;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [__mspabi_mpysll_param_1];
	ld.param.u32 	%r1, [__mspabi_mpysll_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.s32 	%rd1, [%SP+0];
	ld.s32 	%rd2, [%SP+4];
	mul.lo.s64 	%rd3, %rd1, %rd2;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	__mspabi_mpyull
.visible .func  (.param .b64 func_retval0) __mspabi_mpyull(
	.param .b32 __mspabi_mpyull_param_0,
	.param .b32 __mspabi_mpyull_param_1
)
{
	.local .align 4 .b8 	__local_depot123[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<4>;

	mov.u32 	%SPL, __local_depot123;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [__mspabi_mpyull_param_1];
	ld.param.u32 	%r1, [__mspabi_mpyull_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%rd1, [%SP+0];
	ld.u32 	%rd2, [%SP+4];
	mul.lo.s64 	%rd3, %rd1, %rd2;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	__mulhi3
.visible .func  (.param .b32 func_retval0) __mulhi3(
	.param .b32 __mulhi3_param_0,
	.param .b32 __mulhi3_param_1
)
{
	.local .align 4 .b8 	__local_depot124[20];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<25>;

	mov.u32 	%SPL, __local_depot124;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r5, [__mulhi3_param_1];
	ld.param.u32 	%r4, [__mulhi3_param_0];
	st.u32 	[%SP+0], %r4;
	st.u32 	[%SP+4], %r5;
	mov.b32 	%r6, 0;
	st.u32 	[%SP+12], %r6;
	st.u32 	[%SP+16], %r6;
	ld.u32 	%r7, [%SP+4];
	setp.gt.s32 	%p3, %r7, -1;
	@%p3 bra 	$L__BB124_2;
	bra.uni 	$L__BB124_1;
$L__BB124_1:
	ld.u32 	%r8, [%SP+4];
	neg.s32 	%r9, %r8;
	st.u32 	[%SP+4], %r9;
	mov.b32 	%r10, 1;
	st.u32 	[%SP+12], %r10;
	bra.uni 	$L__BB124_2;
$L__BB124_2:
	mov.u16 	%rs1, 0;
	st.u8 	[%SP+8], %rs1;
	bra.uni 	$L__BB124_3;
$L__BB124_3:
	ld.u32 	%r11, [%SP+4];
	setp.eq.s32 	%p5, %r11, 0;
	mov.pred 	%p4, 0;
	mov.pred 	%p11, %p4;
	@%p5 bra 	$L__BB124_5;
	bra.uni 	$L__BB124_4;
$L__BB124_4:
	ld.s8 	%r12, [%SP+8];
	setp.lt.u32 	%p1, %r12, 32;
	mov.pred 	%p11, %p1;
	bra.uni 	$L__BB124_5;
$L__BB124_5:
	mov.pred 	%p2, %p11;
	@!%p2 bra 	$L__BB124_10;
	bra.uni 	$L__BB124_6;
$L__BB124_6:
	ld.u32 	%r15, [%SP+4];
	and.b32  	%r16, %r15, 1;
	setp.eq.b32 	%p7, %r16, 1;
	mov.pred 	%p8, 0;
	xor.pred  	%p9, %p7, %p8;
	not.pred 	%p10, %p9;
	@%p10 bra 	$L__BB124_8;
	bra.uni 	$L__BB124_7;
$L__BB124_7:
	ld.u32 	%r17, [%SP+0];
	ld.u32 	%r18, [%SP+16];
	add.s32 	%r19, %r18, %r17;
	st.u32 	[%SP+16], %r19;
	bra.uni 	$L__BB124_8;
$L__BB124_8:
	ld.u32 	%r20, [%SP+0];
	shl.b32 	%r21, %r20, 1;
	st.u32 	[%SP+0], %r21;
	ld.u32 	%r22, [%SP+4];
	shr.s32 	%r23, %r22, 1;
	st.u32 	[%SP+4], %r23;
	bra.uni 	$L__BB124_9;
$L__BB124_9:
	ld.u8 	%rs2, [%SP+8];
	add.s16 	%rs3, %rs2, 1;
	st.u8 	[%SP+8], %rs3;
	bra.uni 	$L__BB124_3;
$L__BB124_10:
	ld.u32 	%r13, [%SP+12];
	setp.eq.s32 	%p6, %r13, 0;
	@%p6 bra 	$L__BB124_12;
	bra.uni 	$L__BB124_11;
$L__BB124_11:
	ld.u32 	%r14, [%SP+16];
	neg.s32 	%r1, %r14;
	mov.u32 	%r24, %r1;
	bra.uni 	$L__BB124_13;
$L__BB124_12:
	ld.u32 	%r2, [%SP+16];
	mov.u32 	%r24, %r2;
	bra.uni 	$L__BB124_13;
$L__BB124_13:
	mov.u32 	%r3, %r24;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	__divsi3
.visible .func  (.param .b32 func_retval0) __divsi3(
	.param .b32 __divsi3_param_0,
	.param .b32 __divsi3_param_1
)
{
	.local .align 4 .b8 	__local_depot125[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<22>;

	mov.u32 	%SPL, __local_depot125;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [__divsi3_param_1];
	ld.param.u32 	%r1, [__divsi3_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	mov.b32 	%r3, 0;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+0];
	setp.gt.s32 	%p1, %r4, -1;
	@%p1 bra 	$L__BB125_2;
	bra.uni 	$L__BB125_1;
$L__BB125_1:
	ld.u32 	%r5, [%SP+0];
	neg.s32 	%r6, %r5;
	st.u32 	[%SP+0], %r6;
	ld.u32 	%r7, [%SP+8];
	setp.eq.s32 	%p2, %r7, 0;
	selp.u32 	%r8, 1, 0, %p2;
	st.u32 	[%SP+8], %r8;
	bra.uni 	$L__BB125_2;
$L__BB125_2:
	ld.u32 	%r9, [%SP+4];
	setp.gt.s32 	%p3, %r9, -1;
	@%p3 bra 	$L__BB125_4;
	bra.uni 	$L__BB125_3;
$L__BB125_3:
	ld.u32 	%r10, [%SP+4];
	neg.s32 	%r11, %r10;
	st.u32 	[%SP+4], %r11;
	ld.u32 	%r12, [%SP+8];
	setp.eq.s32 	%p4, %r12, 0;
	selp.u32 	%r13, 1, 0, %p4;
	st.u32 	[%SP+8], %r13;
	bra.uni 	$L__BB125_4;
$L__BB125_4:
	ld.u32 	%r14, [%SP+0];
	ld.u32 	%r15, [%SP+4];
	{ // callseq 13, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r14;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r15;
	.param .b32 param2;
	st.param.b32 	[param2+0], 0;
	.param .b32 retval0;
	call.uni (retval0), 
	__udivmodsi4, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r16, [retval0+0];
	} // callseq 13
	st.u32 	[%SP+12], %r16;
	ld.u32 	%r18, [%SP+8];
	setp.eq.s32 	%p5, %r18, 0;
	@%p5 bra 	$L__BB125_6;
	bra.uni 	$L__BB125_5;
$L__BB125_5:
	ld.u32 	%r19, [%SP+12];
	neg.s32 	%r20, %r19;
	st.u32 	[%SP+12], %r20;
	bra.uni 	$L__BB125_6;
$L__BB125_6:
	ld.u32 	%r21, [%SP+12];
	st.param.b32 	[func_retval0+0], %r21;
	ret;

}
	// .globl	__modsi3
.visible .func  (.param .b32 func_retval0) __modsi3(
	.param .b32 __modsi3_param_0,
	.param .b32 __modsi3_param_1
)
{
	.local .align 4 .b8 	__local_depot126[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<19>;

	mov.u32 	%SPL, __local_depot126;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [__modsi3_param_1];
	ld.param.u32 	%r1, [__modsi3_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	mov.b32 	%r3, 0;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r4, [%SP+0];
	setp.gt.s32 	%p1, %r4, -1;
	@%p1 bra 	$L__BB126_2;
	bra.uni 	$L__BB126_1;
$L__BB126_1:
	ld.u32 	%r5, [%SP+0];
	neg.s32 	%r6, %r5;
	st.u32 	[%SP+0], %r6;
	mov.b32 	%r7, 1;
	st.u32 	[%SP+8], %r7;
	bra.uni 	$L__BB126_2;
$L__BB126_2:
	ld.u32 	%r8, [%SP+4];
	setp.gt.s32 	%p2, %r8, -1;
	@%p2 bra 	$L__BB126_4;
	bra.uni 	$L__BB126_3;
$L__BB126_3:
	ld.u32 	%r9, [%SP+4];
	neg.s32 	%r10, %r9;
	st.u32 	[%SP+4], %r10;
	bra.uni 	$L__BB126_4;
$L__BB126_4:
	ld.u32 	%r11, [%SP+0];
	ld.u32 	%r12, [%SP+4];
	{ // callseq 14, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r11;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r12;
	.param .b32 param2;
	st.param.b32 	[param2+0], 1;
	.param .b32 retval0;
	call.uni (retval0), 
	__udivmodsi4, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r13, [retval0+0];
	} // callseq 14
	st.u32 	[%SP+12], %r13;
	ld.u32 	%r15, [%SP+8];
	setp.eq.s32 	%p3, %r15, 0;
	@%p3 bra 	$L__BB126_6;
	bra.uni 	$L__BB126_5;
$L__BB126_5:
	ld.u32 	%r16, [%SP+12];
	neg.s32 	%r17, %r16;
	st.u32 	[%SP+12], %r17;
	bra.uni 	$L__BB126_6;
$L__BB126_6:
	ld.u32 	%r18, [%SP+12];
	st.param.b32 	[func_retval0+0], %r18;
	ret;

}
	// .globl	__udivmodhi4
.visible .func  (.param .b32 func_retval0) __udivmodhi4(
	.param .b32 __udivmodhi4_param_0,
	.param .b32 __udivmodhi4_param_1,
	.param .b32 __udivmodhi4_param_2
)
{
	.local .align 4 .b8 	__local_depot127[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<13>;
	.reg .b32 	%r<20>;

	mov.u32 	%SPL, __local_depot127;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__udivmodhi4_param_2];
	ld.param.u16 	%rs2, [__udivmodhi4_param_1];
	ld.param.u16 	%rs1, [__udivmodhi4_param_0];
	st.u16 	[%SP+2], %rs1;
	st.u16 	[%SP+4], %rs2;
	st.u32 	[%SP+8], %r1;
	mov.u16 	%rs3, 1;
	st.u16 	[%SP+12], %rs3;
	mov.u16 	%rs4, 0;
	st.u16 	[%SP+14], %rs4;
	bra.uni 	$L__BB127_1;
$L__BB127_1:
	ld.u16 	%r2, [%SP+4];
	ld.u16 	%r3, [%SP+2];
	setp.ge.s32 	%p4, %r2, %r3;
	mov.pred 	%p3, 0;
	mov.pred 	%p10, %p3;
	@%p4 bra 	$L__BB127_4;
	bra.uni 	$L__BB127_2;
$L__BB127_2:
	ld.u16 	%rs5, [%SP+12];
	setp.eq.s16 	%p6, %rs5, 0;
	mov.pred 	%p5, 0;
	mov.pred 	%p10, %p5;
	@%p6 bra 	$L__BB127_4;
	bra.uni 	$L__BB127_3;
$L__BB127_3:
	ld.u16 	%r4, [%SP+4];
	and.b32  	%r5, %r4, 32768;
	setp.eq.s32 	%p1, %r5, 0;
	mov.pred 	%p10, %p1;
	bra.uni 	$L__BB127_4;
$L__BB127_4:
	mov.pred 	%p2, %p10;
	@!%p2 bra 	$L__BB127_6;
	bra.uni 	$L__BB127_5;
$L__BB127_5:
	ld.u16 	%rs9, [%SP+4];
	shl.b16 	%rs10, %rs9, 1;
	st.u16 	[%SP+4], %rs10;
	ld.u16 	%rs11, [%SP+12];
	shl.b16 	%rs12, %rs11, 1;
	st.u16 	[%SP+12], %rs12;
	bra.uni 	$L__BB127_1;
$L__BB127_6:
	bra.uni 	$L__BB127_7;
$L__BB127_7:
	ld.u16 	%rs6, [%SP+12];
	setp.eq.s16 	%p7, %rs6, 0;
	@%p7 bra 	$L__BB127_11;
	bra.uni 	$L__BB127_8;
$L__BB127_8:
	ld.u16 	%r8, [%SP+2];
	ld.u16 	%r9, [%SP+4];
	setp.lt.s32 	%p9, %r8, %r9;
	@%p9 bra 	$L__BB127_10;
	bra.uni 	$L__BB127_9;
$L__BB127_9:
	ld.u16 	%r10, [%SP+4];
	ld.u16 	%r11, [%SP+2];
	sub.s32 	%r12, %r11, %r10;
	st.u16 	[%SP+2], %r12;
	ld.u16 	%r13, [%SP+12];
	ld.u16 	%r14, [%SP+14];
	or.b32  	%r15, %r14, %r13;
	st.u16 	[%SP+14], %r15;
	bra.uni 	$L__BB127_10;
$L__BB127_10:
	ld.u16 	%r16, [%SP+12];
	shr.u32 	%r17, %r16, 1;
	st.u16 	[%SP+12], %r17;
	ld.u16 	%r18, [%SP+4];
	shr.u32 	%r19, %r18, 1;
	st.u16 	[%SP+4], %r19;
	bra.uni 	$L__BB127_7;
$L__BB127_11:
	ld.u32 	%r6, [%SP+8];
	setp.eq.s32 	%p8, %r6, 0;
	@%p8 bra 	$L__BB127_13;
	bra.uni 	$L__BB127_12;
$L__BB127_12:
	ld.u16 	%rs8, [%SP+2];
	st.u16 	[%SP+0], %rs8;
	bra.uni 	$L__BB127_14;
$L__BB127_13:
	ld.u16 	%rs7, [%SP+14];
	st.u16 	[%SP+0], %rs7;
	bra.uni 	$L__BB127_14;
$L__BB127_14:
	ld.u16 	%r7, [%SP+0];
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	__udivmodsi4_libgcc
.visible .func  (.param .b32 func_retval0) __udivmodsi4_libgcc(
	.param .b32 __udivmodsi4_libgcc_param_0,
	.param .b32 __udivmodsi4_libgcc_param_1,
	.param .b32 __udivmodsi4_libgcc_param_2
)
{
	.local .align 4 .b8 	__local_depot128[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<32>;

	mov.u32 	%SPL, __local_depot128;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r3, [__udivmodsi4_libgcc_param_2];
	ld.param.u32 	%r2, [__udivmodsi4_libgcc_param_1];
	ld.param.u32 	%r1, [__udivmodsi4_libgcc_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u32 	[%SP+12], %r3;
	mov.b32 	%r4, 1;
	st.u32 	[%SP+16], %r4;
	mov.b32 	%r5, 0;
	st.u32 	[%SP+20], %r5;
	bra.uni 	$L__BB128_1;
$L__BB128_1:
	ld.u32 	%r6, [%SP+8];
	ld.u32 	%r7, [%SP+4];
	setp.ge.u32 	%p4, %r6, %r7;
	mov.pred 	%p3, 0;
	mov.pred 	%p10, %p3;
	@%p4 bra 	$L__BB128_4;
	bra.uni 	$L__BB128_2;
$L__BB128_2:
	ld.u32 	%r8, [%SP+16];
	setp.eq.s32 	%p6, %r8, 0;
	mov.pred 	%p5, 0;
	mov.pred 	%p10, %p5;
	@%p6 bra 	$L__BB128_4;
	bra.uni 	$L__BB128_3;
$L__BB128_3:
	add.u32 	%r9, %SP, 8;
	or.b32  	%r10, %r9, 3;
	ld.u8 	%rs1, [%r10];
	and.b16  	%rs2, %rs1, 128;
	setp.eq.s16 	%p1, %rs2, 0;
	mov.pred 	%p10, %p1;
	bra.uni 	$L__BB128_4;
$L__BB128_4:
	mov.pred 	%p2, %p10;
	@!%p2 bra 	$L__BB128_6;
	bra.uni 	$L__BB128_5;
$L__BB128_5:
	ld.u32 	%r28, [%SP+8];
	shl.b32 	%r29, %r28, 1;
	st.u32 	[%SP+8], %r29;
	ld.u32 	%r30, [%SP+16];
	shl.b32 	%r31, %r30, 1;
	st.u32 	[%SP+16], %r31;
	bra.uni 	$L__BB128_1;
$L__BB128_6:
	bra.uni 	$L__BB128_7;
$L__BB128_7:
	ld.u32 	%r11, [%SP+16];
	setp.eq.s32 	%p7, %r11, 0;
	@%p7 bra 	$L__BB128_11;
	bra.uni 	$L__BB128_8;
$L__BB128_8:
	ld.u32 	%r16, [%SP+4];
	ld.u32 	%r17, [%SP+8];
	setp.lt.u32 	%p9, %r16, %r17;
	@%p9 bra 	$L__BB128_10;
	bra.uni 	$L__BB128_9;
$L__BB128_9:
	ld.u32 	%r18, [%SP+8];
	ld.u32 	%r19, [%SP+4];
	sub.s32 	%r20, %r19, %r18;
	st.u32 	[%SP+4], %r20;
	ld.u32 	%r21, [%SP+16];
	ld.u32 	%r22, [%SP+20];
	or.b32  	%r23, %r22, %r21;
	st.u32 	[%SP+20], %r23;
	bra.uni 	$L__BB128_10;
$L__BB128_10:
	ld.u32 	%r24, [%SP+16];
	shr.u32 	%r25, %r24, 1;
	st.u32 	[%SP+16], %r25;
	ld.u32 	%r26, [%SP+8];
	shr.u32 	%r27, %r26, 1;
	st.u32 	[%SP+8], %r27;
	bra.uni 	$L__BB128_7;
$L__BB128_11:
	ld.u32 	%r12, [%SP+12];
	setp.eq.s32 	%p8, %r12, 0;
	@%p8 bra 	$L__BB128_13;
	bra.uni 	$L__BB128_12;
$L__BB128_12:
	ld.u32 	%r14, [%SP+4];
	st.u32 	[%SP+0], %r14;
	bra.uni 	$L__BB128_14;
$L__BB128_13:
	ld.u32 	%r13, [%SP+20];
	st.u32 	[%SP+0], %r13;
	bra.uni 	$L__BB128_14;
$L__BB128_14:
	ld.u32 	%r15, [%SP+0];
	st.param.b32 	[func_retval0+0], %r15;
	ret;

}
	// .globl	__ashldi3
.visible .func  (.param .b64 func_retval0) __ashldi3(
	.param .b64 __ashldi3_param_0,
	.param .b32 __ashldi3_param_1
)
{
	.local .align 8 .b8 	__local_depot129[40];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<6>;

	mov.u32 	%SPL, __local_depot129;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__ashldi3_param_1];
	ld.param.u64 	%rd1, [__ashldi3_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u32 	[%SP+16], %r1;
	mov.b32 	%r2, 32;
	st.u32 	[%SP+20], %r2;
	ld.u64 	%rd2, [%SP+8];
	st.u64 	[%SP+24], %rd2;
	ld.u8 	%rs1, [%SP+16];
	and.b16  	%rs2, %rs1, 32;
	setp.eq.s16 	%p1, %rs2, 0;
	@%p1 bra 	$L__BB129_2;
	bra.uni 	$L__BB129_1;
$L__BB129_1:
	mov.b32 	%r19, 0;
	st.u32 	[%SP+32], %r19;
	ld.u32 	%r20, [%SP+24];
	ld.u32 	%r21, [%SP+16];
	add.s32 	%r22, %r21, -32;
	shl.b32 	%r23, %r20, %r22;
	add.u32 	%r24, %SP, 32;
	or.b32  	%r25, %r24, 4;
	st.u32 	[%r25], %r23;
	bra.uni 	$L__BB129_5;
$L__BB129_2:
	ld.u32 	%r3, [%SP+16];
	setp.ne.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB129_4;
	bra.uni 	$L__BB129_3;
$L__BB129_3:
	ld.u64 	%rd3, [%SP+8];
	st.u64 	[%SP+0], %rd3;
	bra.uni 	$L__BB129_6;
$L__BB129_4:
	ld.u32 	%r4, [%SP+24];
	ld.u32 	%r5, [%SP+16];
	shl.b32 	%r6, %r4, %r5;
	st.u32 	[%SP+32], %r6;
	add.u32 	%r7, %SP, 24;
	or.b32  	%r8, %r7, 4;
	ld.u32 	%r9, [%r8];
	ld.u32 	%r10, [%SP+16];
	shl.b32 	%r11, %r9, %r10;
	ld.u32 	%r12, [%SP+24];
	mov.b32 	%r13, 32;
	sub.s32 	%r14, %r13, %r10;
	shr.u32 	%r15, %r12, %r14;
	or.b32  	%r16, %r11, %r15;
	add.u32 	%r17, %SP, 32;
	or.b32  	%r18, %r17, 4;
	st.u32 	[%r18], %r16;
	bra.uni 	$L__BB129_5;
$L__BB129_5:
	ld.u64 	%rd4, [%SP+32];
	st.u64 	[%SP+0], %rd4;
	bra.uni 	$L__BB129_6;
$L__BB129_6:
	ld.u64 	%rd5, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd5;
	ret;

}
	// .globl	__ashrdi3
.visible .func  (.param .b64 func_retval0) __ashrdi3(
	.param .b64 __ashrdi3_param_0,
	.param .b32 __ashrdi3_param_1
)
{
	.local .align 8 .b8 	__local_depot130[40];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<6>;

	mov.u32 	%SPL, __local_depot130;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__ashrdi3_param_1];
	ld.param.u64 	%rd1, [__ashrdi3_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u32 	[%SP+16], %r1;
	mov.b32 	%r2, 32;
	st.u32 	[%SP+20], %r2;
	ld.u64 	%rd2, [%SP+8];
	st.u64 	[%SP+24], %rd2;
	ld.u8 	%rs1, [%SP+16];
	and.b16  	%rs2, %rs1, 32;
	setp.eq.s16 	%p1, %rs2, 0;
	@%p1 bra 	$L__BB130_2;
	bra.uni 	$L__BB130_1;
$L__BB130_1:
	add.u32 	%r19, %SP, 24;
	or.b32  	%r20, %r19, 4;
	ld.u32 	%r21, [%r20];
	shr.s32 	%r22, %r21, 31;
	add.u32 	%r23, %SP, 32;
	or.b32  	%r24, %r23, 4;
	st.u32 	[%r24], %r22;
	ld.u32 	%r25, [%r20];
	ld.u32 	%r26, [%SP+16];
	add.s32 	%r27, %r26, -32;
	shr.s32 	%r28, %r25, %r27;
	st.u32 	[%SP+32], %r28;
	bra.uni 	$L__BB130_5;
$L__BB130_2:
	ld.u32 	%r3, [%SP+16];
	setp.ne.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB130_4;
	bra.uni 	$L__BB130_3;
$L__BB130_3:
	ld.u64 	%rd3, [%SP+8];
	st.u64 	[%SP+0], %rd3;
	bra.uni 	$L__BB130_6;
$L__BB130_4:
	add.u32 	%r4, %SP, 24;
	or.b32  	%r5, %r4, 4;
	ld.u32 	%r6, [%r5];
	ld.u32 	%r7, [%SP+16];
	shr.s32 	%r8, %r6, %r7;
	add.u32 	%r9, %SP, 32;
	or.b32  	%r10, %r9, 4;
	st.u32 	[%r10], %r8;
	ld.u32 	%r11, [%r5];
	ld.u32 	%r12, [%SP+16];
	mov.b32 	%r13, 32;
	sub.s32 	%r14, %r13, %r12;
	shl.b32 	%r15, %r11, %r14;
	ld.u32 	%r16, [%SP+24];
	shr.u32 	%r17, %r16, %r12;
	or.b32  	%r18, %r15, %r17;
	st.u32 	[%SP+32], %r18;
	bra.uni 	$L__BB130_5;
$L__BB130_5:
	ld.u64 	%rd4, [%SP+32];
	st.u64 	[%SP+0], %rd4;
	bra.uni 	$L__BB130_6;
$L__BB130_6:
	ld.u64 	%rd5, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd5;
	ret;

}
	// .globl	__bswapdi2
.visible .func  (.param .b64 func_retval0) __bswapdi2(
	.param .b64 __bswapdi2_param_0
)
{
	.local .align 8 .b8 	__local_depot131[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b64 	%rd<24>;

	mov.u32 	%SPL, __local_depot131;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd1, [__bswapdi2_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	shr.u64 	%rd3, %rd2, 56;
	and.b64  	%rd4, %rd2, 71776119061217280;
	shr.u64 	%rd5, %rd4, 40;
	or.b64  	%rd6, %rd3, %rd5;
	and.b64  	%rd7, %rd2, 280375465082880;
	shr.u64 	%rd8, %rd7, 24;
	or.b64  	%rd9, %rd6, %rd8;
	and.b64  	%rd10, %rd2, 1095216660480;
	shr.u64 	%rd11, %rd10, 8;
	or.b64  	%rd12, %rd9, %rd11;
	and.b64  	%rd13, %rd2, 4278190080;
	shl.b64 	%rd14, %rd13, 8;
	or.b64  	%rd15, %rd12, %rd14;
	and.b64  	%rd16, %rd2, 16711680;
	shl.b64 	%rd17, %rd16, 24;
	or.b64  	%rd18, %rd15, %rd17;
	and.b64  	%rd19, %rd2, 65280;
	shl.b64 	%rd20, %rd19, 40;
	or.b64  	%rd21, %rd18, %rd20;
	shl.b64 	%rd22, %rd2, 56;
	or.b64  	%rd23, %rd21, %rd22;
	st.param.b64 	[func_retval0+0], %rd23;
	ret;

}
	// .globl	__bswapsi2
.visible .func  (.param .b32 func_retval0) __bswapsi2(
	.param .b32 __bswapsi2_param_0
)
{
	.local .align 4 .b8 	__local_depot132[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<12>;

	mov.u32 	%SPL, __local_depot132;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__bswapsi2_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	shr.u32 	%r3, %r2, 24;
	and.b32  	%r4, %r2, 16711680;
	shr.u32 	%r5, %r4, 8;
	or.b32  	%r6, %r3, %r5;
	and.b32  	%r7, %r2, 65280;
	shl.b32 	%r8, %r7, 8;
	or.b32  	%r9, %r6, %r8;
	shl.b32 	%r10, %r2, 24;
	or.b32  	%r11, %r9, %r10;
	st.param.b32 	[func_retval0+0], %r11;
	ret;

}
	// .globl	__clzsi2
.visible .func  (.param .b32 func_retval0) __clzsi2(
	.param .b32 __clzsi2_param_0
)
{
	.local .align 4 .b8 	__local_depot133[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<59>;

	mov.u32 	%SPL, __local_depot133;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__clzsi2_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+4], %r2;
	add.u32 	%r3, %SP, 4;
	or.b32  	%r4, %r3, 2;
	ld.u16 	%r5, [%r4];
	shl.b32 	%r6, %r5, 16;
	setp.eq.s32 	%p1, %r6, 0;
	selp.u32 	%r7, 1, 0, %p1;
	shl.b32 	%r8, %r7, 4;
	st.u32 	[%SP+8], %r8;
	ld.u32 	%r9, [%SP+8];
	mov.b32 	%r10, 16;
	sub.s32 	%r11, %r10, %r9;
	ld.u32 	%r12, [%SP+4];
	shr.u32 	%r13, %r12, %r11;
	st.u32 	[%SP+4], %r13;
	ld.u32 	%r14, [%SP+8];
	st.u32 	[%SP+12], %r14;
	or.b32  	%r15, %r3, 1;
	ld.u8 	%r16, [%r15];
	shl.b32 	%r17, %r16, 8;
	setp.eq.s32 	%p2, %r17, 0;
	selp.u32 	%r18, 1, 0, %p2;
	shl.b32 	%r19, %r18, 3;
	st.u32 	[%SP+8], %r19;
	ld.u32 	%r20, [%SP+8];
	mov.b32 	%r21, 8;
	sub.s32 	%r22, %r21, %r20;
	ld.u32 	%r23, [%SP+4];
	shr.u32 	%r24, %r23, %r22;
	st.u32 	[%SP+4], %r24;
	ld.u32 	%r25, [%SP+8];
	ld.u32 	%r26, [%SP+12];
	add.s32 	%r27, %r26, %r25;
	st.u32 	[%SP+12], %r27;
	ld.u32 	%r28, [%SP+4];
	and.b32  	%r29, %r28, 240;
	setp.eq.s32 	%p3, %r29, 0;
	selp.u32 	%r30, 1, 0, %p3;
	shl.b32 	%r31, %r30, 2;
	st.u32 	[%SP+8], %r31;
	ld.u32 	%r32, [%SP+8];
	mov.b32 	%r33, 4;
	sub.s32 	%r34, %r33, %r32;
	ld.u32 	%r35, [%SP+4];
	shr.u32 	%r36, %r35, %r34;
	st.u32 	[%SP+4], %r36;
	ld.u32 	%r37, [%SP+8];
	ld.u32 	%r38, [%SP+12];
	add.s32 	%r39, %r38, %r37;
	st.u32 	[%SP+12], %r39;
	ld.u32 	%r40, [%SP+4];
	and.b32  	%r41, %r40, 12;
	setp.eq.s32 	%p4, %r41, 0;
	selp.u32 	%r42, 1, 0, %p4;
	shl.b32 	%r43, %r42, 1;
	st.u32 	[%SP+8], %r43;
	ld.u32 	%r44, [%SP+8];
	mov.b32 	%r45, 2;
	sub.s32 	%r46, %r45, %r44;
	ld.u32 	%r47, [%SP+4];
	shr.u32 	%r48, %r47, %r46;
	st.u32 	[%SP+4], %r48;
	ld.u32 	%r49, [%SP+8];
	ld.u32 	%r50, [%SP+12];
	add.s32 	%r51, %r50, %r49;
	st.u32 	[%SP+12], %r51;
	ld.u32 	%r52, [%SP+12];
	ld.u32 	%r53, [%SP+4];
	sub.s32 	%r54, %r45, %r53;
	and.b32  	%r55, %r53, 2;
	setp.eq.s32 	%p5, %r55, 0;
	selp.s32 	%r56, -1, 0, %p5;
	and.b32  	%r57, %r54, %r56;
	add.s32 	%r58, %r52, %r57;
	st.param.b32 	[func_retval0+0], %r58;
	ret;

}
	// .globl	__cmpdi2
.visible .func  (.param .b32 func_retval0) __cmpdi2(
	.param .b64 __cmpdi2_param_0,
	.param .b64 __cmpdi2_param_1
)
{
	.local .align 8 .b8 	__local_depot134[40];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<5>;

	mov.u32 	%SPL, __local_depot134;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd2, [__cmpdi2_param_1];
	ld.param.u64 	%rd1, [__cmpdi2_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	ld.u64 	%rd3, [%SP+8];
	st.u64 	[%SP+24], %rd3;
	ld.u64 	%rd4, [%SP+16];
	st.u64 	[%SP+32], %rd4;
	add.u32 	%r1, %SP, 24;
	or.b32  	%r2, %r1, 4;
	ld.u32 	%r3, [%r2];
	add.u32 	%r4, %SP, 32;
	or.b32  	%r5, %r4, 4;
	ld.u32 	%r6, [%r5];
	setp.ge.s32 	%p1, %r3, %r6;
	@%p1 bra 	$L__BB134_2;
	bra.uni 	$L__BB134_1;
$L__BB134_1:
	mov.b32 	%r21, 0;
	st.u32 	[%SP+0], %r21;
	bra.uni 	$L__BB134_9;
$L__BB134_2:
	add.u32 	%r7, %SP, 24;
	or.b32  	%r8, %r7, 4;
	ld.u32 	%r9, [%r8];
	add.u32 	%r10, %SP, 32;
	or.b32  	%r11, %r10, 4;
	ld.u32 	%r12, [%r11];
	setp.le.s32 	%p2, %r9, %r12;
	@%p2 bra 	$L__BB134_4;
	bra.uni 	$L__BB134_3;
$L__BB134_3:
	mov.b32 	%r20, 2;
	st.u32 	[%SP+0], %r20;
	bra.uni 	$L__BB134_9;
$L__BB134_4:
	ld.u32 	%r13, [%SP+24];
	ld.u32 	%r14, [%SP+32];
	setp.ge.u32 	%p3, %r13, %r14;
	@%p3 bra 	$L__BB134_6;
	bra.uni 	$L__BB134_5;
$L__BB134_5:
	mov.b32 	%r19, 0;
	st.u32 	[%SP+0], %r19;
	bra.uni 	$L__BB134_9;
$L__BB134_6:
	ld.u32 	%r15, [%SP+24];
	ld.u32 	%r16, [%SP+32];
	setp.le.u32 	%p4, %r15, %r16;
	@%p4 bra 	$L__BB134_8;
	bra.uni 	$L__BB134_7;
$L__BB134_7:
	mov.b32 	%r18, 2;
	st.u32 	[%SP+0], %r18;
	bra.uni 	$L__BB134_9;
$L__BB134_8:
	mov.b32 	%r17, 1;
	st.u32 	[%SP+0], %r17;
	bra.uni 	$L__BB134_9;
$L__BB134_9:
	ld.u32 	%r22, [%SP+0];
	st.param.b32 	[func_retval0+0], %r22;
	ret;

}
	// .globl	__aeabi_lcmp
.visible .func  (.param .b32 func_retval0) __aeabi_lcmp(
	.param .b64 __aeabi_lcmp_param_0,
	.param .b64 __aeabi_lcmp_param_1
)
{
	.local .align 8 .b8 	__local_depot135[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<5>;

	mov.u32 	%SPL, __local_depot135;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd2, [__aeabi_lcmp_param_1];
	ld.param.u64 	%rd1, [__aeabi_lcmp_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	ld.u64 	%rd4, [%SP+8];
	{ // callseq 15, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd3;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd4;
	.param .b32 retval0;
	call.uni (retval0), 
	__cmpdi2, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1, [retval0+0];
	} // callseq 15
	add.s32 	%r3, %r1, -1;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	__ctzsi2
.visible .func  (.param .b32 func_retval0) __ctzsi2(
	.param .b32 __ctzsi2_param_0
)
{
	.local .align 4 .b8 	__local_depot136[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<49>;

	mov.u32 	%SPL, __local_depot136;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__ctzsi2_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+4], %r2;
	ld.u16 	%rs1, [%SP+4];
	setp.eq.s16 	%p1, %rs1, 0;
	selp.u32 	%r3, 1, 0, %p1;
	shl.b32 	%r4, %r3, 4;
	st.u32 	[%SP+8], %r4;
	ld.u32 	%r5, [%SP+8];
	ld.u32 	%r6, [%SP+4];
	shr.u32 	%r7, %r6, %r5;
	st.u32 	[%SP+4], %r7;
	ld.u32 	%r8, [%SP+8];
	st.u32 	[%SP+12], %r8;
	ld.u8 	%r9, [%SP+4];
	setp.eq.s32 	%p2, %r9, 0;
	selp.u32 	%r10, 1, 0, %p2;
	shl.b32 	%r11, %r10, 3;
	st.u32 	[%SP+8], %r11;
	ld.u32 	%r12, [%SP+8];
	ld.u32 	%r13, [%SP+4];
	shr.u32 	%r14, %r13, %r12;
	st.u32 	[%SP+4], %r14;
	ld.u32 	%r15, [%SP+8];
	ld.u32 	%r16, [%SP+12];
	add.s32 	%r17, %r16, %r15;
	st.u32 	[%SP+12], %r17;
	ld.u32 	%r18, [%SP+4];
	and.b32  	%r19, %r18, 15;
	setp.eq.s32 	%p3, %r19, 0;
	selp.u32 	%r20, 1, 0, %p3;
	shl.b32 	%r21, %r20, 2;
	st.u32 	[%SP+8], %r21;
	ld.u32 	%r22, [%SP+8];
	ld.u32 	%r23, [%SP+4];
	shr.u32 	%r24, %r23, %r22;
	st.u32 	[%SP+4], %r24;
	ld.u32 	%r25, [%SP+8];
	ld.u32 	%r26, [%SP+12];
	add.s32 	%r27, %r26, %r25;
	st.u32 	[%SP+12], %r27;
	ld.u32 	%r28, [%SP+4];
	and.b32  	%r29, %r28, 3;
	setp.eq.s32 	%p4, %r29, 0;
	selp.u32 	%r30, 1, 0, %p4;
	shl.b32 	%r31, %r30, 1;
	st.u32 	[%SP+8], %r31;
	ld.u32 	%r32, [%SP+8];
	ld.u32 	%r33, [%SP+4];
	shr.u32 	%r34, %r33, %r32;
	st.u32 	[%SP+4], %r34;
	ld.u32 	%r35, [%SP+4];
	and.b32  	%r36, %r35, 3;
	st.u32 	[%SP+4], %r36;
	ld.u32 	%r37, [%SP+8];
	ld.u32 	%r38, [%SP+12];
	add.s32 	%r39, %r38, %r37;
	st.u32 	[%SP+12], %r39;
	ld.u32 	%r40, [%SP+12];
	ld.u32 	%r41, [%SP+4];
	shr.u32 	%r42, %r41, 1;
	mov.b32 	%r43, 2;
	sub.s32 	%r44, %r43, %r42;
	and.b32  	%r45, %r41, 1;
	add.s32 	%r46, %r45, -1;
	and.b32  	%r47, %r44, %r46;
	add.s32 	%r48, %r40, %r47;
	st.param.b32 	[func_retval0+0], %r48;
	ret;

}
	// .globl	__lshrdi3
.visible .func  (.param .b64 func_retval0) __lshrdi3(
	.param .b64 __lshrdi3_param_0,
	.param .b32 __lshrdi3_param_1
)
{
	.local .align 8 .b8 	__local_depot137[40];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<6>;

	mov.u32 	%SPL, __local_depot137;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__lshrdi3_param_1];
	ld.param.u64 	%rd1, [__lshrdi3_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u32 	[%SP+16], %r1;
	mov.b32 	%r2, 32;
	st.u32 	[%SP+20], %r2;
	ld.u64 	%rd2, [%SP+8];
	st.u64 	[%SP+24], %rd2;
	ld.u8 	%rs1, [%SP+16];
	and.b16  	%rs2, %rs1, 32;
	setp.eq.s16 	%p1, %rs2, 0;
	@%p1 bra 	$L__BB137_2;
	bra.uni 	$L__BB137_1;
$L__BB137_1:
	add.u32 	%r19, %SP, 32;
	or.b32  	%r20, %r19, 4;
	mov.b32 	%r21, 0;
	st.u32 	[%r20], %r21;
	add.u32 	%r22, %SP, 24;
	or.b32  	%r23, %r22, 4;
	ld.u32 	%r24, [%r23];
	ld.u32 	%r25, [%SP+16];
	add.s32 	%r26, %r25, -32;
	shr.u32 	%r27, %r24, %r26;
	st.u32 	[%SP+32], %r27;
	bra.uni 	$L__BB137_5;
$L__BB137_2:
	ld.u32 	%r3, [%SP+16];
	setp.ne.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB137_4;
	bra.uni 	$L__BB137_3;
$L__BB137_3:
	ld.u64 	%rd3, [%SP+8];
	st.u64 	[%SP+0], %rd3;
	bra.uni 	$L__BB137_6;
$L__BB137_4:
	add.u32 	%r4, %SP, 24;
	or.b32  	%r5, %r4, 4;
	ld.u32 	%r6, [%r5];
	ld.u32 	%r7, [%SP+16];
	shr.u32 	%r8, %r6, %r7;
	add.u32 	%r9, %SP, 32;
	or.b32  	%r10, %r9, 4;
	st.u32 	[%r10], %r8;
	ld.u32 	%r11, [%r5];
	ld.u32 	%r12, [%SP+16];
	mov.b32 	%r13, 32;
	sub.s32 	%r14, %r13, %r12;
	shl.b32 	%r15, %r11, %r14;
	ld.u32 	%r16, [%SP+24];
	shr.u32 	%r17, %r16, %r12;
	or.b32  	%r18, %r15, %r17;
	st.u32 	[%SP+32], %r18;
	bra.uni 	$L__BB137_5;
$L__BB137_5:
	ld.u64 	%rd4, [%SP+32];
	st.u64 	[%SP+0], %rd4;
	bra.uni 	$L__BB137_6;
$L__BB137_6:
	ld.u64 	%rd5, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd5;
	ret;

}
	// .globl	__muldsi3
.visible .func  (.param .b64 func_retval0) __muldsi3(
	.param .b32 __muldsi3_param_0,
	.param .b32 __muldsi3_param_1
)
{
	.local .align 8 .b8 	__local_depot138[32];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<48>;
	.reg .b64 	%rd<2>;

	mov.u32 	%SPL, __local_depot138;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r2, [__muldsi3_param_1];
	ld.param.u32 	%r1, [__muldsi3_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	mov.b32 	%r3, 16;
	st.u32 	[%SP+16], %r3;
	mov.b32 	%r4, 65535;
	st.u32 	[%SP+20], %r4;
	ld.u16 	%r5, [%SP+0];
	ld.u16 	%r6, [%SP+4];
	mul.lo.s32 	%r7, %r5, %r6;
	st.u32 	[%SP+8], %r7;
	add.u32 	%r8, %SP, 8;
	or.b32  	%r9, %r8, 2;
	ld.u16 	%r10, [%r9];
	st.u32 	[%SP+24], %r10;
	ld.u16 	%r11, [%SP+8];
	st.u32 	[%SP+8], %r11;
	add.u32 	%r12, %SP, 0;
	or.b32  	%r13, %r12, 2;
	ld.u16 	%r14, [%r13];
	ld.u16 	%r15, [%SP+4];
	mul.lo.s32 	%r16, %r14, %r15;
	ld.u32 	%r17, [%SP+24];
	add.s32 	%r18, %r17, %r16;
	st.u32 	[%SP+24], %r18;
	ld.u32 	%r19, [%SP+24];
	shl.b32 	%r20, %r19, 16;
	ld.u32 	%r21, [%SP+8];
	add.s32 	%r22, %r21, %r20;
	st.u32 	[%SP+8], %r22;
	add.u32 	%r23, %SP, 24;
	or.b32  	%r24, %r23, 2;
	ld.u16 	%r25, [%r24];
	or.b32  	%r26, %r8, 4;
	st.u32 	[%r26], %r25;
	ld.u16 	%r27, [%r9];
	st.u32 	[%SP+24], %r27;
	ld.u16 	%r28, [%SP+8];
	st.u32 	[%SP+8], %r28;
	add.u32 	%r29, %SP, 4;
	or.b32  	%r30, %r29, 2;
	ld.u16 	%r31, [%r30];
	ld.u16 	%r32, [%SP+0];
	mul.lo.s32 	%r33, %r31, %r32;
	ld.u32 	%r34, [%SP+24];
	add.s32 	%r35, %r34, %r33;
	st.u32 	[%SP+24], %r35;
	ld.u32 	%r36, [%SP+24];
	shl.b32 	%r37, %r36, 16;
	ld.u32 	%r38, [%SP+8];
	add.s32 	%r39, %r38, %r37;
	st.u32 	[%SP+8], %r39;
	ld.u16 	%r40, [%r24];
	ld.u32 	%r41, [%r26];
	add.s32 	%r42, %r41, %r40;
	st.u32 	[%r26], %r42;
	ld.u16 	%r43, [%r13];
	ld.u16 	%r44, [%r30];
	mul.lo.s32 	%r45, %r43, %r44;
	ld.u32 	%r46, [%r26];
	add.s32 	%r47, %r46, %r45;
	st.u32 	[%r26], %r47;
	ld.u64 	%rd1, [%SP+8];
	st.param.b64 	[func_retval0+0], %rd1;
	ret;

}
	// .globl	__muldi3_compiler_rt
.visible .func  (.param .b64 func_retval0) __muldi3_compiler_rt(
	.param .b64 __muldi3_compiler_rt_param_0,
	.param .b64 __muldi3_compiler_rt_param_1
)
{
	.local .align 8 .b8 	__local_depot139[40];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<8>;

	mov.u32 	%SPL, __local_depot139;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd2, [__muldi3_compiler_rt_param_1];
	ld.param.u64 	%rd1, [__muldi3_compiler_rt_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+8];
	st.u64 	[%SP+24], %rd4;
	ld.u32 	%r1, [%SP+16];
	ld.u32 	%r2, [%SP+24];
	{ // callseq 16, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r1;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r2;
	.param .b64 retval0;
	call.uni (retval0), 
	__muldsi3, 
	(
	param0, 
	param1
	);
	ld.param.b64 	%rd5, [retval0+0];
	} // callseq 16
	st.u64 	[%SP+32], %rd5;
	add.u32 	%r3, %SP, 16;
	or.b32  	%r4, %r3, 4;
	ld.u32 	%r5, [%r4];
	ld.u32 	%r6, [%SP+24];
	mul.lo.s32 	%r7, %r5, %r6;
	ld.u32 	%r8, [%SP+16];
	add.u32 	%r9, %SP, 24;
	or.b32  	%r10, %r9, 4;
	ld.u32 	%r11, [%r10];
	mul.lo.s32 	%r12, %r8, %r11;
	add.s32 	%r13, %r7, %r12;
	add.u32 	%r14, %SP, 32;
	or.b32  	%r15, %r14, 4;
	ld.u32 	%r16, [%r15];
	add.s32 	%r17, %r16, %r13;
	st.u32 	[%r15], %r17;
	ld.u64 	%rd7, [%SP+32];
	st.param.b64 	[func_retval0+0], %rd7;
	ret;

}
	// .globl	__negdi2
.visible .func  (.param .b64 func_retval0) __negdi2(
	.param .b64 __negdi2_param_0
)
{
	.local .align 8 .b8 	__local_depot140[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b64 	%rd<4>;

	mov.u32 	%SPL, __local_depot140;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd1, [__negdi2_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	neg.s64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	__paritydi2
.visible .func  (.param .b32 func_retval0) __paritydi2(
	.param .b64 __paritydi2_param_0
)
{
	.local .align 8 .b8 	__local_depot141[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<3>;

	mov.u32 	%SPL, __local_depot141;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd1, [__paritydi2_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	st.u64 	[%SP+8], %rd2;
	add.u32 	%r1, %SP, 8;
	or.b32  	%r2, %r1, 4;
	ld.u32 	%r3, [%r2];
	ld.u32 	%r4, [%SP+8];
	xor.b32  	%r5, %r3, %r4;
	st.u32 	[%SP+16], %r5;
	ld.u32 	%r6, [%SP+16];
	shr.u32 	%r7, %r6, 16;
	xor.b32  	%r8, %r6, %r7;
	st.u32 	[%SP+16], %r8;
	ld.u32 	%r9, [%SP+16];
	shr.u32 	%r10, %r9, 8;
	xor.b32  	%r11, %r9, %r10;
	st.u32 	[%SP+16], %r11;
	ld.u32 	%r12, [%SP+16];
	shr.u32 	%r13, %r12, 4;
	xor.b32  	%r14, %r12, %r13;
	st.u32 	[%SP+16], %r14;
	ld.u32 	%r15, [%SP+16];
	and.b32  	%r16, %r15, 15;
	mov.b32 	%r17, 27030;
	shr.u32 	%r18, %r17, %r16;
	and.b32  	%r19, %r18, 1;
	st.param.b32 	[func_retval0+0], %r19;
	ret;

}
	// .globl	__paritysi2
.visible .func  (.param .b32 func_retval0) __paritysi2(
	.param .b32 __paritysi2_param_0
)
{
	.local .align 4 .b8 	__local_depot142[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<17>;

	mov.u32 	%SPL, __local_depot142;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__paritysi2_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+4];
	shr.u32 	%r4, %r3, 16;
	xor.b32  	%r5, %r3, %r4;
	st.u32 	[%SP+4], %r5;
	ld.u32 	%r6, [%SP+4];
	shr.u32 	%r7, %r6, 8;
	xor.b32  	%r8, %r6, %r7;
	st.u32 	[%SP+4], %r8;
	ld.u32 	%r9, [%SP+4];
	shr.u32 	%r10, %r9, 4;
	xor.b32  	%r11, %r9, %r10;
	st.u32 	[%SP+4], %r11;
	ld.u32 	%r12, [%SP+4];
	and.b32  	%r13, %r12, 15;
	mov.b32 	%r14, 27030;
	shr.u32 	%r15, %r14, %r13;
	and.b32  	%r16, %r15, 1;
	st.param.b32 	[func_retval0+0], %r16;
	ret;

}
	// .globl	__popcountdi2
.visible .func  (.param .b32 func_retval0) __popcountdi2(
	.param .b64 __popcountdi2_param_0
)
{
	.local .align 8 .b8 	__local_depot143[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<19>;

	mov.u32 	%SPL, __local_depot143;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd1, [__popcountdi2_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+8];
	shr.u64 	%rd4, %rd3, 1;
	and.b64  	%rd5, %rd4, 6148914691236517205;
	sub.s64 	%rd6, %rd3, %rd5;
	st.u64 	[%SP+8], %rd6;
	ld.u64 	%rd7, [%SP+8];
	shr.u64 	%rd8, %rd7, 2;
	and.b64  	%rd9, %rd8, 3689348814741910323;
	and.b64  	%rd10, %rd7, 3689348814741910323;
	add.s64 	%rd11, %rd9, %rd10;
	st.u64 	[%SP+8], %rd11;
	ld.u64 	%rd12, [%SP+8];
	shr.u64 	%rd13, %rd12, 4;
	add.s64 	%rd14, %rd12, %rd13;
	and.b64  	%rd15, %rd14, 1085102592571150095;
	st.u64 	[%SP+8], %rd15;
	ld.u64 	%rd16, [%SP+8];
	shr.u64 	%rd17, %rd16, 32;
	add.s64 	%rd18, %rd16, %rd17;
	st.u32 	[%SP+16], %rd18;
	ld.u32 	%r1, [%SP+16];
	shr.u32 	%r2, %r1, 16;
	add.s32 	%r3, %r1, %r2;
	st.u32 	[%SP+16], %r3;
	ld.u32 	%r4, [%SP+16];
	shr.u32 	%r5, %r4, 8;
	add.s32 	%r6, %r4, %r5;
	and.b32  	%r7, %r6, 127;
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	__popcountsi2
.visible .func  (.param .b32 func_retval0) __popcountsi2(
	.param .b32 __popcountsi2_param_0
)
{
	.local .align 4 .b8 	__local_depot144[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<23>;

	mov.u32 	%SPL, __local_depot144;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__popcountsi2_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+4];
	shr.u32 	%r4, %r3, 1;
	and.b32  	%r5, %r4, 1431655765;
	sub.s32 	%r6, %r3, %r5;
	st.u32 	[%SP+4], %r6;
	ld.u32 	%r7, [%SP+4];
	shr.u32 	%r8, %r7, 2;
	and.b32  	%r9, %r8, 858993459;
	and.b32  	%r10, %r7, 858993459;
	add.s32 	%r11, %r9, %r10;
	st.u32 	[%SP+4], %r11;
	ld.u32 	%r12, [%SP+4];
	shr.u32 	%r13, %r12, 4;
	add.s32 	%r14, %r12, %r13;
	and.b32  	%r15, %r14, 252645135;
	st.u32 	[%SP+4], %r15;
	ld.u32 	%r16, [%SP+4];
	shr.u32 	%r17, %r16, 16;
	add.s32 	%r18, %r16, %r17;
	st.u32 	[%SP+4], %r18;
	ld.u32 	%r19, [%SP+4];
	shr.u32 	%r20, %r19, 8;
	add.s32 	%r21, %r19, %r20;
	and.b32  	%r22, %r21, 63;
	st.param.b32 	[func_retval0+0], %r22;
	ret;

}
	// .globl	__powidf2
.visible .func  (.param .b64 func_retval0) __powidf2(
	.param .b64 __powidf2_param_0,
	.param .b32 __powidf2_param_1
)
{
	.local .align 8 .b8 	__local_depot145[24];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<2>;
	.reg .f64 	%fd<12>;

	mov.u32 	%SPL, __local_depot145;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__powidf2_param_1];
	ld.param.f64 	%fd4, [__powidf2_param_0];
	st.f64 	[%SP+0], %fd4;
	st.u32 	[%SP+8], %r1;
	ld.u32 	%r2, [%SP+8];
	shr.u32 	%r3, %r2, 31;
	st.u32 	[%SP+12], %r3;
	mov.u64 	%rd1, 4607182418800017408;
	st.u64 	[%SP+16], %rd1;
	bra.uni 	$L__BB145_1;
$L__BB145_1:
	ld.u32 	%r4, [%SP+8];
	and.b32  	%r5, %r4, 1;
	setp.eq.b32 	%p1, %r5, 1;
	mov.pred 	%p2, 0;
	xor.pred  	%p3, %p1, %p2;
	not.pred 	%p4, %p3;
	@%p4 bra 	$L__BB145_3;
	bra.uni 	$L__BB145_2;
$L__BB145_2:
	ld.f64 	%fd5, [%SP+0];
	ld.f64 	%fd6, [%SP+16];
	mul.rn.f64 	%fd7, %fd6, %fd5;
	st.f64 	[%SP+16], %fd7;
	bra.uni 	$L__BB145_3;
$L__BB145_3:
	ld.u32 	%r6, [%SP+8];
	shr.u32 	%r7, %r6, 31;
	add.s32 	%r8, %r6, %r7;
	shr.s32 	%r9, %r8, 1;
	st.u32 	[%SP+8], %r9;
	ld.u32 	%r10, [%SP+8];
	setp.ne.s32 	%p5, %r10, 0;
	@%p5 bra 	$L__BB145_5;
	bra.uni 	$L__BB145_4;
$L__BB145_4:
	bra.uni 	$L__BB145_6;
$L__BB145_5:
	ld.f64 	%fd8, [%SP+0];
	mul.rn.f64 	%fd9, %fd8, %fd8;
	st.f64 	[%SP+0], %fd9;
	bra.uni 	$L__BB145_1;
$L__BB145_6:
	ld.u32 	%r11, [%SP+12];
	setp.eq.s32 	%p6, %r11, 0;
	@%p6 bra 	$L__BB145_8;
	bra.uni 	$L__BB145_7;
$L__BB145_7:
	ld.f64 	%fd10, [%SP+16];
	rcp.rn.f64 	%fd1, %fd10;
	mov.f64 	%fd11, %fd1;
	bra.uni 	$L__BB145_9;
$L__BB145_8:
	ld.f64 	%fd2, [%SP+16];
	mov.f64 	%fd11, %fd2;
	bra.uni 	$L__BB145_9;
$L__BB145_9:
	mov.f64 	%fd3, %fd11;
	st.param.f64 	[func_retval0+0], %fd3;
	ret;

}
	// .globl	__powisf2
.visible .func  (.param .b32 func_retval0) __powisf2(
	.param .b32 __powisf2_param_0,
	.param .b32 __powisf2_param_1
)
{
	.local .align 4 .b8 	__local_depot146[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<12>;

	mov.u32 	%SPL, __local_depot146;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u32 	%r1, [__powisf2_param_1];
	ld.param.f32 	%f4, [__powisf2_param_0];
	st.f32 	[%SP+0], %f4;
	st.u32 	[%SP+4], %r1;
	ld.u32 	%r2, [%SP+4];
	shr.u32 	%r3, %r2, 31;
	st.u32 	[%SP+8], %r3;
	mov.b32 	%r4, 1065353216;
	st.u32 	[%SP+12], %r4;
	bra.uni 	$L__BB146_1;
$L__BB146_1:
	ld.u32 	%r5, [%SP+4];
	and.b32  	%r6, %r5, 1;
	setp.eq.b32 	%p1, %r6, 1;
	mov.pred 	%p2, 0;
	xor.pred  	%p3, %p1, %p2;
	not.pred 	%p4, %p3;
	@%p4 bra 	$L__BB146_3;
	bra.uni 	$L__BB146_2;
$L__BB146_2:
	ld.f32 	%f5, [%SP+0];
	ld.f32 	%f6, [%SP+12];
	mul.rn.f32 	%f7, %f6, %f5;
	st.f32 	[%SP+12], %f7;
	bra.uni 	$L__BB146_3;
$L__BB146_3:
	ld.u32 	%r7, [%SP+4];
	shr.u32 	%r8, %r7, 31;
	add.s32 	%r9, %r7, %r8;
	shr.s32 	%r10, %r9, 1;
	st.u32 	[%SP+4], %r10;
	ld.u32 	%r11, [%SP+4];
	setp.ne.s32 	%p5, %r11, 0;
	@%p5 bra 	$L__BB146_5;
	bra.uni 	$L__BB146_4;
$L__BB146_4:
	bra.uni 	$L__BB146_6;
$L__BB146_5:
	ld.f32 	%f8, [%SP+0];
	mul.rn.f32 	%f9, %f8, %f8;
	st.f32 	[%SP+0], %f9;
	bra.uni 	$L__BB146_1;
$L__BB146_6:
	ld.u32 	%r12, [%SP+8];
	setp.eq.s32 	%p6, %r12, 0;
	@%p6 bra 	$L__BB146_8;
	bra.uni 	$L__BB146_7;
$L__BB146_7:
	ld.f32 	%f10, [%SP+12];
	rcp.rn.f32 	%f1, %f10;
	mov.f32 	%f11, %f1;
	bra.uni 	$L__BB146_9;
$L__BB146_8:
	ld.f32 	%f2, [%SP+12];
	mov.f32 	%f11, %f2;
	bra.uni 	$L__BB146_9;
$L__BB146_9:
	mov.f32 	%f3, %f11;
	st.param.f32 	[func_retval0+0], %f3;
	ret;

}
	// .globl	__ucmpdi2
.visible .func  (.param .b32 func_retval0) __ucmpdi2(
	.param .b64 __ucmpdi2_param_0,
	.param .b64 __ucmpdi2_param_1
)
{
	.local .align 8 .b8 	__local_depot147[40];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<5>;

	mov.u32 	%SPL, __local_depot147;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd2, [__ucmpdi2_param_1];
	ld.param.u64 	%rd1, [__ucmpdi2_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	ld.u64 	%rd3, [%SP+8];
	st.u64 	[%SP+24], %rd3;
	ld.u64 	%rd4, [%SP+16];
	st.u64 	[%SP+32], %rd4;
	add.u32 	%r1, %SP, 24;
	or.b32  	%r2, %r1, 4;
	ld.u32 	%r3, [%r2];
	add.u32 	%r4, %SP, 32;
	or.b32  	%r5, %r4, 4;
	ld.u32 	%r6, [%r5];
	setp.ge.u32 	%p1, %r3, %r6;
	@%p1 bra 	$L__BB147_2;
	bra.uni 	$L__BB147_1;
$L__BB147_1:
	mov.b32 	%r21, 0;
	st.u32 	[%SP+0], %r21;
	bra.uni 	$L__BB147_9;
$L__BB147_2:
	add.u32 	%r7, %SP, 24;
	or.b32  	%r8, %r7, 4;
	ld.u32 	%r9, [%r8];
	add.u32 	%r10, %SP, 32;
	or.b32  	%r11, %r10, 4;
	ld.u32 	%r12, [%r11];
	setp.le.u32 	%p2, %r9, %r12;
	@%p2 bra 	$L__BB147_4;
	bra.uni 	$L__BB147_3;
$L__BB147_3:
	mov.b32 	%r20, 2;
	st.u32 	[%SP+0], %r20;
	bra.uni 	$L__BB147_9;
$L__BB147_4:
	ld.u32 	%r13, [%SP+24];
	ld.u32 	%r14, [%SP+32];
	setp.ge.u32 	%p3, %r13, %r14;
	@%p3 bra 	$L__BB147_6;
	bra.uni 	$L__BB147_5;
$L__BB147_5:
	mov.b32 	%r19, 0;
	st.u32 	[%SP+0], %r19;
	bra.uni 	$L__BB147_9;
$L__BB147_6:
	ld.u32 	%r15, [%SP+24];
	ld.u32 	%r16, [%SP+32];
	setp.le.u32 	%p4, %r15, %r16;
	@%p4 bra 	$L__BB147_8;
	bra.uni 	$L__BB147_7;
$L__BB147_7:
	mov.b32 	%r18, 2;
	st.u32 	[%SP+0], %r18;
	bra.uni 	$L__BB147_9;
$L__BB147_8:
	mov.b32 	%r17, 1;
	st.u32 	[%SP+0], %r17;
	bra.uni 	$L__BB147_9;
$L__BB147_9:
	ld.u32 	%r22, [%SP+0];
	st.param.b32 	[func_retval0+0], %r22;
	ret;

}
	// .globl	__aeabi_ulcmp
.visible .func  (.param .b32 func_retval0) __aeabi_ulcmp(
	.param .b64 __aeabi_ulcmp_param_0,
	.param .b64 __aeabi_ulcmp_param_1
)
{
	.local .align 8 .b8 	__local_depot148[16];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<5>;

	mov.u32 	%SPL, __local_depot148;
	cvta.local.u32 	%SP, %SPL;
	ld.param.u64 	%rd2, [__aeabi_ulcmp_param_1];
	ld.param.u64 	%rd1, [__aeabi_ulcmp_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	ld.u64 	%rd4, [%SP+8];
	{ // callseq 17, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd3;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd4;
	.param .b32 retval0;
	call.uni (retval0), 
	__ucmpdi2, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1, [retval0+0];
	} // callseq 17
	add.s32 	%r3, %r1, -1;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
