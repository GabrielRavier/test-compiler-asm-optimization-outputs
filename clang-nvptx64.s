//
// Generated by LLVM NVPTX Back-End
//

.version 4.2
.target sm_20
.address_size 64

	// .globl	make_ti
.global .align 1 .b8 l64a_$_s[7];
.global .align 1 .b8 digits[65] = {46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122};
.global .align 8 .u64 seed;

.visible .func  (.param .align 16 .b8 func_retval0[16]) make_ti(
	.param .b64 make_ti_param_0,
	.param .b64 make_ti_param_1
)
{
	.local .align 16 .b8 	__local_depot0[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<9>;

	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [make_ti_param_1];
	ld.param.u64 	%rd1, [make_ti_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	add.u64 	%rd4, %SP, 16;
	or.b64  	%rd5, %rd4, 8;
	st.u64 	[%rd5], %rd3;
	ld.u64 	%rd6, [%SP+8];
	st.u64 	[%SP+16], %rd6;
	ld.u64 	%rd7, [%rd5];
	ld.u64 	%rd8, [%SP+16];
	st.param.v2.b64 	[func_retval0+0], {%rd8, %rd7};
	ret;

}
	// .globl	make_tu
.visible .func  (.param .align 16 .b8 func_retval0[16]) make_tu(
	.param .b64 make_tu_param_0,
	.param .b64 make_tu_param_1
)
{
	.local .align 16 .b8 	__local_depot1[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<9>;

	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [make_tu_param_1];
	ld.param.u64 	%rd1, [make_tu_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	add.u64 	%rd4, %SP, 16;
	or.b64  	%rd5, %rd4, 8;
	st.u64 	[%rd5], %rd3;
	ld.u64 	%rd6, [%SP+8];
	st.u64 	[%SP+16], %rd6;
	ld.u64 	%rd7, [%rd5];
	ld.u64 	%rd8, [%SP+16];
	st.param.v2.b64 	[func_retval0+0], {%rd8, %rd7};
	ret;

}
	// .globl	memmove
.visible .func  (.param .b64 func_retval0) memmove(
	.param .b64 memmove_param_0,
	.param .b64 memmove_param_1,
	.param .b64 memmove_param_2
)
{
	.local .align 8 .b8 	__local_depot2[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .b64 	%rd<31>;

	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [memmove_param_2];
	ld.param.u64 	%rd2, [memmove_param_1];
	ld.param.u64 	%rd1, [memmove_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+0];
	st.u64 	[%SP+24], %rd4;
	ld.u64 	%rd5, [%SP+8];
	st.u64 	[%SP+32], %rd5;
	ld.u64 	%rd6, [%SP+32];
	ld.u64 	%rd7, [%SP+24];
	setp.ge.u64 	%p1, %rd6, %rd7;
	@%p1 bra 	$L__BB2_6;
	bra.uni 	$L__BB2_1;
$L__BB2_1:
	ld.u64 	%rd17, [%SP+16];
	ld.u64 	%rd18, [%SP+32];
	add.s64 	%rd19, %rd18, %rd17;
	st.u64 	[%SP+32], %rd19;
	ld.u64 	%rd20, [%SP+16];
	ld.u64 	%rd21, [%SP+24];
	add.s64 	%rd22, %rd21, %rd20;
	st.u64 	[%SP+24], %rd22;
	bra.uni 	$L__BB2_2;
$L__BB2_2:
	ld.u64 	%rd23, [%SP+16];
	setp.eq.s64 	%p4, %rd23, 0;
	@%p4 bra 	$L__BB2_5;
	bra.uni 	$L__BB2_3;
$L__BB2_3:
	ld.u64 	%rd25, [%SP+32];
	add.s64 	%rd26, %rd25, -1;
	st.u64 	[%SP+32], %rd26;
	ld.u8 	%rs2, [%rd25+-1];
	ld.u64 	%rd27, [%SP+24];
	add.s64 	%rd28, %rd27, -1;
	st.u64 	[%SP+24], %rd28;
	st.u8 	[%rd27+-1], %rs2;
	bra.uni 	$L__BB2_4;
$L__BB2_4:
	ld.u64 	%rd29, [%SP+16];
	add.s64 	%rd30, %rd29, -1;
	st.u64 	[%SP+16], %rd30;
	bra.uni 	$L__BB2_2;
$L__BB2_5:
	bra.uni 	$L__BB2_13;
$L__BB2_6:
	ld.u64 	%rd8, [%SP+32];
	ld.u64 	%rd9, [%SP+24];
	setp.eq.s64 	%p2, %rd8, %rd9;
	@%p2 bra 	$L__BB2_12;
	bra.uni 	$L__BB2_7;
$L__BB2_7:
	bra.uni 	$L__BB2_8;
$L__BB2_8:
	ld.u64 	%rd10, [%SP+16];
	setp.eq.s64 	%p3, %rd10, 0;
	@%p3 bra 	$L__BB2_11;
	bra.uni 	$L__BB2_9;
$L__BB2_9:
	ld.u64 	%rd11, [%SP+32];
	add.s64 	%rd12, %rd11, 1;
	st.u64 	[%SP+32], %rd12;
	ld.u8 	%rs1, [%rd11];
	ld.u64 	%rd13, [%SP+24];
	add.s64 	%rd14, %rd13, 1;
	st.u64 	[%SP+24], %rd14;
	st.u8 	[%rd13], %rs1;
	bra.uni 	$L__BB2_10;
$L__BB2_10:
	ld.u64 	%rd15, [%SP+16];
	add.s64 	%rd16, %rd15, -1;
	st.u64 	[%SP+16], %rd16;
	bra.uni 	$L__BB2_8;
$L__BB2_11:
	bra.uni 	$L__BB2_12;
$L__BB2_12:
	bra.uni 	$L__BB2_13;
$L__BB2_13:
	ld.u64 	%rd24, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd24;
	ret;

}
	// .globl	memccpy
.visible .func  (.param .b64 func_retval0) memccpy(
	.param .b64 memccpy_param_0,
	.param .b64 memccpy_param_1,
	.param .b32 memccpy_param_2,
	.param .b64 memccpy_param_3
)
{
	.local .align 8 .b8 	__local_depot3[56];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<20>;

	mov.u64 	%SPL, __local_depot3;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [memccpy_param_3];
	ld.param.u32 	%r1, [memccpy_param_2];
	ld.param.u64 	%rd2, [memccpy_param_1];
	ld.param.u64 	%rd1, [memccpy_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	st.u32 	[%SP+24], %r1;
	st.u64 	[%SP+32], %rd3;
	ld.u64 	%rd4, [%SP+8];
	st.u64 	[%SP+40], %rd4;
	ld.u64 	%rd5, [%SP+16];
	st.u64 	[%SP+48], %rd5;
	ld.u8 	%r2, [%SP+24];
	st.u32 	[%SP+24], %r2;
	bra.uni 	$L__BB3_1;
$L__BB3_1:
	ld.u64 	%rd6, [%SP+32];
	setp.eq.s64 	%p4, %rd6, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB3_3;
	bra.uni 	$L__BB3_2;
$L__BB3_2:
	ld.u64 	%rd7, [%SP+48];
	ld.u8 	%rs1, [%rd7];
	ld.u64 	%rd8, [%SP+40];
	st.u8 	[%rd8], %rs1;
	cvt.u32.u16 	%r3, %rs1;
	and.b32  	%r4, %r3, 255;
	ld.u32 	%r5, [%SP+24];
	setp.ne.s32 	%p1, %r4, %r5;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB3_3;
$L__BB3_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB3_6;
	bra.uni 	$L__BB3_4;
$L__BB3_4:
	bra.uni 	$L__BB3_5;
$L__BB3_5:
	ld.u64 	%rd14, [%SP+32];
	add.s64 	%rd15, %rd14, -1;
	st.u64 	[%SP+32], %rd15;
	ld.u64 	%rd16, [%SP+48];
	add.s64 	%rd17, %rd16, 1;
	st.u64 	[%SP+48], %rd17;
	ld.u64 	%rd18, [%SP+40];
	add.s64 	%rd19, %rd18, 1;
	st.u64 	[%SP+40], %rd19;
	bra.uni 	$L__BB3_1;
$L__BB3_6:
	ld.u64 	%rd9, [%SP+32];
	setp.eq.s64 	%p5, %rd9, 0;
	@%p5 bra 	$L__BB3_8;
	bra.uni 	$L__BB3_7;
$L__BB3_7:
	ld.u64 	%rd11, [%SP+40];
	add.s64 	%rd12, %rd11, 1;
	st.u64 	[%SP+0], %rd12;
	bra.uni 	$L__BB3_9;
$L__BB3_8:
	mov.u64 	%rd10, 0;
	st.u64 	[%SP+0], %rd10;
	bra.uni 	$L__BB3_9;
$L__BB3_9:
	ld.u64 	%rd13, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd13;
	ret;

}
	// .globl	memchr
.visible .func  (.param .b64 func_retval0) memchr(
	.param .b64 memchr_param_0,
	.param .b32 memchr_param_1,
	.param .b64 memchr_param_2
)
{
	.local .align 8 .b8 	__local_depot4[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<15>;

	mov.u64 	%SPL, __local_depot4;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd4, [memchr_param_2];
	ld.param.u32 	%r1, [memchr_param_1];
	ld.param.u64 	%rd3, [memchr_param_0];
	st.u64 	[%SP+0], %rd3;
	st.u32 	[%SP+8], %r1;
	st.u64 	[%SP+16], %rd4;
	ld.u64 	%rd5, [%SP+0];
	st.u64 	[%SP+24], %rd5;
	ld.u8 	%r2, [%SP+8];
	st.u32 	[%SP+8], %r2;
	bra.uni 	$L__BB4_1;
$L__BB4_1:
	ld.u64 	%rd6, [%SP+16];
	setp.eq.s64 	%p4, %rd6, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB4_3;
	bra.uni 	$L__BB4_2;
$L__BB4_2:
	ld.u64 	%rd7, [%SP+24];
	ld.u8 	%r3, [%rd7];
	ld.u32 	%r4, [%SP+8];
	setp.ne.s32 	%p1, %r3, %r4;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB4_3;
$L__BB4_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB4_6;
	bra.uni 	$L__BB4_4;
$L__BB4_4:
	bra.uni 	$L__BB4_5;
$L__BB4_5:
	ld.u64 	%rd10, [%SP+24];
	add.s64 	%rd11, %rd10, 1;
	st.u64 	[%SP+24], %rd11;
	ld.u64 	%rd12, [%SP+16];
	add.s64 	%rd13, %rd12, -1;
	st.u64 	[%SP+16], %rd13;
	bra.uni 	$L__BB4_1;
$L__BB4_6:
	ld.u64 	%rd8, [%SP+16];
	setp.eq.s64 	%p5, %rd8, 0;
	@%p5 bra 	$L__BB4_8;
	bra.uni 	$L__BB4_7;
$L__BB4_7:
	ld.u64 	%rd1, [%SP+24];
	mov.u64 	%rd14, %rd1;
	bra.uni 	$L__BB4_9;
$L__BB4_8:
	mov.u64 	%rd9, 0;
	mov.u64 	%rd14, %rd9;
	bra.uni 	$L__BB4_9;
$L__BB4_9:
	mov.u64 	%rd2, %rd14;
	st.param.b64 	[func_retval0+0], %rd2;
	ret;

}
	// .globl	memcmp
.visible .func  (.param .b32 func_retval0) memcmp(
	.param .b64 memcmp_param_0,
	.param .b64 memcmp_param_1,
	.param .b64 memcmp_param_2
)
{
	.local .align 8 .b8 	__local_depot5[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<18>;

	mov.u64 	%SPL, __local_depot5;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [memcmp_param_2];
	ld.param.u64 	%rd2, [memcmp_param_1];
	ld.param.u64 	%rd1, [memcmp_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+0];
	st.u64 	[%SP+24], %rd4;
	ld.u64 	%rd5, [%SP+8];
	st.u64 	[%SP+32], %rd5;
	bra.uni 	$L__BB5_1;
$L__BB5_1:
	ld.u64 	%rd6, [%SP+16];
	setp.eq.s64 	%p4, %rd6, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB5_3;
	bra.uni 	$L__BB5_2;
$L__BB5_2:
	ld.u64 	%rd7, [%SP+24];
	ld.u8 	%r3, [%rd7];
	ld.u64 	%rd8, [%SP+32];
	ld.u8 	%r4, [%rd8];
	setp.eq.s32 	%p1, %r3, %r4;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB5_3;
$L__BB5_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB5_6;
	bra.uni 	$L__BB5_4;
$L__BB5_4:
	bra.uni 	$L__BB5_5;
$L__BB5_5:
	ld.u64 	%rd12, [%SP+16];
	add.s64 	%rd13, %rd12, -1;
	st.u64 	[%SP+16], %rd13;
	ld.u64 	%rd14, [%SP+24];
	add.s64 	%rd15, %rd14, 1;
	st.u64 	[%SP+24], %rd15;
	ld.u64 	%rd16, [%SP+32];
	add.s64 	%rd17, %rd16, 1;
	st.u64 	[%SP+32], %rd17;
	bra.uni 	$L__BB5_1;
$L__BB5_6:
	ld.u64 	%rd9, [%SP+16];
	setp.eq.s64 	%p5, %rd9, 0;
	@%p5 bra 	$L__BB5_8;
	bra.uni 	$L__BB5_7;
$L__BB5_7:
	ld.u64 	%rd10, [%SP+24];
	ld.u8 	%r6, [%rd10];
	ld.u64 	%rd11, [%SP+32];
	ld.u8 	%r7, [%rd11];
	sub.s32 	%r1, %r6, %r7;
	mov.u32 	%r8, %r1;
	bra.uni 	$L__BB5_9;
$L__BB5_8:
	mov.b32 	%r5, 0;
	mov.u32 	%r8, %r5;
	bra.uni 	$L__BB5_9;
$L__BB5_9:
	mov.u32 	%r2, %r8;
	st.param.b32 	[func_retval0+0], %r2;
	ret;

}
	// .globl	memcpy
.visible .func  (.param .b64 func_retval0) memcpy(
	.param .b64 memcpy_param_0,
	.param .b64 memcpy_param_1,
	.param .b64 memcpy_param_2
)
{
	.local .align 8 .b8 	__local_depot6[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<14>;

	mov.u64 	%SPL, __local_depot6;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [memcpy_param_2];
	ld.param.u64 	%rd2, [memcpy_param_1];
	ld.param.u64 	%rd1, [memcpy_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+0];
	st.u64 	[%SP+24], %rd4;
	ld.u64 	%rd5, [%SP+8];
	st.u64 	[%SP+32], %rd5;
	bra.uni 	$L__BB6_1;
$L__BB6_1:
	ld.u64 	%rd6, [%SP+16];
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB6_4;
	bra.uni 	$L__BB6_2;
$L__BB6_2:
	ld.u64 	%rd8, [%SP+32];
	add.s64 	%rd9, %rd8, 1;
	st.u64 	[%SP+32], %rd9;
	ld.u8 	%rs1, [%rd8];
	ld.u64 	%rd10, [%SP+24];
	add.s64 	%rd11, %rd10, 1;
	st.u64 	[%SP+24], %rd11;
	st.u8 	[%rd10], %rs1;
	bra.uni 	$L__BB6_3;
$L__BB6_3:
	ld.u64 	%rd12, [%SP+16];
	add.s64 	%rd13, %rd12, -1;
	st.u64 	[%SP+16], %rd13;
	bra.uni 	$L__BB6_1;
$L__BB6_4:
	ld.u64 	%rd7, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd7;
	ret;

}
	// .globl	memrchr
.visible .func  (.param .b64 func_retval0) memrchr(
	.param .b64 memrchr_param_0,
	.param .b32 memrchr_param_1,
	.param .b64 memrchr_param_2
)
{
	.local .align 8 .b8 	__local_depot7[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;

	mov.u64 	%SPL, __local_depot7;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [memrchr_param_2];
	ld.param.u32 	%r1, [memrchr_param_1];
	ld.param.u64 	%rd1, [memrchr_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u32 	[%SP+16], %r1;
	st.u64 	[%SP+24], %rd2;
	ld.u64 	%rd3, [%SP+8];
	st.u64 	[%SP+32], %rd3;
	ld.u8 	%r2, [%SP+16];
	st.u32 	[%SP+16], %r2;
	bra.uni 	$L__BB7_1;
$L__BB7_1:
	ld.u64 	%rd4, [%SP+24];
	add.s64 	%rd5, %rd4, -1;
	st.u64 	[%SP+24], %rd5;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB7_5;
	bra.uni 	$L__BB7_2;
$L__BB7_2:
	ld.u64 	%rd7, [%SP+32];
	ld.u64 	%rd8, [%SP+24];
	add.s64 	%rd9, %rd7, %rd8;
	ld.u8 	%r3, [%rd9];
	ld.u32 	%r4, [%SP+16];
	setp.ne.s32 	%p2, %r3, %r4;
	@%p2 bra 	$L__BB7_4;
	bra.uni 	$L__BB7_3;
$L__BB7_3:
	ld.u64 	%rd10, [%SP+32];
	ld.u64 	%rd11, [%SP+24];
	add.s64 	%rd12, %rd10, %rd11;
	st.u64 	[%SP+0], %rd12;
	bra.uni 	$L__BB7_6;
$L__BB7_4:
	bra.uni 	$L__BB7_1;
$L__BB7_5:
	mov.u64 	%rd6, 0;
	st.u64 	[%SP+0], %rd6;
	bra.uni 	$L__BB7_6;
$L__BB7_6:
	ld.u64 	%rd13, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd13;
	ret;

}
	// .globl	memset
.visible .func  (.param .b64 func_retval0) memset(
	.param .b64 memset_param_0,
	.param .b32 memset_param_1,
	.param .b64 memset_param_2
)
{
	.local .align 8 .b8 	__local_depot8[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<11>;

	mov.u64 	%SPL, __local_depot8;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [memset_param_2];
	ld.param.u32 	%r1, [memset_param_1];
	ld.param.u64 	%rd1, [memset_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	st.u64 	[%SP+16], %rd2;
	ld.u64 	%rd3, [%SP+0];
	st.u64 	[%SP+24], %rd3;
	bra.uni 	$L__BB8_1;
$L__BB8_1:
	ld.u64 	%rd4, [%SP+16];
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB8_4;
	bra.uni 	$L__BB8_2;
$L__BB8_2:
	ld.u32 	%r2, [%SP+8];
	ld.u64 	%rd6, [%SP+24];
	st.u8 	[%rd6], %r2;
	bra.uni 	$L__BB8_3;
$L__BB8_3:
	ld.u64 	%rd7, [%SP+16];
	add.s64 	%rd8, %rd7, -1;
	st.u64 	[%SP+16], %rd8;
	ld.u64 	%rd9, [%SP+24];
	add.s64 	%rd10, %rd9, 1;
	st.u64 	[%SP+24], %rd10;
	bra.uni 	$L__BB8_1;
$L__BB8_4:
	ld.u64 	%rd5, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd5;
	ret;

}
	// .globl	stpcpy
.visible .func  (.param .b64 func_retval0) stpcpy(
	.param .b64 stpcpy_param_0,
	.param .b64 stpcpy_param_1
)
{
	.local .align 8 .b8 	__local_depot9[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<10>;

	mov.u64 	%SPL, __local_depot9;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [stpcpy_param_1];
	ld.param.u64 	%rd1, [stpcpy_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	bra.uni 	$L__BB9_1;
$L__BB9_1:
	ld.u64 	%rd3, [%SP+8];
	ld.u8 	%rs1, [%rd3];
	ld.u64 	%rd4, [%SP+0];
	st.u8 	[%rd4], %rs1;
	setp.eq.s16 	%p1, %rs1, 0;
	@%p1 bra 	$L__BB9_4;
	bra.uni 	$L__BB9_2;
$L__BB9_2:
	bra.uni 	$L__BB9_3;
$L__BB9_3:
	ld.u64 	%rd6, [%SP+8];
	add.s64 	%rd7, %rd6, 1;
	st.u64 	[%SP+8], %rd7;
	ld.u64 	%rd8, [%SP+0];
	add.s64 	%rd9, %rd8, 1;
	st.u64 	[%SP+0], %rd9;
	bra.uni 	$L__BB9_1;
$L__BB9_4:
	ld.u64 	%rd5, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd5;
	ret;

}
	// .globl	strchrnul
.visible .func  (.param .b64 func_retval0) strchrnul(
	.param .b64 strchrnul_param_0,
	.param .b32 strchrnul_param_1
)
{
	.local .align 8 .b8 	__local_depot10[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<7>;

	mov.u64 	%SPL, __local_depot10;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [strchrnul_param_1];
	ld.param.u64 	%rd1, [strchrnul_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	ld.u8 	%r2, [%SP+8];
	st.u32 	[%SP+8], %r2;
	bra.uni 	$L__BB10_1;
$L__BB10_1:
	ld.u64 	%rd2, [%SP+0];
	ld.s8 	%r3, [%rd2];
	setp.eq.s32 	%p4, %r3, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB10_3;
	bra.uni 	$L__BB10_2;
$L__BB10_2:
	ld.u64 	%rd3, [%SP+0];
	ld.u8 	%r4, [%rd3];
	ld.u32 	%r5, [%SP+8];
	setp.ne.s32 	%p1, %r4, %r5;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB10_3;
$L__BB10_3:
	mov.pred 	%p2, %p5;
	@!%p2 bra 	$L__BB10_6;
	bra.uni 	$L__BB10_4;
$L__BB10_4:
	bra.uni 	$L__BB10_5;
$L__BB10_5:
	ld.u64 	%rd5, [%SP+0];
	add.s64 	%rd6, %rd5, 1;
	st.u64 	[%SP+0], %rd6;
	bra.uni 	$L__BB10_1;
$L__BB10_6:
	ld.u64 	%rd4, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd4;
	ret;

}
	// .globl	strchr
.visible .func  (.param .b64 func_retval0) strchr(
	.param .b64 strchr_param_0,
	.param .b32 strchr_param_1
)
{
	.local .align 8 .b8 	__local_depot11[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<8>;

	mov.u64 	%SPL, __local_depot11;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [strchr_param_1];
	ld.param.u64 	%rd1, [strchr_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u32 	[%SP+16], %r1;
	bra.uni 	$L__BB11_1;
$L__BB11_1:
	ld.u64 	%rd2, [%SP+8];
	ld.s8 	%r2, [%rd2];
	ld.u32 	%r3, [%SP+16];
	setp.ne.s32 	%p1, %r2, %r3;
	@%p1 bra 	$L__BB11_3;
	bra.uni 	$L__BB11_2;
$L__BB11_2:
	ld.u64 	%rd6, [%SP+8];
	st.u64 	[%SP+0], %rd6;
	bra.uni 	$L__BB11_6;
$L__BB11_3:
	bra.uni 	$L__BB11_4;
$L__BB11_4:
	ld.u64 	%rd3, [%SP+8];
	add.s64 	%rd4, %rd3, 1;
	st.u64 	[%SP+8], %rd4;
	ld.u8 	%rs1, [%rd3];
	setp.ne.s16 	%p2, %rs1, 0;
	@%p2 bra 	$L__BB11_1;
	bra.uni 	$L__BB11_5;
$L__BB11_5:
	mov.u64 	%rd5, 0;
	st.u64 	[%SP+0], %rd5;
	bra.uni 	$L__BB11_6;
$L__BB11_6:
	ld.u64 	%rd7, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd7;
	ret;

}
	// .globl	strcmp
.visible .func  (.param .b32 func_retval0) strcmp(
	.param .b64 strcmp_param_0,
	.param .b64 strcmp_param_1
)
{
	.local .align 8 .b8 	__local_depot12[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<12>;

	mov.u64 	%SPL, __local_depot12;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [strcmp_param_1];
	ld.param.u64 	%rd1, [strcmp_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	bra.uni 	$L__BB12_1;
$L__BB12_1:
	ld.u64 	%rd3, [%SP+0];
	ld.s8 	%r1, [%rd3];
	ld.u64 	%rd4, [%SP+8];
	ld.s8 	%r2, [%rd4];
	setp.ne.s32 	%p4, %r1, %r2;
	mov.pred 	%p3, 0;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB12_3;
	bra.uni 	$L__BB12_2;
$L__BB12_2:
	ld.u64 	%rd5, [%SP+0];
	ld.s8 	%r3, [%rd5];
	setp.ne.s32 	%p1, %r3, 0;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB12_3;
$L__BB12_3:
	mov.pred 	%p2, %p5;
	@!%p2 bra 	$L__BB12_6;
	bra.uni 	$L__BB12_4;
$L__BB12_4:
	bra.uni 	$L__BB12_5;
$L__BB12_5:
	ld.u64 	%rd8, [%SP+0];
	add.s64 	%rd9, %rd8, 1;
	st.u64 	[%SP+0], %rd9;
	ld.u64 	%rd10, [%SP+8];
	add.s64 	%rd11, %rd10, 1;
	st.u64 	[%SP+8], %rd11;
	bra.uni 	$L__BB12_1;
$L__BB12_6:
	ld.u64 	%rd6, [%SP+0];
	ld.u8 	%r4, [%rd6];
	ld.u64 	%rd7, [%SP+8];
	ld.u8 	%r5, [%rd7];
	sub.s32 	%r6, %r4, %r5;
	st.param.b32 	[func_retval0+0], %r6;
	ret;

}
	// .globl	strlen
.visible .func  (.param .b64 func_retval0) strlen(
	.param .b64 strlen_param_0
)
{
	.local .align 8 .b8 	__local_depot13[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<9>;

	mov.u64 	%SPL, __local_depot13;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [strlen_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	st.u64 	[%SP+8], %rd2;
	bra.uni 	$L__BB13_1;
$L__BB13_1:
	ld.u64 	%rd3, [%SP+0];
	ld.u8 	%rs1, [%rd3];
	setp.eq.s16 	%p1, %rs1, 0;
	@%p1 bra 	$L__BB13_4;
	bra.uni 	$L__BB13_2;
$L__BB13_2:
	bra.uni 	$L__BB13_3;
$L__BB13_3:
	ld.u64 	%rd7, [%SP+0];
	add.s64 	%rd8, %rd7, 1;
	st.u64 	[%SP+0], %rd8;
	bra.uni 	$L__BB13_1;
$L__BB13_4:
	ld.u64 	%rd4, [%SP+0];
	ld.u64 	%rd5, [%SP+8];
	sub.s64 	%rd6, %rd4, %rd5;
	st.param.b64 	[func_retval0+0], %rd6;
	ret;

}
	// .globl	strncmp
.visible .func  (.param .b32 func_retval0) strncmp(
	.param .b64 strncmp_param_0,
	.param .b64 strncmp_param_1,
	.param .b64 strncmp_param_2
)
{
	.local .align 8 .b8 	__local_depot14[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<21>;

	mov.u64 	%SPL, __local_depot14;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [strncmp_param_2];
	ld.param.u64 	%rd2, [strncmp_param_1];
	ld.param.u64 	%rd1, [strncmp_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	st.u64 	[%SP+24], %rd3;
	ld.u64 	%rd4, [%SP+8];
	st.u64 	[%SP+32], %rd4;
	ld.u64 	%rd5, [%SP+16];
	st.u64 	[%SP+40], %rd5;
	ld.u64 	%rd6, [%SP+24];
	add.s64 	%rd7, %rd6, -1;
	st.u64 	[%SP+24], %rd7;
	setp.ne.s64 	%p3, %rd6, 0;
	@%p3 bra 	$L__BB14_2;
	bra.uni 	$L__BB14_1;
$L__BB14_1:
	mov.b32 	%r1, 0;
	st.u32 	[%SP+0], %r1;
	bra.uni 	$L__BB14_11;
$L__BB14_2:
	bra.uni 	$L__BB14_3;
$L__BB14_3:
	ld.u64 	%rd8, [%SP+32];
	ld.u8 	%rs1, [%rd8];
	setp.eq.s16 	%p5, %rs1, 0;
	mov.pred 	%p4, 0;
	mov.pred 	%p10, %p4;
	@%p5 bra 	$L__BB14_7;
	bra.uni 	$L__BB14_4;
$L__BB14_4:
	ld.u64 	%rd9, [%SP+40];
	ld.u8 	%rs2, [%rd9];
	setp.eq.s16 	%p7, %rs2, 0;
	mov.pred 	%p6, 0;
	mov.pred 	%p10, %p6;
	@%p7 bra 	$L__BB14_7;
	bra.uni 	$L__BB14_5;
$L__BB14_5:
	ld.u64 	%rd10, [%SP+24];
	setp.eq.s64 	%p9, %rd10, 0;
	mov.pred 	%p8, 0;
	mov.pred 	%p10, %p8;
	@%p9 bra 	$L__BB14_7;
	bra.uni 	$L__BB14_6;
$L__BB14_6:
	ld.u64 	%rd11, [%SP+32];
	ld.u8 	%r2, [%rd11];
	ld.u64 	%rd12, [%SP+40];
	ld.u8 	%r3, [%rd12];
	setp.eq.s32 	%p1, %r2, %r3;
	mov.pred 	%p10, %p1;
	bra.uni 	$L__BB14_7;
$L__BB14_7:
	mov.pred 	%p2, %p10;
	@!%p2 bra 	$L__BB14_10;
	bra.uni 	$L__BB14_8;
$L__BB14_8:
	bra.uni 	$L__BB14_9;
$L__BB14_9:
	ld.u64 	%rd15, [%SP+32];
	add.s64 	%rd16, %rd15, 1;
	st.u64 	[%SP+32], %rd16;
	ld.u64 	%rd17, [%SP+40];
	add.s64 	%rd18, %rd17, 1;
	st.u64 	[%SP+40], %rd18;
	ld.u64 	%rd19, [%SP+24];
	add.s64 	%rd20, %rd19, -1;
	st.u64 	[%SP+24], %rd20;
	bra.uni 	$L__BB14_3;
$L__BB14_10:
	ld.u64 	%rd13, [%SP+32];
	ld.u8 	%r4, [%rd13];
	ld.u64 	%rd14, [%SP+40];
	ld.u8 	%r5, [%rd14];
	sub.s32 	%r6, %r4, %r5;
	st.u32 	[%SP+0], %r6;
	bra.uni 	$L__BB14_11;
$L__BB14_11:
	ld.u32 	%r7, [%SP+0];
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	swab
.visible .func swab(
	.param .b64 swab_param_0,
	.param .b64 swab_param_1,
	.param .b64 swab_param_2
)
{
	.local .align 8 .b8 	__local_depot15[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b64 	%rd<17>;

	mov.u64 	%SPL, __local_depot15;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [swab_param_2];
	ld.param.u64 	%rd2, [swab_param_1];
	ld.param.u64 	%rd1, [swab_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+0];
	st.u64 	[%SP+24], %rd4;
	ld.u64 	%rd5, [%SP+8];
	st.u64 	[%SP+32], %rd5;
	bra.uni 	$L__BB15_1;
$L__BB15_1:
	ld.u64 	%rd6, [%SP+16];
	setp.lt.s64 	%p1, %rd6, 2;
	@%p1 bra 	$L__BB15_4;
	bra.uni 	$L__BB15_2;
$L__BB15_2:
	ld.u64 	%rd7, [%SP+24];
	ld.u8 	%rs1, [%rd7+1];
	ld.u64 	%rd8, [%SP+32];
	st.u8 	[%rd8], %rs1;
	ld.u64 	%rd9, [%SP+24];
	ld.u8 	%rs2, [%rd9];
	ld.u64 	%rd10, [%SP+32];
	st.u8 	[%rd10+1], %rs2;
	ld.u64 	%rd11, [%SP+32];
	add.s64 	%rd12, %rd11, 2;
	st.u64 	[%SP+32], %rd12;
	ld.u64 	%rd13, [%SP+24];
	add.s64 	%rd14, %rd13, 2;
	st.u64 	[%SP+24], %rd14;
	bra.uni 	$L__BB15_3;
$L__BB15_3:
	ld.u64 	%rd15, [%SP+16];
	add.s64 	%rd16, %rd15, -2;
	st.u64 	[%SP+16], %rd16;
	bra.uni 	$L__BB15_1;
$L__BB15_4:
	ret;

}
	// .globl	isalpha
.visible .func  (.param .b32 func_retval0) isalpha(
	.param .b32 isalpha_param_0
)
{
	.local .align 4 .b8 	__local_depot16[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;

	mov.u64 	%SPL, __local_depot16;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [isalpha_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	or.b32  	%r3, %r2, 32;
	add.s32 	%r4, %r3, -97;
	setp.lt.u32 	%p1, %r4, 26;
	selp.u32 	%r5, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	isascii
.visible .func  (.param .b32 func_retval0) isascii(
	.param .b32 isascii_param_0
)
{
	.local .align 4 .b8 	__local_depot17[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u64 	%SPL, __local_depot17;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [isascii_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	and.b32  	%r3, %r2, -128;
	setp.eq.s32 	%p1, %r3, 0;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	isblank
.visible .func  (.param .b32 func_retval0) isblank(
	.param .b32 isblank_param_0
)
{
	.local .align 4 .b8 	__local_depot18[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<5>;

	mov.u64 	%SPL, __local_depot18;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [isblank_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	setp.eq.s32 	%p4, %r2, 32;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB18_2;
	bra.uni 	$L__BB18_1;
$L__BB18_1:
	ld.u32 	%r3, [%SP+0];
	setp.eq.s32 	%p1, %r3, 9;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB18_2;
$L__BB18_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r4, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	iscntrl
.visible .func  (.param .b32 func_retval0) iscntrl(
	.param .b32 iscntrl_param_0
)
{
	.local .align 4 .b8 	__local_depot19[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<5>;

	mov.u64 	%SPL, __local_depot19;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [iscntrl_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	setp.lt.u32 	%p4, %r2, 32;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB19_2;
	bra.uni 	$L__BB19_1;
$L__BB19_1:
	ld.u32 	%r3, [%SP+0];
	setp.eq.s32 	%p1, %r3, 127;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB19_2;
$L__BB19_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r4, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	isdigit
.visible .func  (.param .b32 func_retval0) isdigit(
	.param .b32 isdigit_param_0
)
{
	.local .align 4 .b8 	__local_depot20[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u64 	%SPL, __local_depot20;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [isdigit_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -48;
	setp.lt.u32 	%p1, %r3, 10;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	isgraph
.visible .func  (.param .b32 func_retval0) isgraph(
	.param .b32 isgraph_param_0
)
{
	.local .align 4 .b8 	__local_depot21[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u64 	%SPL, __local_depot21;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [isgraph_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -33;
	setp.lt.u32 	%p1, %r3, 94;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	islower
.visible .func  (.param .b32 func_retval0) islower(
	.param .b32 islower_param_0
)
{
	.local .align 4 .b8 	__local_depot22[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u64 	%SPL, __local_depot22;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [islower_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -97;
	setp.lt.u32 	%p1, %r3, 26;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	isprint
.visible .func  (.param .b32 func_retval0) isprint(
	.param .b32 isprint_param_0
)
{
	.local .align 4 .b8 	__local_depot23[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u64 	%SPL, __local_depot23;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [isprint_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -32;
	setp.lt.u32 	%p1, %r3, 95;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	isspace
.visible .func  (.param .b32 func_retval0) isspace(
	.param .b32 isspace_param_0
)
{
	.local .align 4 .b8 	__local_depot24[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<6>;

	mov.u64 	%SPL, __local_depot24;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [isspace_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	setp.eq.s32 	%p4, %r2, 32;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB24_2;
	bra.uni 	$L__BB24_1;
$L__BB24_1:
	ld.u32 	%r3, [%SP+0];
	add.s32 	%r4, %r3, -9;
	setp.lt.u32 	%p1, %r4, 5;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB24_2;
$L__BB24_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r5, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	isupper
.visible .func  (.param .b32 func_retval0) isupper(
	.param .b32 isupper_param_0
)
{
	.local .align 4 .b8 	__local_depot25[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u64 	%SPL, __local_depot25;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [isupper_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -65;
	setp.lt.u32 	%p1, %r3, 26;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	iswcntrl
.visible .func  (.param .b32 func_retval0) iswcntrl(
	.param .b32 iswcntrl_param_0
)
{
	.local .align 4 .b8 	__local_depot26[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<10>;

	mov.u64 	%SPL, __local_depot26;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [iswcntrl_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	setp.lt.u32 	%p4, %r2, 32;
	mov.pred 	%p3, -1;
	mov.pred 	%p9, %p3;
	@%p4 bra 	$L__BB26_4;
	bra.uni 	$L__BB26_1;
$L__BB26_1:
	ld.u32 	%r3, [%SP+0];
	add.s32 	%r4, %r3, -127;
	setp.lt.u32 	%p6, %r4, 33;
	mov.pred 	%p5, -1;
	mov.pred 	%p9, %p5;
	@%p6 bra 	$L__BB26_4;
	bra.uni 	$L__BB26_2;
$L__BB26_2:
	ld.u32 	%r5, [%SP+0];
	add.s32 	%r6, %r5, -8232;
	setp.lt.u32 	%p8, %r6, 2;
	mov.pred 	%p7, -1;
	mov.pred 	%p9, %p7;
	@%p8 bra 	$L__BB26_4;
	bra.uni 	$L__BB26_3;
$L__BB26_3:
	ld.u32 	%r7, [%SP+0];
	add.s32 	%r8, %r7, -65529;
	setp.lt.u32 	%p1, %r8, 3;
	mov.pred 	%p9, %p1;
	bra.uni 	$L__BB26_4;
$L__BB26_4:
	mov.pred 	%p2, %p9;
	selp.u32 	%r9, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	iswdigit
.visible .func  (.param .b32 func_retval0) iswdigit(
	.param .b32 iswdigit_param_0
)
{
	.local .align 4 .b8 	__local_depot27[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;

	mov.u64 	%SPL, __local_depot27;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [iswdigit_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -48;
	setp.lt.u32 	%p1, %r3, 10;
	selp.u32 	%r4, 1, 0, %p1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	iswprint
.visible .func  (.param .b32 func_retval0) iswprint(
	.param .b32 iswprint_param_0
)
{
	.local .align 4 .b8 	__local_depot28[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<8>;
	.reg .b32 	%r<20>;

	mov.u64 	%SPL, __local_depot28;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [iswprint_param_0];
	st.u32 	[%SP+4], %r1;
	ld.u32 	%r2, [%SP+4];
	setp.gt.u32 	%p1, %r2, 254;
	@%p1 bra 	$L__BB28_2;
	bra.uni 	$L__BB28_1;
$L__BB28_1:
	ld.u32 	%r15, [%SP+4];
	add.s32 	%r16, %r15, 1;
	and.b32  	%r17, %r16, 127;
	setp.gt.s32 	%p7, %r17, 32;
	selp.u32 	%r18, 1, 0, %p7;
	st.u32 	[%SP+0], %r18;
	bra.uni 	$L__BB28_10;
$L__BB28_2:
	ld.u32 	%r3, [%SP+4];
	setp.lt.u32 	%p2, %r3, 8232;
	@%p2 bra 	$L__BB28_5;
	bra.uni 	$L__BB28_3;
$L__BB28_3:
	ld.u32 	%r4, [%SP+4];
	add.s32 	%r5, %r4, -8234;
	setp.lt.u32 	%p3, %r5, 47062;
	@%p3 bra 	$L__BB28_5;
	bra.uni 	$L__BB28_4;
$L__BB28_4:
	ld.u32 	%r6, [%SP+4];
	add.s32 	%r7, %r6, -57344;
	setp.gt.u32 	%p4, %r7, 8184;
	@%p4 bra 	$L__BB28_6;
	bra.uni 	$L__BB28_5;
$L__BB28_5:
	mov.b32 	%r14, 1;
	st.u32 	[%SP+0], %r14;
	bra.uni 	$L__BB28_10;
$L__BB28_6:
	ld.u32 	%r8, [%SP+4];
	add.s32 	%r9, %r8, -65532;
	setp.gt.u32 	%p5, %r9, 1048579;
	@%p5 bra 	$L__BB28_8;
	bra.uni 	$L__BB28_7;
$L__BB28_7:
	ld.u32 	%r10, [%SP+4];
	and.b32  	%r11, %r10, 65534;
	setp.ne.s32 	%p6, %r11, 65534;
	@%p6 bra 	$L__BB28_9;
	bra.uni 	$L__BB28_8;
$L__BB28_8:
	mov.b32 	%r13, 0;
	st.u32 	[%SP+0], %r13;
	bra.uni 	$L__BB28_10;
$L__BB28_9:
	mov.b32 	%r12, 1;
	st.u32 	[%SP+0], %r12;
	bra.uni 	$L__BB28_10;
$L__BB28_10:
	ld.u32 	%r19, [%SP+0];
	st.param.b32 	[func_retval0+0], %r19;
	ret;

}
	// .globl	iswxdigit
.visible .func  (.param .b32 func_retval0) iswxdigit(
	.param .b32 iswxdigit_param_0
)
{
	.local .align 4 .b8 	__local_depot29[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<8>;

	mov.u64 	%SPL, __local_depot29;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [iswxdigit_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -48;
	setp.lt.u32 	%p4, %r3, 10;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB29_2;
	bra.uni 	$L__BB29_1;
$L__BB29_1:
	ld.u32 	%r4, [%SP+0];
	or.b32  	%r5, %r4, 32;
	add.s32 	%r6, %r5, -97;
	setp.lt.u32 	%p1, %r6, 6;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB29_2;
$L__BB29_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r7, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	toascii
.visible .func  (.param .b32 func_retval0) toascii(
	.param .b32 toascii_param_0
)
{
	.local .align 4 .b8 	__local_depot30[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<4>;

	mov.u64 	%SPL, __local_depot30;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [toascii_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	and.b32  	%r3, %r2, 127;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	fdim
.visible .func  (.param .b64 func_retval0) fdim(
	.param .b64 fdim_param_0,
	.param .b64 fdim_param_1
)
{
	.local .align 8 .b8 	__local_depot31[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<4>;
	.reg .f64 	%fd<16>;

	mov.u64 	%SPL, __local_depot31;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd4, [fdim_param_1];
	ld.param.f64 	%fd3, [fdim_param_0];
	st.f64 	[%SP+8], %fd3;
	st.f64 	[%SP+16], %fd4;
	ld.f64 	%fd5, [%SP+8];
	setp.num.f64 	%p1, %fd5, %fd5;
	@%p1 bra 	$L__BB31_2;
	bra.uni 	$L__BB31_1;
$L__BB31_1:
	ld.f64 	%fd13, [%SP+8];
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB31_8;
$L__BB31_2:
	ld.f64 	%fd6, [%SP+16];
	setp.num.f64 	%p2, %fd6, %fd6;
	@%p2 bra 	$L__BB31_4;
	bra.uni 	$L__BB31_3;
$L__BB31_3:
	ld.f64 	%fd12, [%SP+16];
	st.f64 	[%SP+0], %fd12;
	bra.uni 	$L__BB31_8;
$L__BB31_4:
	ld.f64 	%fd7, [%SP+8];
	ld.f64 	%fd8, [%SP+16];
	setp.leu.f64 	%p3, %fd7, %fd8;
	@%p3 bra 	$L__BB31_6;
	bra.uni 	$L__BB31_5;
$L__BB31_5:
	ld.f64 	%fd10, [%SP+8];
	ld.f64 	%fd11, [%SP+16];
	sub.rn.f64 	%fd1, %fd10, %fd11;
	mov.f64 	%fd15, %fd1;
	bra.uni 	$L__BB31_7;
$L__BB31_6:
	mov.f64 	%fd9, 0d0000000000000000;
	mov.f64 	%fd15, %fd9;
	bra.uni 	$L__BB31_7;
$L__BB31_7:
	mov.f64 	%fd2, %fd15;
	st.f64 	[%SP+0], %fd2;
	bra.uni 	$L__BB31_8;
$L__BB31_8:
	ld.f64 	%fd14, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd14;
	ret;

}
	// .globl	fdimf
.visible .func  (.param .b32 func_retval0) fdimf(
	.param .b32 fdimf_param_0,
	.param .b32 fdimf_param_1
)
{
	.local .align 4 .b8 	__local_depot32[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<4>;
	.reg .f32 	%f<16>;

	mov.u64 	%SPL, __local_depot32;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f4, [fdimf_param_1];
	ld.param.f32 	%f3, [fdimf_param_0];
	st.f32 	[%SP+4], %f3;
	st.f32 	[%SP+8], %f4;
	ld.f32 	%f5, [%SP+4];
	setp.num.f32 	%p1, %f5, %f5;
	@%p1 bra 	$L__BB32_2;
	bra.uni 	$L__BB32_1;
$L__BB32_1:
	ld.f32 	%f13, [%SP+4];
	st.f32 	[%SP+0], %f13;
	bra.uni 	$L__BB32_8;
$L__BB32_2:
	ld.f32 	%f6, [%SP+8];
	setp.num.f32 	%p2, %f6, %f6;
	@%p2 bra 	$L__BB32_4;
	bra.uni 	$L__BB32_3;
$L__BB32_3:
	ld.f32 	%f12, [%SP+8];
	st.f32 	[%SP+0], %f12;
	bra.uni 	$L__BB32_8;
$L__BB32_4:
	ld.f32 	%f7, [%SP+4];
	ld.f32 	%f8, [%SP+8];
	setp.leu.f32 	%p3, %f7, %f8;
	@%p3 bra 	$L__BB32_6;
	bra.uni 	$L__BB32_5;
$L__BB32_5:
	ld.f32 	%f10, [%SP+4];
	ld.f32 	%f11, [%SP+8];
	sub.rn.f32 	%f1, %f10, %f11;
	mov.f32 	%f15, %f1;
	bra.uni 	$L__BB32_7;
$L__BB32_6:
	mov.f32 	%f9, 0f00000000;
	mov.f32 	%f15, %f9;
	bra.uni 	$L__BB32_7;
$L__BB32_7:
	mov.f32 	%f2, %f15;
	st.f32 	[%SP+0], %f2;
	bra.uni 	$L__BB32_8;
$L__BB32_8:
	ld.f32 	%f14, [%SP+0];
	st.param.f32 	[func_retval0+0], %f14;
	ret;

}
	// .globl	fmax
.visible .func  (.param .b64 func_retval0) fmax(
	.param .b64 fmax_param_0,
	.param .b64 fmax_param_1
)
{
	.local .align 8 .b8 	__local_depot33[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<6>;
	.reg .f64 	%fd<18>;

	mov.u64 	%SPL, __local_depot33;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd8, [fmax_param_1];
	ld.param.f64 	%fd7, [fmax_param_0];
	st.f64 	[%SP+8], %fd7;
	st.f64 	[%SP+16], %fd8;
	ld.f64 	%fd9, [%SP+8];
	setp.num.f64 	%p1, %fd9, %fd9;
	@%p1 bra 	$L__BB33_2;
	bra.uni 	$L__BB33_1;
$L__BB33_1:
	ld.f64 	%fd14, [%SP+16];
	st.f64 	[%SP+0], %fd14;
	bra.uni 	$L__BB33_13;
$L__BB33_2:
	ld.f64 	%fd10, [%SP+16];
	setp.num.f64 	%p2, %fd10, %fd10;
	@%p2 bra 	$L__BB33_4;
	bra.uni 	$L__BB33_3;
$L__BB33_3:
	ld.f64 	%fd13, [%SP+8];
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB33_13;
$L__BB33_4:
	ld.u64 	%rd1, [%SP+8];
	shr.u64 	%rd2, %rd1, 63;
	cvt.u32.u64 	%r1, %rd2;
	ld.u64 	%rd3, [%SP+16];
	shr.u64 	%rd4, %rd3, 63;
	cvt.u32.u64 	%r2, %rd4;
	setp.eq.s32 	%p3, %r1, %r2;
	@%p3 bra 	$L__BB33_9;
	bra.uni 	$L__BB33_5;
$L__BB33_5:
	ld.u64 	%rd5, [%SP+8];
	setp.gt.s64 	%p5, %rd5, -1;
	@%p5 bra 	$L__BB33_7;
	bra.uni 	$L__BB33_6;
$L__BB33_6:
	ld.f64 	%fd1, [%SP+16];
	mov.f64 	%fd16, %fd1;
	bra.uni 	$L__BB33_8;
$L__BB33_7:
	ld.f64 	%fd2, [%SP+8];
	mov.f64 	%fd16, %fd2;
	bra.uni 	$L__BB33_8;
$L__BB33_8:
	mov.f64 	%fd3, %fd16;
	st.f64 	[%SP+0], %fd3;
	bra.uni 	$L__BB33_13;
$L__BB33_9:
	ld.f64 	%fd11, [%SP+8];
	ld.f64 	%fd12, [%SP+16];
	setp.geu.f64 	%p4, %fd11, %fd12;
	@%p4 bra 	$L__BB33_11;
	bra.uni 	$L__BB33_10;
$L__BB33_10:
	ld.f64 	%fd4, [%SP+16];
	mov.f64 	%fd17, %fd4;
	bra.uni 	$L__BB33_12;
$L__BB33_11:
	ld.f64 	%fd5, [%SP+8];
	mov.f64 	%fd17, %fd5;
	bra.uni 	$L__BB33_12;
$L__BB33_12:
	mov.f64 	%fd6, %fd17;
	st.f64 	[%SP+0], %fd6;
	bra.uni 	$L__BB33_13;
$L__BB33_13:
	ld.f64 	%fd15, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd15;
	ret;

}
	// .globl	fmaxf
.visible .func  (.param .b32 func_retval0) fmaxf(
	.param .b32 fmaxf_param_0,
	.param .b32 fmaxf_param_1
)
{
	.local .align 4 .b8 	__local_depot34[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<6>;
	.reg .f32 	%f<18>;

	mov.u64 	%SPL, __local_depot34;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f8, [fmaxf_param_1];
	ld.param.f32 	%f7, [fmaxf_param_0];
	st.f32 	[%SP+4], %f7;
	st.f32 	[%SP+8], %f8;
	ld.f32 	%f9, [%SP+4];
	setp.num.f32 	%p1, %f9, %f9;
	@%p1 bra 	$L__BB34_2;
	bra.uni 	$L__BB34_1;
$L__BB34_1:
	ld.f32 	%f14, [%SP+8];
	st.f32 	[%SP+0], %f14;
	bra.uni 	$L__BB34_13;
$L__BB34_2:
	ld.f32 	%f10, [%SP+8];
	setp.num.f32 	%p2, %f10, %f10;
	@%p2 bra 	$L__BB34_4;
	bra.uni 	$L__BB34_3;
$L__BB34_3:
	ld.f32 	%f13, [%SP+4];
	st.f32 	[%SP+0], %f13;
	bra.uni 	$L__BB34_13;
$L__BB34_4:
	ld.u32 	%r1, [%SP+4];
	shr.u32 	%r2, %r1, 31;
	ld.u32 	%r3, [%SP+8];
	shr.u32 	%r4, %r3, 31;
	setp.eq.s32 	%p3, %r2, %r4;
	@%p3 bra 	$L__BB34_9;
	bra.uni 	$L__BB34_5;
$L__BB34_5:
	ld.u32 	%r5, [%SP+4];
	setp.gt.s32 	%p5, %r5, -1;
	@%p5 bra 	$L__BB34_7;
	bra.uni 	$L__BB34_6;
$L__BB34_6:
	ld.f32 	%f1, [%SP+8];
	mov.f32 	%f16, %f1;
	bra.uni 	$L__BB34_8;
$L__BB34_7:
	ld.f32 	%f2, [%SP+4];
	mov.f32 	%f16, %f2;
	bra.uni 	$L__BB34_8;
$L__BB34_8:
	mov.f32 	%f3, %f16;
	st.f32 	[%SP+0], %f3;
	bra.uni 	$L__BB34_13;
$L__BB34_9:
	ld.f32 	%f11, [%SP+4];
	ld.f32 	%f12, [%SP+8];
	setp.geu.f32 	%p4, %f11, %f12;
	@%p4 bra 	$L__BB34_11;
	bra.uni 	$L__BB34_10;
$L__BB34_10:
	ld.f32 	%f4, [%SP+8];
	mov.f32 	%f17, %f4;
	bra.uni 	$L__BB34_12;
$L__BB34_11:
	ld.f32 	%f5, [%SP+4];
	mov.f32 	%f17, %f5;
	bra.uni 	$L__BB34_12;
$L__BB34_12:
	mov.f32 	%f6, %f17;
	st.f32 	[%SP+0], %f6;
	bra.uni 	$L__BB34_13;
$L__BB34_13:
	ld.f32 	%f15, [%SP+0];
	st.param.f32 	[func_retval0+0], %f15;
	ret;

}
	// .globl	fmaxl
.visible .func  (.param .b64 func_retval0) fmaxl(
	.param .b64 fmaxl_param_0,
	.param .b64 fmaxl_param_1
)
{
	.local .align 8 .b8 	__local_depot35[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<6>;
	.reg .f64 	%fd<18>;

	mov.u64 	%SPL, __local_depot35;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd8, [fmaxl_param_1];
	ld.param.f64 	%fd7, [fmaxl_param_0];
	st.f64 	[%SP+8], %fd7;
	st.f64 	[%SP+16], %fd8;
	ld.f64 	%fd9, [%SP+8];
	setp.num.f64 	%p1, %fd9, %fd9;
	@%p1 bra 	$L__BB35_2;
	bra.uni 	$L__BB35_1;
$L__BB35_1:
	ld.f64 	%fd14, [%SP+16];
	st.f64 	[%SP+0], %fd14;
	bra.uni 	$L__BB35_13;
$L__BB35_2:
	ld.f64 	%fd10, [%SP+16];
	setp.num.f64 	%p2, %fd10, %fd10;
	@%p2 bra 	$L__BB35_4;
	bra.uni 	$L__BB35_3;
$L__BB35_3:
	ld.f64 	%fd13, [%SP+8];
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB35_13;
$L__BB35_4:
	ld.u64 	%rd1, [%SP+8];
	shr.u64 	%rd2, %rd1, 63;
	cvt.u32.u64 	%r1, %rd2;
	ld.u64 	%rd3, [%SP+16];
	shr.u64 	%rd4, %rd3, 63;
	cvt.u32.u64 	%r2, %rd4;
	setp.eq.s32 	%p3, %r1, %r2;
	@%p3 bra 	$L__BB35_9;
	bra.uni 	$L__BB35_5;
$L__BB35_5:
	ld.u64 	%rd5, [%SP+8];
	setp.gt.s64 	%p5, %rd5, -1;
	@%p5 bra 	$L__BB35_7;
	bra.uni 	$L__BB35_6;
$L__BB35_6:
	ld.f64 	%fd1, [%SP+16];
	mov.f64 	%fd16, %fd1;
	bra.uni 	$L__BB35_8;
$L__BB35_7:
	ld.f64 	%fd2, [%SP+8];
	mov.f64 	%fd16, %fd2;
	bra.uni 	$L__BB35_8;
$L__BB35_8:
	mov.f64 	%fd3, %fd16;
	st.f64 	[%SP+0], %fd3;
	bra.uni 	$L__BB35_13;
$L__BB35_9:
	ld.f64 	%fd11, [%SP+8];
	ld.f64 	%fd12, [%SP+16];
	setp.geu.f64 	%p4, %fd11, %fd12;
	@%p4 bra 	$L__BB35_11;
	bra.uni 	$L__BB35_10;
$L__BB35_10:
	ld.f64 	%fd4, [%SP+16];
	mov.f64 	%fd17, %fd4;
	bra.uni 	$L__BB35_12;
$L__BB35_11:
	ld.f64 	%fd5, [%SP+8];
	mov.f64 	%fd17, %fd5;
	bra.uni 	$L__BB35_12;
$L__BB35_12:
	mov.f64 	%fd6, %fd17;
	st.f64 	[%SP+0], %fd6;
	bra.uni 	$L__BB35_13;
$L__BB35_13:
	ld.f64 	%fd15, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd15;
	ret;

}
	// .globl	fmin
.visible .func  (.param .b64 func_retval0) fmin(
	.param .b64 fmin_param_0,
	.param .b64 fmin_param_1
)
{
	.local .align 8 .b8 	__local_depot36[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<6>;
	.reg .f64 	%fd<18>;

	mov.u64 	%SPL, __local_depot36;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd8, [fmin_param_1];
	ld.param.f64 	%fd7, [fmin_param_0];
	st.f64 	[%SP+8], %fd7;
	st.f64 	[%SP+16], %fd8;
	ld.f64 	%fd9, [%SP+8];
	setp.num.f64 	%p1, %fd9, %fd9;
	@%p1 bra 	$L__BB36_2;
	bra.uni 	$L__BB36_1;
$L__BB36_1:
	ld.f64 	%fd14, [%SP+16];
	st.f64 	[%SP+0], %fd14;
	bra.uni 	$L__BB36_13;
$L__BB36_2:
	ld.f64 	%fd10, [%SP+16];
	setp.num.f64 	%p2, %fd10, %fd10;
	@%p2 bra 	$L__BB36_4;
	bra.uni 	$L__BB36_3;
$L__BB36_3:
	ld.f64 	%fd13, [%SP+8];
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB36_13;
$L__BB36_4:
	ld.u64 	%rd1, [%SP+8];
	shr.u64 	%rd2, %rd1, 63;
	cvt.u32.u64 	%r1, %rd2;
	ld.u64 	%rd3, [%SP+16];
	shr.u64 	%rd4, %rd3, 63;
	cvt.u32.u64 	%r2, %rd4;
	setp.eq.s32 	%p3, %r1, %r2;
	@%p3 bra 	$L__BB36_9;
	bra.uni 	$L__BB36_5;
$L__BB36_5:
	ld.u64 	%rd5, [%SP+8];
	setp.gt.s64 	%p5, %rd5, -1;
	@%p5 bra 	$L__BB36_7;
	bra.uni 	$L__BB36_6;
$L__BB36_6:
	ld.f64 	%fd1, [%SP+8];
	mov.f64 	%fd16, %fd1;
	bra.uni 	$L__BB36_8;
$L__BB36_7:
	ld.f64 	%fd2, [%SP+16];
	mov.f64 	%fd16, %fd2;
	bra.uni 	$L__BB36_8;
$L__BB36_8:
	mov.f64 	%fd3, %fd16;
	st.f64 	[%SP+0], %fd3;
	bra.uni 	$L__BB36_13;
$L__BB36_9:
	ld.f64 	%fd11, [%SP+8];
	ld.f64 	%fd12, [%SP+16];
	setp.geu.f64 	%p4, %fd11, %fd12;
	@%p4 bra 	$L__BB36_11;
	bra.uni 	$L__BB36_10;
$L__BB36_10:
	ld.f64 	%fd4, [%SP+8];
	mov.f64 	%fd17, %fd4;
	bra.uni 	$L__BB36_12;
$L__BB36_11:
	ld.f64 	%fd5, [%SP+16];
	mov.f64 	%fd17, %fd5;
	bra.uni 	$L__BB36_12;
$L__BB36_12:
	mov.f64 	%fd6, %fd17;
	st.f64 	[%SP+0], %fd6;
	bra.uni 	$L__BB36_13;
$L__BB36_13:
	ld.f64 	%fd15, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd15;
	ret;

}
	// .globl	fminf
.visible .func  (.param .b32 func_retval0) fminf(
	.param .b32 fminf_param_0,
	.param .b32 fminf_param_1
)
{
	.local .align 4 .b8 	__local_depot37[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<6>;
	.reg .f32 	%f<18>;

	mov.u64 	%SPL, __local_depot37;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f8, [fminf_param_1];
	ld.param.f32 	%f7, [fminf_param_0];
	st.f32 	[%SP+4], %f7;
	st.f32 	[%SP+8], %f8;
	ld.f32 	%f9, [%SP+4];
	setp.num.f32 	%p1, %f9, %f9;
	@%p1 bra 	$L__BB37_2;
	bra.uni 	$L__BB37_1;
$L__BB37_1:
	ld.f32 	%f14, [%SP+8];
	st.f32 	[%SP+0], %f14;
	bra.uni 	$L__BB37_13;
$L__BB37_2:
	ld.f32 	%f10, [%SP+8];
	setp.num.f32 	%p2, %f10, %f10;
	@%p2 bra 	$L__BB37_4;
	bra.uni 	$L__BB37_3;
$L__BB37_3:
	ld.f32 	%f13, [%SP+4];
	st.f32 	[%SP+0], %f13;
	bra.uni 	$L__BB37_13;
$L__BB37_4:
	ld.u32 	%r1, [%SP+4];
	shr.u32 	%r2, %r1, 31;
	ld.u32 	%r3, [%SP+8];
	shr.u32 	%r4, %r3, 31;
	setp.eq.s32 	%p3, %r2, %r4;
	@%p3 bra 	$L__BB37_9;
	bra.uni 	$L__BB37_5;
$L__BB37_5:
	ld.u32 	%r5, [%SP+4];
	setp.gt.s32 	%p5, %r5, -1;
	@%p5 bra 	$L__BB37_7;
	bra.uni 	$L__BB37_6;
$L__BB37_6:
	ld.f32 	%f1, [%SP+4];
	mov.f32 	%f16, %f1;
	bra.uni 	$L__BB37_8;
$L__BB37_7:
	ld.f32 	%f2, [%SP+8];
	mov.f32 	%f16, %f2;
	bra.uni 	$L__BB37_8;
$L__BB37_8:
	mov.f32 	%f3, %f16;
	st.f32 	[%SP+0], %f3;
	bra.uni 	$L__BB37_13;
$L__BB37_9:
	ld.f32 	%f11, [%SP+4];
	ld.f32 	%f12, [%SP+8];
	setp.geu.f32 	%p4, %f11, %f12;
	@%p4 bra 	$L__BB37_11;
	bra.uni 	$L__BB37_10;
$L__BB37_10:
	ld.f32 	%f4, [%SP+4];
	mov.f32 	%f17, %f4;
	bra.uni 	$L__BB37_12;
$L__BB37_11:
	ld.f32 	%f5, [%SP+8];
	mov.f32 	%f17, %f5;
	bra.uni 	$L__BB37_12;
$L__BB37_12:
	mov.f32 	%f6, %f17;
	st.f32 	[%SP+0], %f6;
	bra.uni 	$L__BB37_13;
$L__BB37_13:
	ld.f32 	%f15, [%SP+0];
	st.param.f32 	[func_retval0+0], %f15;
	ret;

}
	// .globl	fminl
.visible .func  (.param .b64 func_retval0) fminl(
	.param .b64 fminl_param_0,
	.param .b64 fminl_param_1
)
{
	.local .align 8 .b8 	__local_depot38[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<6>;
	.reg .f64 	%fd<18>;

	mov.u64 	%SPL, __local_depot38;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd8, [fminl_param_1];
	ld.param.f64 	%fd7, [fminl_param_0];
	st.f64 	[%SP+8], %fd7;
	st.f64 	[%SP+16], %fd8;
	ld.f64 	%fd9, [%SP+8];
	setp.num.f64 	%p1, %fd9, %fd9;
	@%p1 bra 	$L__BB38_2;
	bra.uni 	$L__BB38_1;
$L__BB38_1:
	ld.f64 	%fd14, [%SP+16];
	st.f64 	[%SP+0], %fd14;
	bra.uni 	$L__BB38_13;
$L__BB38_2:
	ld.f64 	%fd10, [%SP+16];
	setp.num.f64 	%p2, %fd10, %fd10;
	@%p2 bra 	$L__BB38_4;
	bra.uni 	$L__BB38_3;
$L__BB38_3:
	ld.f64 	%fd13, [%SP+8];
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB38_13;
$L__BB38_4:
	ld.u64 	%rd1, [%SP+8];
	shr.u64 	%rd2, %rd1, 63;
	cvt.u32.u64 	%r1, %rd2;
	ld.u64 	%rd3, [%SP+16];
	shr.u64 	%rd4, %rd3, 63;
	cvt.u32.u64 	%r2, %rd4;
	setp.eq.s32 	%p3, %r1, %r2;
	@%p3 bra 	$L__BB38_9;
	bra.uni 	$L__BB38_5;
$L__BB38_5:
	ld.u64 	%rd5, [%SP+8];
	setp.gt.s64 	%p5, %rd5, -1;
	@%p5 bra 	$L__BB38_7;
	bra.uni 	$L__BB38_6;
$L__BB38_6:
	ld.f64 	%fd1, [%SP+8];
	mov.f64 	%fd16, %fd1;
	bra.uni 	$L__BB38_8;
$L__BB38_7:
	ld.f64 	%fd2, [%SP+16];
	mov.f64 	%fd16, %fd2;
	bra.uni 	$L__BB38_8;
$L__BB38_8:
	mov.f64 	%fd3, %fd16;
	st.f64 	[%SP+0], %fd3;
	bra.uni 	$L__BB38_13;
$L__BB38_9:
	ld.f64 	%fd11, [%SP+8];
	ld.f64 	%fd12, [%SP+16];
	setp.geu.f64 	%p4, %fd11, %fd12;
	@%p4 bra 	$L__BB38_11;
	bra.uni 	$L__BB38_10;
$L__BB38_10:
	ld.f64 	%fd4, [%SP+8];
	mov.f64 	%fd17, %fd4;
	bra.uni 	$L__BB38_12;
$L__BB38_11:
	ld.f64 	%fd5, [%SP+16];
	mov.f64 	%fd17, %fd5;
	bra.uni 	$L__BB38_12;
$L__BB38_12:
	mov.f64 	%fd6, %fd17;
	st.f64 	[%SP+0], %fd6;
	bra.uni 	$L__BB38_13;
$L__BB38_13:
	ld.f64 	%fd15, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd15;
	ret;

}
	// .globl	l64a
.visible .func  (.param .b64 func_retval0) l64a(
	.param .b64 l64a_param_0
)
{
	.local .align 8 .b8 	__local_depot39[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<16>;

	mov.u64 	%SPL, __local_depot39;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [l64a_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	st.u32 	[%SP+16], %rd2;
	mov.u64 	%rd3, l64a_$_s;
	cvta.global.u64 	%rd4, %rd3;
	st.u64 	[%SP+8], %rd4;
	bra.uni 	$L__BB39_1;
$L__BB39_1:
	ld.u32 	%r1, [%SP+16];
	setp.eq.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB39_4;
	bra.uni 	$L__BB39_2;
$L__BB39_2:
	ld.u32 	%rd8, [%SP+16];
	and.b64  	%rd9, %rd8, 63;
	mov.u64 	%rd10, digits;
	cvta.global.u64 	%rd11, %rd10;
	add.s64 	%rd12, %rd11, %rd9;
	ld.u8 	%rs2, [%rd12];
	ld.u64 	%rd13, [%SP+8];
	st.u8 	[%rd13], %rs2;
	bra.uni 	$L__BB39_3;
$L__BB39_3:
	ld.u64 	%rd14, [%SP+8];
	add.s64 	%rd15, %rd14, 1;
	st.u64 	[%SP+8], %rd15;
	ld.u32 	%r2, [%SP+16];
	shr.u32 	%r3, %r2, 6;
	st.u32 	[%SP+16], %r3;
	bra.uni 	$L__BB39_1;
$L__BB39_4:
	ld.u64 	%rd5, [%SP+8];
	mov.u16 	%rs1, 0;
	st.u8 	[%rd5], %rs1;
	mov.u64 	%rd6, l64a_$_s;
	cvta.global.u64 	%rd7, %rd6;
	st.param.b64 	[func_retval0+0], %rd7;
	ret;

}
	// .globl	srand
.visible .func srand(
	.param .b32 srand_param_0
)
{
	.local .align 4 .b8 	__local_depot40[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<4>;

	mov.u64 	%SPL, __local_depot40;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [srand_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	add.s32 	%r3, %r2, -1;
	cvt.u64.u32 	%rd1, %r3;
	mov.u64 	%rd2, seed;
	cvta.global.u64 	%rd3, %rd2;
	st.u64 	[%rd3], %rd1;
	ret;

}
	// .globl	rand
.visible .func  (.param .b32 func_retval0) rand()
{
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<8>;

	mov.u64 	%rd1, seed;
	cvta.global.u64 	%rd2, %rd1;
	ld.u64 	%rd3, [%rd2];
	mul.lo.s64 	%rd4, %rd3, 6364136223846793005;
	add.s64 	%rd5, %rd4, 1;
	st.u64 	[%rd2], %rd5;
	ld.u64 	%rd6, [%rd2];
	shr.u64 	%rd7, %rd6, 33;
	cvt.u32.u64 	%r1, %rd7;
	st.param.b32 	[func_retval0+0], %r1;
	ret;

}
	// .globl	insque
.visible .func insque(
	.param .b64 insque_param_0,
	.param .b64 insque_param_1
)
{
	.local .align 8 .b8 	__local_depot42[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b64 	%rd<20>;

	mov.u64 	%SPL, __local_depot42;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [insque_param_1];
	ld.param.u64 	%rd1, [insque_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+8];
	st.u64 	[%SP+24], %rd4;
	ld.u64 	%rd5, [%SP+24];
	setp.ne.s64 	%p1, %rd5, 0;
	@%p1 bra 	$L__BB42_2;
	bra.uni 	$L__BB42_1;
$L__BB42_1:
	ld.u64 	%rd6, [%SP+16];
	mov.u64 	%rd7, 0;
	st.u64 	[%rd6+8], %rd7;
	ld.u64 	%rd8, [%SP+16];
	st.u64 	[%rd8], %rd7;
	bra.uni 	$L__BB42_4;
$L__BB42_2:
	ld.u64 	%rd9, [%SP+24];
	ld.u64 	%rd10, [%rd9];
	ld.u64 	%rd11, [%SP+16];
	st.u64 	[%rd11], %rd10;
	ld.u64 	%rd12, [%SP+24];
	ld.u64 	%rd13, [%SP+16];
	st.u64 	[%rd13+8], %rd12;
	ld.u64 	%rd14, [%SP+16];
	ld.u64 	%rd15, [%SP+24];
	st.u64 	[%rd15], %rd14;
	ld.u64 	%rd16, [%SP+16];
	ld.u64 	%rd17, [%rd16];
	setp.eq.s64 	%p2, %rd17, 0;
	@%p2 bra 	$L__BB42_4;
	bra.uni 	$L__BB42_3;
$L__BB42_3:
	ld.u64 	%rd18, [%SP+16];
	ld.u64 	%rd19, [%rd18];
	st.u64 	[%rd19+8], %rd18;
	bra.uni 	$L__BB42_4;
$L__BB42_4:
	ret;

}
	// .globl	remque
.visible .func remque(
	.param .b64 remque_param_0
)
{
	.local .align 8 .b8 	__local_depot43[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b64 	%rd<13>;

	mov.u64 	%SPL, __local_depot43;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [remque_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+8];
	ld.u64 	%rd4, [%rd3];
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB43_2;
	bra.uni 	$L__BB43_1;
$L__BB43_1:
	ld.u64 	%rd5, [%SP+8];
	ld.u64 	%rd6, [%rd5+8];
	ld.u64 	%rd7, [%rd5];
	st.u64 	[%rd7+8], %rd6;
	bra.uni 	$L__BB43_2;
$L__BB43_2:
	ld.u64 	%rd8, [%SP+8];
	ld.u64 	%rd9, [%rd8+8];
	setp.eq.s64 	%p2, %rd9, 0;
	@%p2 bra 	$L__BB43_4;
	bra.uni 	$L__BB43_3;
$L__BB43_3:
	ld.u64 	%rd10, [%SP+8];
	ld.u64 	%rd11, [%rd10];
	ld.u64 	%rd12, [%rd10+8];
	st.u64 	[%rd12], %rd11;
	bra.uni 	$L__BB43_4;
$L__BB43_4:
	ret;

}
	// .globl	abs
.visible .func  (.param .b32 func_retval0) abs(
	.param .b32 abs_param_0
)
{
	.local .align 4 .b8 	__local_depot44[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;

	mov.u64 	%SPL, __local_depot44;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r4, [abs_param_0];
	st.u32 	[%SP+0], %r4;
	ld.u32 	%r5, [%SP+0];
	setp.lt.s32 	%p1, %r5, 1;
	@%p1 bra 	$L__BB44_2;
	bra.uni 	$L__BB44_1;
$L__BB44_1:
	ld.u32 	%r1, [%SP+0];
	mov.u32 	%r7, %r1;
	bra.uni 	$L__BB44_3;
$L__BB44_2:
	ld.u32 	%r6, [%SP+0];
	neg.s32 	%r2, %r6;
	mov.u32 	%r7, %r2;
	bra.uni 	$L__BB44_3;
$L__BB44_3:
	mov.u32 	%r3, %r7;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	atoi
.visible .func  (.param .b32 func_retval0) atoi(
	.param .b64 atoi_param_0
)
{
	.local .align 8 .b8 	__local_depot45[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<11>;

	mov.u64 	%SPL, __local_depot45;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [atoi_param_0];
	st.u64 	[%SP+0], %rd1;
	mov.b32 	%r4, 0;
	st.u32 	[%SP+8], %r4;
	st.u32 	[%SP+12], %r4;
	bra.uni 	$L__BB45_1;
$L__BB45_1:
	ld.u64 	%rd2, [%SP+0];
	ld.s8 	%r5, [%rd2];
	{ // callseq 0, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r5;
	.param .b32 retval0;
	call.uni (retval0), 
	isspace, 
	(
	param0
	);
	ld.param.b32 	%r6, [retval0+0];
	} // callseq 0
	setp.eq.s32 	%p1, %r6, 0;
	@%p1 bra 	$L__BB45_3;
	bra.uni 	$L__BB45_2;
$L__BB45_2:
	ld.u64 	%rd9, [%SP+0];
	add.s64 	%rd10, %rd9, 1;
	st.u64 	[%SP+0], %rd10;
	bra.uni 	$L__BB45_1;
$L__BB45_3:
	ld.u64 	%rd3, [%SP+0];
	ld.s8 	%r8, [%rd3];
	setp.eq.s32 	%p2, %r8, 43;
	@%p2 bra 	$L__BB45_5;
	bra.uni 	$L__BB45_13;
$L__BB45_13:
	setp.ne.s32 	%p3, %r8, 45;
	@%p3 bra 	$L__BB45_6;
	bra.uni 	$L__BB45_4;
$L__BB45_4:
	mov.b32 	%r9, 1;
	st.u32 	[%SP+12], %r9;
	bra.uni 	$L__BB45_5;
$L__BB45_5:
	ld.u64 	%rd4, [%SP+0];
	add.s64 	%rd5, %rd4, 1;
	st.u64 	[%SP+0], %rd5;
	bra.uni 	$L__BB45_6;
$L__BB45_6:
	bra.uni 	$L__BB45_7;
$L__BB45_7:
	ld.u64 	%rd6, [%SP+0];
	ld.s8 	%r10, [%rd6];
	{ // callseq 1, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r10;
	.param .b32 retval0;
	call.uni (retval0), 
	isdigit, 
	(
	param0
	);
	ld.param.b32 	%r11, [retval0+0];
	} // callseq 1
	setp.eq.s32 	%p4, %r11, 0;
	@%p4 bra 	$L__BB45_9;
	bra.uni 	$L__BB45_8;
$L__BB45_8:
	ld.u32 	%r15, [%SP+8];
	mul.lo.s32 	%r16, %r15, 10;
	ld.u64 	%rd7, [%SP+0];
	add.s64 	%rd8, %rd7, 1;
	st.u64 	[%SP+0], %rd8;
	ld.s8 	%r17, [%rd7];
	sub.s32 	%r18, %r16, %r17;
	add.s32 	%r19, %r18, 48;
	st.u32 	[%SP+8], %r19;
	bra.uni 	$L__BB45_7;
$L__BB45_9:
	ld.u32 	%r13, [%SP+12];
	setp.eq.s32 	%p5, %r13, 0;
	@%p5 bra 	$L__BB45_11;
	bra.uni 	$L__BB45_10;
$L__BB45_10:
	ld.u32 	%r1, [%SP+8];
	mov.u32 	%r20, %r1;
	bra.uni 	$L__BB45_12;
$L__BB45_11:
	ld.u32 	%r14, [%SP+8];
	neg.s32 	%r2, %r14;
	mov.u32 	%r20, %r2;
	bra.uni 	$L__BB45_12;
$L__BB45_12:
	mov.u32 	%r3, %r20;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	atol
.visible .func  (.param .b64 func_retval0) atol(
	.param .b64 atol_param_0
)
{
	.local .align 8 .b8 	__local_depot46[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<21>;

	mov.u64 	%SPL, __local_depot46;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd4, [atol_param_0];
	st.u64 	[%SP+0], %rd4;
	mov.u64 	%rd5, 0;
	st.u64 	[%SP+8], %rd5;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+16], %r1;
	bra.uni 	$L__BB46_1;
$L__BB46_1:
	ld.u64 	%rd6, [%SP+0];
	ld.s8 	%r2, [%rd6];
	{ // callseq 2, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r2;
	.param .b32 retval0;
	call.uni (retval0), 
	isspace, 
	(
	param0
	);
	ld.param.b32 	%r3, [retval0+0];
	} // callseq 2
	setp.eq.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB46_3;
	bra.uni 	$L__BB46_2;
$L__BB46_2:
	ld.u64 	%rd18, [%SP+0];
	add.s64 	%rd19, %rd18, 1;
	st.u64 	[%SP+0], %rd19;
	bra.uni 	$L__BB46_1;
$L__BB46_3:
	ld.u64 	%rd7, [%SP+0];
	ld.s8 	%r5, [%rd7];
	setp.eq.s32 	%p2, %r5, 43;
	@%p2 bra 	$L__BB46_5;
	bra.uni 	$L__BB46_13;
$L__BB46_13:
	setp.ne.s32 	%p3, %r5, 45;
	@%p3 bra 	$L__BB46_6;
	bra.uni 	$L__BB46_4;
$L__BB46_4:
	mov.b32 	%r6, 1;
	st.u32 	[%SP+16], %r6;
	bra.uni 	$L__BB46_5;
$L__BB46_5:
	ld.u64 	%rd8, [%SP+0];
	add.s64 	%rd9, %rd8, 1;
	st.u64 	[%SP+0], %rd9;
	bra.uni 	$L__BB46_6;
$L__BB46_6:
	bra.uni 	$L__BB46_7;
$L__BB46_7:
	ld.u64 	%rd10, [%SP+0];
	ld.s8 	%r7, [%rd10];
	{ // callseq 3, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r7;
	.param .b32 retval0;
	call.uni (retval0), 
	isdigit, 
	(
	param0
	);
	ld.param.b32 	%r8, [retval0+0];
	} // callseq 3
	setp.eq.s32 	%p4, %r8, 0;
	@%p4 bra 	$L__BB46_9;
	bra.uni 	$L__BB46_8;
$L__BB46_8:
	ld.u64 	%rd12, [%SP+8];
	mul.lo.s64 	%rd13, %rd12, 10;
	ld.u64 	%rd14, [%SP+0];
	add.s64 	%rd15, %rd14, 1;
	st.u64 	[%SP+0], %rd15;
	ld.s8 	%r11, [%rd14];
	add.s32 	%r12, %r11, -48;
	cvt.s64.s32 	%rd16, %r12;
	sub.s64 	%rd17, %rd13, %rd16;
	st.u64 	[%SP+8], %rd17;
	bra.uni 	$L__BB46_7;
$L__BB46_9:
	ld.u32 	%r10, [%SP+16];
	setp.eq.s32 	%p5, %r10, 0;
	@%p5 bra 	$L__BB46_11;
	bra.uni 	$L__BB46_10;
$L__BB46_10:
	ld.u64 	%rd1, [%SP+8];
	mov.u64 	%rd20, %rd1;
	bra.uni 	$L__BB46_12;
$L__BB46_11:
	ld.u64 	%rd11, [%SP+8];
	neg.s64 	%rd2, %rd11;
	mov.u64 	%rd20, %rd2;
	bra.uni 	$L__BB46_12;
$L__BB46_12:
	mov.u64 	%rd3, %rd20;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	atoll
.visible .func  (.param .b64 func_retval0) atoll(
	.param .b64 atoll_param_0
)
{
	.local .align 8 .b8 	__local_depot47[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<21>;

	mov.u64 	%SPL, __local_depot47;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd4, [atoll_param_0];
	st.u64 	[%SP+0], %rd4;
	mov.u64 	%rd5, 0;
	st.u64 	[%SP+8], %rd5;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+16], %r1;
	bra.uni 	$L__BB47_1;
$L__BB47_1:
	ld.u64 	%rd6, [%SP+0];
	ld.s8 	%r2, [%rd6];
	{ // callseq 4, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r2;
	.param .b32 retval0;
	call.uni (retval0), 
	isspace, 
	(
	param0
	);
	ld.param.b32 	%r3, [retval0+0];
	} // callseq 4
	setp.eq.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB47_3;
	bra.uni 	$L__BB47_2;
$L__BB47_2:
	ld.u64 	%rd18, [%SP+0];
	add.s64 	%rd19, %rd18, 1;
	st.u64 	[%SP+0], %rd19;
	bra.uni 	$L__BB47_1;
$L__BB47_3:
	ld.u64 	%rd7, [%SP+0];
	ld.s8 	%r5, [%rd7];
	setp.eq.s32 	%p2, %r5, 43;
	@%p2 bra 	$L__BB47_5;
	bra.uni 	$L__BB47_13;
$L__BB47_13:
	setp.ne.s32 	%p3, %r5, 45;
	@%p3 bra 	$L__BB47_6;
	bra.uni 	$L__BB47_4;
$L__BB47_4:
	mov.b32 	%r6, 1;
	st.u32 	[%SP+16], %r6;
	bra.uni 	$L__BB47_5;
$L__BB47_5:
	ld.u64 	%rd8, [%SP+0];
	add.s64 	%rd9, %rd8, 1;
	st.u64 	[%SP+0], %rd9;
	bra.uni 	$L__BB47_6;
$L__BB47_6:
	bra.uni 	$L__BB47_7;
$L__BB47_7:
	ld.u64 	%rd10, [%SP+0];
	ld.s8 	%r7, [%rd10];
	{ // callseq 5, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r7;
	.param .b32 retval0;
	call.uni (retval0), 
	isdigit, 
	(
	param0
	);
	ld.param.b32 	%r8, [retval0+0];
	} // callseq 5
	setp.eq.s32 	%p4, %r8, 0;
	@%p4 bra 	$L__BB47_9;
	bra.uni 	$L__BB47_8;
$L__BB47_8:
	ld.u64 	%rd12, [%SP+8];
	mul.lo.s64 	%rd13, %rd12, 10;
	ld.u64 	%rd14, [%SP+0];
	add.s64 	%rd15, %rd14, 1;
	st.u64 	[%SP+0], %rd15;
	ld.s8 	%r11, [%rd14];
	add.s32 	%r12, %r11, -48;
	cvt.s64.s32 	%rd16, %r12;
	sub.s64 	%rd17, %rd13, %rd16;
	st.u64 	[%SP+8], %rd17;
	bra.uni 	$L__BB47_7;
$L__BB47_9:
	ld.u32 	%r10, [%SP+16];
	setp.eq.s32 	%p5, %r10, 0;
	@%p5 bra 	$L__BB47_11;
	bra.uni 	$L__BB47_10;
$L__BB47_10:
	ld.u64 	%rd1, [%SP+8];
	mov.u64 	%rd20, %rd1;
	bra.uni 	$L__BB47_12;
$L__BB47_11:
	ld.u64 	%rd11, [%SP+8];
	neg.s64 	%rd2, %rd11;
	mov.u64 	%rd20, %rd2;
	bra.uni 	$L__BB47_12;
$L__BB47_12:
	mov.u64 	%rd3, %rd20;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	bsearch
.visible .func  (.param .b64 func_retval0) bsearch(
	.param .b64 bsearch_param_0,
	.param .b64 bsearch_param_1,
	.param .b64 bsearch_param_2,
	.param .b64 bsearch_param_3,
	.param .b64 bsearch_param_4
)
{
	.local .align 8 .b8 	__local_depot48[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<28>;

	mov.u64 	%SPL, __local_depot48;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd5, [bsearch_param_4];
	ld.param.u64 	%rd4, [bsearch_param_3];
	ld.param.u64 	%rd3, [bsearch_param_2];
	ld.param.u64 	%rd2, [bsearch_param_1];
	ld.param.u64 	%rd1, [bsearch_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	st.u64 	[%SP+24], %rd3;
	st.u64 	[%SP+32], %rd4;
	st.u64 	[%SP+40], %rd5;
	bra.uni 	$L__BB48_1;
$L__BB48_1:
	ld.u64 	%rd6, [%SP+24];
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB48_9;
	bra.uni 	$L__BB48_2;
$L__BB48_2:
	ld.u64 	%rd8, [%SP+16];
	ld.u64 	%rd9, [%SP+32];
	ld.u64 	%rd10, [%SP+24];
	shr.u64 	%rd11, %rd10, 1;
	mul.lo.s64 	%rd12, %rd9, %rd11;
	add.s64 	%rd13, %rd8, %rd12;
	st.u64 	[%SP+48], %rd13;
	ld.u64 	%rd14, [%SP+40];
	ld.u64 	%rd15, [%SP+8];
	ld.u64 	%rd16, [%SP+48];
	{ // callseq 6, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd15;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd16;
	.param .b32 retval0;
	prototype_6 : .callprototype (.param .b32 _) _ (.param .b64 _, .param .b64 _);
	call (retval0), 
	%rd14, 
	(
	param0, 
	param1
	)
	, prototype_6;
	ld.param.b32 	%r1, [retval0+0];
	} // callseq 6
	st.u32 	[%SP+56], %r1;
	ld.u32 	%r3, [%SP+56];
	setp.gt.s32 	%p2, %r3, -1;
	@%p2 bra 	$L__BB48_4;
	bra.uni 	$L__BB48_3;
$L__BB48_3:
	ld.u64 	%rd26, [%SP+24];
	shr.u64 	%rd27, %rd26, 1;
	st.u64 	[%SP+24], %rd27;
	bra.uni 	$L__BB48_8;
$L__BB48_4:
	ld.u32 	%r4, [%SP+56];
	setp.lt.s32 	%p3, %r4, 1;
	@%p3 bra 	$L__BB48_6;
	bra.uni 	$L__BB48_5;
$L__BB48_5:
	ld.u64 	%rd19, [%SP+48];
	ld.u64 	%rd20, [%SP+32];
	add.s64 	%rd21, %rd19, %rd20;
	st.u64 	[%SP+16], %rd21;
	ld.u64 	%rd22, [%SP+24];
	shr.u64 	%rd23, %rd22, 1;
	not.b64 	%rd24, %rd23;
	add.s64 	%rd25, %rd24, %rd22;
	st.u64 	[%SP+24], %rd25;
	bra.uni 	$L__BB48_7;
$L__BB48_6:
	ld.u64 	%rd17, [%SP+48];
	st.u64 	[%SP+0], %rd17;
	bra.uni 	$L__BB48_10;
$L__BB48_7:
	bra.uni 	$L__BB48_8;
$L__BB48_8:
	bra.uni 	$L__BB48_1;
$L__BB48_9:
	mov.u64 	%rd7, 0;
	st.u64 	[%SP+0], %rd7;
	bra.uni 	$L__BB48_10;
$L__BB48_10:
	ld.u64 	%rd18, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd18;
	ret;

}
	// .globl	bsearch_r
.visible .func  (.param .b64 func_retval0) bsearch_r(
	.param .b64 bsearch_r_param_0,
	.param .b64 bsearch_r_param_1,
	.param .b64 bsearch_r_param_2,
	.param .b64 bsearch_r_param_3,
	.param .b64 bsearch_r_param_4,
	.param .b64 bsearch_r_param_5
)
{
	.local .align 8 .b8 	__local_depot49[80];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<24>;

	mov.u64 	%SPL, __local_depot49;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd6, [bsearch_r_param_5];
	ld.param.u64 	%rd5, [bsearch_r_param_4];
	ld.param.u64 	%rd4, [bsearch_r_param_3];
	ld.param.u64 	%rd3, [bsearch_r_param_2];
	ld.param.u64 	%rd2, [bsearch_r_param_1];
	ld.param.u64 	%rd1, [bsearch_r_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	st.u64 	[%SP+24], %rd3;
	st.u64 	[%SP+32], %rd4;
	st.u64 	[%SP+40], %rd5;
	st.u64 	[%SP+48], %rd6;
	ld.u64 	%rd7, [%SP+16];
	st.u64 	[%SP+56], %rd7;
	ld.u64 	%rd8, [%SP+24];
	st.u32 	[%SP+64], %rd8;
	bra.uni 	$L__BB49_1;
$L__BB49_1:
	ld.u32 	%r1, [%SP+64];
	setp.eq.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB49_8;
	bra.uni 	$L__BB49_2;
$L__BB49_2:
	ld.u64 	%rd10, [%SP+56];
	ld.u32 	%r2, [%SP+64];
	shr.s32 	%r3, %r2, 1;
	cvt.s64.s32 	%rd11, %r3;
	ld.u64 	%rd12, [%SP+32];
	mul.lo.s64 	%rd13, %rd11, %rd12;
	add.s64 	%rd14, %rd10, %rd13;
	st.u64 	[%SP+72], %rd14;
	ld.u64 	%rd15, [%SP+40];
	ld.u64 	%rd16, [%SP+8];
	ld.u64 	%rd17, [%SP+72];
	ld.u64 	%rd18, [%SP+48];
	{ // callseq 7, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd16;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd17;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd18;
	.param .b32 retval0;
	prototype_7 : .callprototype (.param .b32 _) _ (.param .b64 _, .param .b64 _, .param .b64 _);
	call (retval0), 
	%rd15, 
	(
	param0, 
	param1, 
	param2
	)
	, prototype_7;
	ld.param.b32 	%r4, [retval0+0];
	} // callseq 7
	st.u32 	[%SP+68], %r4;
	ld.u32 	%r6, [%SP+68];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB49_4;
	bra.uni 	$L__BB49_3;
$L__BB49_3:
	ld.u64 	%rd22, [%SP+72];
	st.u64 	[%SP+0], %rd22;
	bra.uni 	$L__BB49_9;
$L__BB49_4:
	ld.u32 	%r7, [%SP+68];
	setp.lt.s32 	%p3, %r7, 1;
	@%p3 bra 	$L__BB49_6;
	bra.uni 	$L__BB49_5;
$L__BB49_5:
	ld.u64 	%rd19, [%SP+72];
	ld.u64 	%rd20, [%SP+32];
	add.s64 	%rd21, %rd19, %rd20;
	st.u64 	[%SP+56], %rd21;
	ld.u32 	%r8, [%SP+64];
	add.s32 	%r9, %r8, -1;
	st.u32 	[%SP+64], %r9;
	bra.uni 	$L__BB49_6;
$L__BB49_6:
	bra.uni 	$L__BB49_7;
$L__BB49_7:
	ld.u32 	%r10, [%SP+64];
	shr.s32 	%r11, %r10, 1;
	st.u32 	[%SP+64], %r11;
	bra.uni 	$L__BB49_1;
$L__BB49_8:
	mov.u64 	%rd9, 0;
	st.u64 	[%SP+0], %rd9;
	bra.uni 	$L__BB49_9;
$L__BB49_9:
	ld.u64 	%rd23, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd23;
	ret;

}
	// .globl	div
.visible .func  (.param .align 4 .b8 func_retval0[8]) div(
	.param .b32 div_param_0,
	.param .b32 div_param_1
)
{
	.local .align 4 .b8 	__local_depot50[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<11>;

	mov.u64 	%SPL, __local_depot50;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [div_param_1];
	ld.param.u32 	%r1, [div_param_0];
	st.u32 	[%SP+8], %r1;
	st.u32 	[%SP+12], %r2;
	ld.u32 	%r3, [%SP+8];
	ld.u32 	%r4, [%SP+12];
	div.s32 	%r5, %r3, %r4;
	st.u32 	[%SP+0], %r5;
	ld.u32 	%r6, [%SP+8];
	ld.u32 	%r7, [%SP+12];
	rem.s32 	%r8, %r6, %r7;
	st.u32 	[%SP+4], %r8;
	ld.u32 	%r9, [%SP+4];
	ld.u32 	%r10, [%SP+0];
	st.param.b32 	[func_retval0+0], %r10;
	st.param.b32 	[func_retval0+4], %r9;
	ret;

}
	// .globl	imaxabs
.visible .func  (.param .b64 func_retval0) imaxabs(
	.param .b64 imaxabs_param_0
)
{
	.local .align 8 .b8 	__local_depot51[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b64 	%rd<8>;

	mov.u64 	%SPL, __local_depot51;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd4, [imaxabs_param_0];
	st.u64 	[%SP+0], %rd4;
	ld.u64 	%rd5, [%SP+0];
	setp.lt.s64 	%p1, %rd5, 1;
	@%p1 bra 	$L__BB51_2;
	bra.uni 	$L__BB51_1;
$L__BB51_1:
	ld.u64 	%rd1, [%SP+0];
	mov.u64 	%rd7, %rd1;
	bra.uni 	$L__BB51_3;
$L__BB51_2:
	ld.u64 	%rd6, [%SP+0];
	neg.s64 	%rd2, %rd6;
	mov.u64 	%rd7, %rd2;
	bra.uni 	$L__BB51_3;
$L__BB51_3:
	mov.u64 	%rd3, %rd7;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	imaxdiv
.visible .func  (.param .align 8 .b8 func_retval0[16]) imaxdiv(
	.param .b64 imaxdiv_param_0,
	.param .b64 imaxdiv_param_1
)
{
	.local .align 8 .b8 	__local_depot52[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<11>;

	mov.u64 	%SPL, __local_depot52;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [imaxdiv_param_1];
	ld.param.u64 	%rd1, [imaxdiv_param_0];
	st.u64 	[%SP+16], %rd1;
	st.u64 	[%SP+24], %rd2;
	ld.u64 	%rd3, [%SP+16];
	ld.u64 	%rd4, [%SP+24];
	div.s64 	%rd5, %rd3, %rd4;
	st.u64 	[%SP+0], %rd5;
	ld.u64 	%rd6, [%SP+16];
	ld.u64 	%rd7, [%SP+24];
	rem.s64 	%rd8, %rd6, %rd7;
	st.u64 	[%SP+8], %rd8;
	ld.u64 	%rd9, [%SP+8];
	ld.u64 	%rd10, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd10;
	st.param.b64 	[func_retval0+8], %rd9;
	ret;

}
	// .globl	labs
.visible .func  (.param .b64 func_retval0) labs(
	.param .b64 labs_param_0
)
{
	.local .align 8 .b8 	__local_depot53[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b64 	%rd<8>;

	mov.u64 	%SPL, __local_depot53;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd4, [labs_param_0];
	st.u64 	[%SP+0], %rd4;
	ld.u64 	%rd5, [%SP+0];
	setp.lt.s64 	%p1, %rd5, 1;
	@%p1 bra 	$L__BB53_2;
	bra.uni 	$L__BB53_1;
$L__BB53_1:
	ld.u64 	%rd1, [%SP+0];
	mov.u64 	%rd7, %rd1;
	bra.uni 	$L__BB53_3;
$L__BB53_2:
	ld.u64 	%rd6, [%SP+0];
	neg.s64 	%rd2, %rd6;
	mov.u64 	%rd7, %rd2;
	bra.uni 	$L__BB53_3;
$L__BB53_3:
	mov.u64 	%rd3, %rd7;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	ldiv
.visible .func  (.param .align 8 .b8 func_retval0[16]) ldiv(
	.param .b64 ldiv_param_0,
	.param .b64 ldiv_param_1
)
{
	.local .align 8 .b8 	__local_depot54[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<11>;

	mov.u64 	%SPL, __local_depot54;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [ldiv_param_1];
	ld.param.u64 	%rd1, [ldiv_param_0];
	st.u64 	[%SP+16], %rd1;
	st.u64 	[%SP+24], %rd2;
	ld.u64 	%rd3, [%SP+16];
	ld.u64 	%rd4, [%SP+24];
	div.s64 	%rd5, %rd3, %rd4;
	st.u64 	[%SP+0], %rd5;
	ld.u64 	%rd6, [%SP+16];
	ld.u64 	%rd7, [%SP+24];
	rem.s64 	%rd8, %rd6, %rd7;
	st.u64 	[%SP+8], %rd8;
	ld.u64 	%rd9, [%SP+8];
	ld.u64 	%rd10, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd10;
	st.param.b64 	[func_retval0+8], %rd9;
	ret;

}
	// .globl	llabs
.visible .func  (.param .b64 func_retval0) llabs(
	.param .b64 llabs_param_0
)
{
	.local .align 8 .b8 	__local_depot55[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b64 	%rd<8>;

	mov.u64 	%SPL, __local_depot55;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd4, [llabs_param_0];
	st.u64 	[%SP+0], %rd4;
	ld.u64 	%rd5, [%SP+0];
	setp.lt.s64 	%p1, %rd5, 1;
	@%p1 bra 	$L__BB55_2;
	bra.uni 	$L__BB55_1;
$L__BB55_1:
	ld.u64 	%rd1, [%SP+0];
	mov.u64 	%rd7, %rd1;
	bra.uni 	$L__BB55_3;
$L__BB55_2:
	ld.u64 	%rd6, [%SP+0];
	neg.s64 	%rd2, %rd6;
	mov.u64 	%rd7, %rd2;
	bra.uni 	$L__BB55_3;
$L__BB55_3:
	mov.u64 	%rd3, %rd7;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	lldiv
.visible .func  (.param .align 8 .b8 func_retval0[16]) lldiv(
	.param .b64 lldiv_param_0,
	.param .b64 lldiv_param_1
)
{
	.local .align 8 .b8 	__local_depot56[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<11>;

	mov.u64 	%SPL, __local_depot56;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [lldiv_param_1];
	ld.param.u64 	%rd1, [lldiv_param_0];
	st.u64 	[%SP+16], %rd1;
	st.u64 	[%SP+24], %rd2;
	ld.u64 	%rd3, [%SP+16];
	ld.u64 	%rd4, [%SP+24];
	div.s64 	%rd5, %rd3, %rd4;
	st.u64 	[%SP+0], %rd5;
	ld.u64 	%rd6, [%SP+16];
	ld.u64 	%rd7, [%SP+24];
	rem.s64 	%rd8, %rd6, %rd7;
	st.u64 	[%SP+8], %rd8;
	ld.u64 	%rd9, [%SP+8];
	ld.u64 	%rd10, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd10;
	st.param.b64 	[func_retval0+8], %rd9;
	ret;

}
	// .globl	wcschr
.visible .func  (.param .b64 func_retval0) wcschr(
	.param .b64 wcschr_param_0,
	.param .b32 wcschr_param_1
)
{
	.local .align 8 .b8 	__local_depot57[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;

	mov.u64 	%SPL, __local_depot57;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [wcschr_param_1];
	ld.param.u64 	%rd3, [wcschr_param_0];
	st.u64 	[%SP+0], %rd3;
	st.u32 	[%SP+8], %r1;
	bra.uni 	$L__BB57_1;
$L__BB57_1:
	ld.u64 	%rd4, [%SP+0];
	ld.u32 	%r2, [%rd4];
	setp.eq.s32 	%p4, %r2, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB57_3;
	bra.uni 	$L__BB57_2;
$L__BB57_2:
	ld.u64 	%rd5, [%SP+0];
	ld.u32 	%r3, [%rd5];
	ld.u32 	%r4, [%SP+8];
	setp.ne.s32 	%p1, %r3, %r4;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB57_3;
$L__BB57_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB57_6;
	bra.uni 	$L__BB57_4;
$L__BB57_4:
	bra.uni 	$L__BB57_5;
$L__BB57_5:
	ld.u64 	%rd8, [%SP+0];
	add.s64 	%rd9, %rd8, 4;
	st.u64 	[%SP+0], %rd9;
	bra.uni 	$L__BB57_1;
$L__BB57_6:
	ld.u64 	%rd6, [%SP+0];
	ld.u32 	%r5, [%rd6];
	setp.eq.s32 	%p5, %r5, 0;
	@%p5 bra 	$L__BB57_8;
	bra.uni 	$L__BB57_7;
$L__BB57_7:
	ld.u64 	%rd1, [%SP+0];
	mov.u64 	%rd10, %rd1;
	bra.uni 	$L__BB57_9;
$L__BB57_8:
	mov.u64 	%rd7, 0;
	mov.u64 	%rd10, %rd7;
	bra.uni 	$L__BB57_9;
$L__BB57_9:
	mov.u64 	%rd2, %rd10;
	st.param.b64 	[func_retval0+0], %rd2;
	ret;

}
	// .globl	wcscmp
.visible .func  (.param .b32 func_retval0) wcscmp(
	.param .b64 wcscmp_param_0,
	.param .b64 wcscmp_param_1
)
{
	.local .align 8 .b8 	__local_depot58[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<15>;

	mov.u64 	%SPL, __local_depot58;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [wcscmp_param_1];
	ld.param.u64 	%rd1, [wcscmp_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	bra.uni 	$L__BB58_1;
$L__BB58_1:
	ld.u64 	%rd3, [%SP+0];
	ld.u32 	%r3, [%rd3];
	ld.u64 	%rd4, [%SP+8];
	ld.u32 	%r4, [%rd4];
	setp.ne.s32 	%p4, %r3, %r4;
	mov.pred 	%p3, 0;
	mov.pred 	%p9, %p3;
	@%p4 bra 	$L__BB58_4;
	bra.uni 	$L__BB58_2;
$L__BB58_2:
	ld.u64 	%rd5, [%SP+0];
	ld.u32 	%r5, [%rd5];
	setp.eq.s32 	%p6, %r5, 0;
	mov.pred 	%p5, 0;
	mov.pred 	%p9, %p5;
	@%p6 bra 	$L__BB58_4;
	bra.uni 	$L__BB58_3;
$L__BB58_3:
	ld.u64 	%rd6, [%SP+8];
	ld.u32 	%r6, [%rd6];
	setp.ne.s32 	%p1, %r6, 0;
	mov.pred 	%p9, %p1;
	bra.uni 	$L__BB58_4;
$L__BB58_4:
	mov.pred 	%p2, %p9;
	@!%p2 bra 	$L__BB58_7;
	bra.uni 	$L__BB58_5;
$L__BB58_5:
	bra.uni 	$L__BB58_6;
$L__BB58_6:
	ld.u64 	%rd11, [%SP+0];
	add.s64 	%rd12, %rd11, 4;
	st.u64 	[%SP+0], %rd12;
	ld.u64 	%rd13, [%SP+8];
	add.s64 	%rd14, %rd13, 4;
	st.u64 	[%SP+8], %rd14;
	bra.uni 	$L__BB58_1;
$L__BB58_7:
	ld.u64 	%rd7, [%SP+0];
	ld.u32 	%r7, [%rd7];
	ld.u64 	%rd8, [%SP+8];
	ld.u32 	%r8, [%rd8];
	setp.ge.s32 	%p7, %r7, %r8;
	@%p7 bra 	$L__BB58_9;
	bra.uni 	$L__BB58_8;
$L__BB58_8:
	mov.b32 	%r11, -1;
	mov.u32 	%r12, %r11;
	bra.uni 	$L__BB58_10;
$L__BB58_9:
	ld.u64 	%rd9, [%SP+0];
	ld.u32 	%r9, [%rd9];
	ld.u64 	%rd10, [%SP+8];
	ld.u32 	%r10, [%rd10];
	setp.gt.s32 	%p8, %r9, %r10;
	selp.u32 	%r1, 1, 0, %p8;
	mov.u32 	%r12, %r1;
	bra.uni 	$L__BB58_10;
$L__BB58_10:
	mov.u32 	%r2, %r12;
	st.param.b32 	[func_retval0+0], %r2;
	ret;

}
	// .globl	wcscpy
.visible .func  (.param .b64 func_retval0) wcscpy(
	.param .b64 wcscpy_param_0,
	.param .b64 wcscpy_param_1
)
{
	.local .align 8 .b8 	__local_depot59[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<9>;

	mov.u64 	%SPL, __local_depot59;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [wcscpy_param_1];
	ld.param.u64 	%rd1, [wcscpy_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	st.u64 	[%SP+16], %rd3;
	bra.uni 	$L__BB59_1;
$L__BB59_1:
	ld.u64 	%rd4, [%SP+8];
	add.s64 	%rd5, %rd4, 4;
	st.u64 	[%SP+8], %rd5;
	ld.u32 	%r1, [%rd4];
	ld.u64 	%rd6, [%SP+0];
	add.s64 	%rd7, %rd6, 4;
	st.u64 	[%SP+0], %rd7;
	st.u32 	[%rd6], %r1;
	setp.eq.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB59_3;
	bra.uni 	$L__BB59_2;
$L__BB59_2:
	bra.uni 	$L__BB59_1;
$L__BB59_3:
	ld.u64 	%rd8, [%SP+16];
	st.param.b64 	[func_retval0+0], %rd8;
	ret;

}
	// .globl	wcslen
.visible .func  (.param .b64 func_retval0) wcslen(
	.param .b64 wcslen_param_0
)
{
	.local .align 8 .b8 	__local_depot60[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<10>;

	mov.u64 	%SPL, __local_depot60;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [wcslen_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	st.u64 	[%SP+8], %rd2;
	bra.uni 	$L__BB60_1;
$L__BB60_1:
	ld.u64 	%rd3, [%SP+0];
	ld.u32 	%r1, [%rd3];
	setp.eq.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB60_4;
	bra.uni 	$L__BB60_2;
$L__BB60_2:
	bra.uni 	$L__BB60_3;
$L__BB60_3:
	ld.u64 	%rd8, [%SP+0];
	add.s64 	%rd9, %rd8, 4;
	st.u64 	[%SP+0], %rd9;
	bra.uni 	$L__BB60_1;
$L__BB60_4:
	ld.u64 	%rd4, [%SP+0];
	ld.u64 	%rd5, [%SP+8];
	sub.s64 	%rd6, %rd4, %rd5;
	shr.s64 	%rd7, %rd6, 2;
	st.param.b64 	[func_retval0+0], %rd7;
	ret;

}
	// .globl	wcsncmp
.visible .func  (.param .b32 func_retval0) wcsncmp(
	.param .b64 wcsncmp_param_0,
	.param .b64 wcsncmp_param_1,
	.param .b64 wcsncmp_param_2
)
{
	.local .align 8 .b8 	__local_depot61[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<13>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<20>;

	mov.u64 	%SPL, __local_depot61;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [wcsncmp_param_2];
	ld.param.u64 	%rd2, [wcsncmp_param_1];
	ld.param.u64 	%rd1, [wcsncmp_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	bra.uni 	$L__BB61_1;
$L__BB61_1:
	ld.u64 	%rd4, [%SP+16];
	setp.eq.s64 	%p4, %rd4, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p12, %p3;
	@%p4 bra 	$L__BB61_5;
	bra.uni 	$L__BB61_2;
$L__BB61_2:
	ld.u64 	%rd5, [%SP+0];
	ld.u32 	%r4, [%rd5];
	ld.u64 	%rd6, [%SP+8];
	ld.u32 	%r5, [%rd6];
	setp.ne.s32 	%p6, %r4, %r5;
	mov.pred 	%p5, 0;
	mov.pred 	%p12, %p5;
	@%p6 bra 	$L__BB61_5;
	bra.uni 	$L__BB61_3;
$L__BB61_3:
	ld.u64 	%rd7, [%SP+0];
	ld.u32 	%r6, [%rd7];
	setp.eq.s32 	%p8, %r6, 0;
	mov.pred 	%p7, 0;
	mov.pred 	%p12, %p7;
	@%p8 bra 	$L__BB61_5;
	bra.uni 	$L__BB61_4;
$L__BB61_4:
	ld.u64 	%rd8, [%SP+8];
	ld.u32 	%r7, [%rd8];
	setp.ne.s32 	%p1, %r7, 0;
	mov.pred 	%p12, %p1;
	bra.uni 	$L__BB61_5;
$L__BB61_5:
	mov.pred 	%p2, %p12;
	@!%p2 bra 	$L__BB61_8;
	bra.uni 	$L__BB61_6;
$L__BB61_6:
	bra.uni 	$L__BB61_7;
$L__BB61_7:
	ld.u64 	%rd14, [%SP+16];
	add.s64 	%rd15, %rd14, -1;
	st.u64 	[%SP+16], %rd15;
	ld.u64 	%rd16, [%SP+0];
	add.s64 	%rd17, %rd16, 4;
	st.u64 	[%SP+0], %rd17;
	ld.u64 	%rd18, [%SP+8];
	add.s64 	%rd19, %rd18, 4;
	st.u64 	[%SP+8], %rd19;
	bra.uni 	$L__BB61_1;
$L__BB61_8:
	ld.u64 	%rd9, [%SP+16];
	setp.eq.s64 	%p9, %rd9, 0;
	@%p9 bra 	$L__BB61_13;
	bra.uni 	$L__BB61_9;
$L__BB61_9:
	ld.u64 	%rd10, [%SP+0];
	ld.u32 	%r9, [%rd10];
	ld.u64 	%rd11, [%SP+8];
	ld.u32 	%r10, [%rd11];
	setp.ge.s32 	%p10, %r9, %r10;
	@%p10 bra 	$L__BB61_11;
	bra.uni 	$L__BB61_10;
$L__BB61_10:
	mov.b32 	%r13, -1;
	mov.u32 	%r14, %r13;
	bra.uni 	$L__BB61_12;
$L__BB61_11:
	ld.u64 	%rd12, [%SP+0];
	ld.u32 	%r11, [%rd12];
	ld.u64 	%rd13, [%SP+8];
	ld.u32 	%r12, [%rd13];
	setp.gt.s32 	%p11, %r11, %r12;
	selp.u32 	%r1, 1, 0, %p11;
	mov.u32 	%r14, %r1;
	bra.uni 	$L__BB61_12;
$L__BB61_12:
	mov.u32 	%r2, %r14;
	mov.u32 	%r15, %r2;
	bra.uni 	$L__BB61_14;
$L__BB61_13:
	mov.b32 	%r8, 0;
	mov.u32 	%r15, %r8;
	bra.uni 	$L__BB61_14;
$L__BB61_14:
	mov.u32 	%r3, %r15;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	wmemchr
.visible .func  (.param .b64 func_retval0) wmemchr(
	.param .b64 wmemchr_param_0,
	.param .b32 wmemchr_param_1,
	.param .b64 wmemchr_param_2
)
{
	.local .align 8 .b8 	__local_depot62[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<14>;

	mov.u64 	%SPL, __local_depot62;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd4, [wmemchr_param_2];
	ld.param.u32 	%r1, [wmemchr_param_1];
	ld.param.u64 	%rd3, [wmemchr_param_0];
	st.u64 	[%SP+0], %rd3;
	st.u32 	[%SP+8], %r1;
	st.u64 	[%SP+16], %rd4;
	bra.uni 	$L__BB62_1;
$L__BB62_1:
	ld.u64 	%rd5, [%SP+16];
	setp.eq.s64 	%p4, %rd5, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB62_3;
	bra.uni 	$L__BB62_2;
$L__BB62_2:
	ld.u64 	%rd6, [%SP+0];
	ld.u32 	%r2, [%rd6];
	ld.u32 	%r3, [%SP+8];
	setp.ne.s32 	%p1, %r2, %r3;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB62_3;
$L__BB62_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB62_6;
	bra.uni 	$L__BB62_4;
$L__BB62_4:
	bra.uni 	$L__BB62_5;
$L__BB62_5:
	ld.u64 	%rd9, [%SP+16];
	add.s64 	%rd10, %rd9, -1;
	st.u64 	[%SP+16], %rd10;
	ld.u64 	%rd11, [%SP+0];
	add.s64 	%rd12, %rd11, 4;
	st.u64 	[%SP+0], %rd12;
	bra.uni 	$L__BB62_1;
$L__BB62_6:
	ld.u64 	%rd7, [%SP+16];
	setp.eq.s64 	%p5, %rd7, 0;
	@%p5 bra 	$L__BB62_8;
	bra.uni 	$L__BB62_7;
$L__BB62_7:
	ld.u64 	%rd1, [%SP+0];
	mov.u64 	%rd13, %rd1;
	bra.uni 	$L__BB62_9;
$L__BB62_8:
	mov.u64 	%rd8, 0;
	mov.u64 	%rd13, %rd8;
	bra.uni 	$L__BB62_9;
$L__BB62_9:
	mov.u64 	%rd2, %rd13;
	st.param.b64 	[func_retval0+0], %rd2;
	ret;

}
	// .globl	wmemcmp
.visible .func  (.param .b32 func_retval0) wmemcmp(
	.param .b64 wmemcmp_param_0,
	.param .b64 wmemcmp_param_1,
	.param .b64 wmemcmp_param_2
)
{
	.local .align 8 .b8 	__local_depot63[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<18>;

	mov.u64 	%SPL, __local_depot63;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [wmemcmp_param_2];
	ld.param.u64 	%rd2, [wmemcmp_param_1];
	ld.param.u64 	%rd1, [wmemcmp_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	bra.uni 	$L__BB63_1;
$L__BB63_1:
	ld.u64 	%rd4, [%SP+16];
	setp.eq.s64 	%p4, %rd4, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p8, %p3;
	@%p4 bra 	$L__BB63_3;
	bra.uni 	$L__BB63_2;
$L__BB63_2:
	ld.u64 	%rd5, [%SP+0];
	ld.u32 	%r4, [%rd5];
	ld.u64 	%rd6, [%SP+8];
	ld.u32 	%r5, [%rd6];
	setp.eq.s32 	%p1, %r4, %r5;
	mov.pred 	%p8, %p1;
	bra.uni 	$L__BB63_3;
$L__BB63_3:
	mov.pred 	%p2, %p8;
	@!%p2 bra 	$L__BB63_6;
	bra.uni 	$L__BB63_4;
$L__BB63_4:
	bra.uni 	$L__BB63_5;
$L__BB63_5:
	ld.u64 	%rd12, [%SP+16];
	add.s64 	%rd13, %rd12, -1;
	st.u64 	[%SP+16], %rd13;
	ld.u64 	%rd14, [%SP+0];
	add.s64 	%rd15, %rd14, 4;
	st.u64 	[%SP+0], %rd15;
	ld.u64 	%rd16, [%SP+8];
	add.s64 	%rd17, %rd16, 4;
	st.u64 	[%SP+8], %rd17;
	bra.uni 	$L__BB63_1;
$L__BB63_6:
	ld.u64 	%rd7, [%SP+16];
	setp.eq.s64 	%p5, %rd7, 0;
	@%p5 bra 	$L__BB63_11;
	bra.uni 	$L__BB63_7;
$L__BB63_7:
	ld.u64 	%rd8, [%SP+0];
	ld.u32 	%r7, [%rd8];
	ld.u64 	%rd9, [%SP+8];
	ld.u32 	%r8, [%rd9];
	setp.ge.s32 	%p6, %r7, %r8;
	@%p6 bra 	$L__BB63_9;
	bra.uni 	$L__BB63_8;
$L__BB63_8:
	mov.b32 	%r11, -1;
	mov.u32 	%r12, %r11;
	bra.uni 	$L__BB63_10;
$L__BB63_9:
	ld.u64 	%rd10, [%SP+0];
	ld.u32 	%r9, [%rd10];
	ld.u64 	%rd11, [%SP+8];
	ld.u32 	%r10, [%rd11];
	setp.gt.s32 	%p7, %r9, %r10;
	selp.u32 	%r1, 1, 0, %p7;
	mov.u32 	%r12, %r1;
	bra.uni 	$L__BB63_10;
$L__BB63_10:
	mov.u32 	%r2, %r12;
	mov.u32 	%r13, %r2;
	bra.uni 	$L__BB63_12;
$L__BB63_11:
	mov.b32 	%r6, 0;
	mov.u32 	%r13, %r6;
	bra.uni 	$L__BB63_12;
$L__BB63_12:
	mov.u32 	%r3, %r13;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	wmemcpy
.visible .func  (.param .b64 func_retval0) wmemcpy(
	.param .b64 wmemcpy_param_0,
	.param .b64 wmemcpy_param_1,
	.param .b64 wmemcpy_param_2
)
{
	.local .align 8 .b8 	__local_depot64[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<12>;

	mov.u64 	%SPL, __local_depot64;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [wmemcpy_param_2];
	ld.param.u64 	%rd2, [wmemcpy_param_1];
	ld.param.u64 	%rd1, [wmemcpy_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+0];
	st.u64 	[%SP+24], %rd4;
	bra.uni 	$L__BB64_1;
$L__BB64_1:
	ld.u64 	%rd5, [%SP+16];
	add.s64 	%rd6, %rd5, -1;
	st.u64 	[%SP+16], %rd6;
	setp.eq.s64 	%p1, %rd5, 0;
	@%p1 bra 	$L__BB64_3;
	bra.uni 	$L__BB64_2;
$L__BB64_2:
	ld.u64 	%rd8, [%SP+8];
	add.s64 	%rd9, %rd8, 4;
	st.u64 	[%SP+8], %rd9;
	ld.u32 	%r1, [%rd8];
	ld.u64 	%rd10, [%SP+0];
	add.s64 	%rd11, %rd10, 4;
	st.u64 	[%SP+0], %rd11;
	st.u32 	[%rd10], %r1;
	bra.uni 	$L__BB64_1;
$L__BB64_3:
	ld.u64 	%rd7, [%SP+24];
	st.param.b64 	[func_retval0+0], %rd7;
	ret;

}
	// .globl	wmemmove
.visible .func  (.param .b64 func_retval0) wmemmove(
	.param .b64 wmemmove_param_0,
	.param .b64 wmemmove_param_1,
	.param .b64 wmemmove_param_2
)
{
	.local .align 8 .b8 	__local_depot65[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<29>;

	mov.u64 	%SPL, __local_depot65;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [wmemmove_param_2];
	ld.param.u64 	%rd2, [wmemmove_param_1];
	ld.param.u64 	%rd1, [wmemmove_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	st.u64 	[%SP+24], %rd3;
	ld.u64 	%rd4, [%SP+8];
	st.u64 	[%SP+32], %rd4;
	ld.u64 	%rd5, [%SP+8];
	ld.u64 	%rd6, [%SP+16];
	setp.ne.s64 	%p1, %rd5, %rd6;
	@%p1 bra 	$L__BB65_2;
	bra.uni 	$L__BB65_1;
$L__BB65_1:
	ld.u64 	%rd27, [%SP+8];
	st.u64 	[%SP+0], %rd27;
	bra.uni 	$L__BB65_12;
$L__BB65_2:
	ld.u64 	%rd7, [%SP+8];
	ld.u64 	%rd8, [%SP+16];
	sub.s64 	%rd9, %rd7, %rd8;
	ld.u64 	%rd10, [%SP+24];
	shl.b64 	%rd11, %rd10, 2;
	setp.ge.u64 	%p2, %rd9, %rd11;
	@%p2 bra 	$L__BB65_7;
	bra.uni 	$L__BB65_3;
$L__BB65_3:
	bra.uni 	$L__BB65_4;
$L__BB65_4:
	ld.u64 	%rd18, [%SP+24];
	add.s64 	%rd19, %rd18, -1;
	st.u64 	[%SP+24], %rd19;
	setp.eq.s64 	%p4, %rd18, 0;
	@%p4 bra 	$L__BB65_6;
	bra.uni 	$L__BB65_5;
$L__BB65_5:
	ld.u64 	%rd21, [%SP+16];
	ld.u64 	%rd22, [%SP+24];
	shl.b64 	%rd23, %rd22, 2;
	add.s64 	%rd24, %rd21, %rd23;
	ld.u32 	%r2, [%rd24];
	ld.u64 	%rd25, [%SP+8];
	add.s64 	%rd26, %rd25, %rd23;
	st.u32 	[%rd26], %r2;
	bra.uni 	$L__BB65_4;
$L__BB65_6:
	bra.uni 	$L__BB65_11;
$L__BB65_7:
	bra.uni 	$L__BB65_8;
$L__BB65_8:
	ld.u64 	%rd12, [%SP+24];
	add.s64 	%rd13, %rd12, -1;
	st.u64 	[%SP+24], %rd13;
	setp.eq.s64 	%p3, %rd12, 0;
	@%p3 bra 	$L__BB65_10;
	bra.uni 	$L__BB65_9;
$L__BB65_9:
	ld.u64 	%rd14, [%SP+16];
	add.s64 	%rd15, %rd14, 4;
	st.u64 	[%SP+16], %rd15;
	ld.u32 	%r1, [%rd14];
	ld.u64 	%rd16, [%SP+8];
	add.s64 	%rd17, %rd16, 4;
	st.u64 	[%SP+8], %rd17;
	st.u32 	[%rd16], %r1;
	bra.uni 	$L__BB65_8;
$L__BB65_10:
	bra.uni 	$L__BB65_11;
$L__BB65_11:
	ld.u64 	%rd20, [%SP+32];
	st.u64 	[%SP+0], %rd20;
	bra.uni 	$L__BB65_12;
$L__BB65_12:
	ld.u64 	%rd28, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd28;
	ret;

}
	// .globl	wmemset
.visible .func  (.param .b64 func_retval0) wmemset(
	.param .b64 wmemset_param_0,
	.param .b32 wmemset_param_1,
	.param .b64 wmemset_param_2
)
{
	.local .align 8 .b8 	__local_depot66[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<9>;

	mov.u64 	%SPL, __local_depot66;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [wmemset_param_2];
	ld.param.u32 	%r1, [wmemset_param_1];
	ld.param.u64 	%rd1, [wmemset_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	st.u64 	[%SP+16], %rd2;
	ld.u64 	%rd3, [%SP+0];
	st.u64 	[%SP+24], %rd3;
	bra.uni 	$L__BB66_1;
$L__BB66_1:
	ld.u64 	%rd4, [%SP+16];
	add.s64 	%rd5, %rd4, -1;
	st.u64 	[%SP+16], %rd5;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB66_3;
	bra.uni 	$L__BB66_2;
$L__BB66_2:
	ld.u32 	%r2, [%SP+8];
	ld.u64 	%rd7, [%SP+0];
	add.s64 	%rd8, %rd7, 4;
	st.u64 	[%SP+0], %rd8;
	st.u32 	[%rd7], %r2;
	bra.uni 	$L__BB66_1;
$L__BB66_3:
	ld.u64 	%rd6, [%SP+24];
	st.param.b64 	[func_retval0+0], %rd6;
	ret;

}
	// .globl	bcopy
.visible .func bcopy(
	.param .b64 bcopy_param_0,
	.param .b64 bcopy_param_1,
	.param .b64 bcopy_param_2
)
{
	.local .align 8 .b8 	__local_depot67[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .b64 	%rd<30>;

	mov.u64 	%SPL, __local_depot67;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [bcopy_param_2];
	ld.param.u64 	%rd2, [bcopy_param_1];
	ld.param.u64 	%rd1, [bcopy_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+0];
	st.u64 	[%SP+24], %rd4;
	ld.u64 	%rd5, [%SP+8];
	st.u64 	[%SP+32], %rd5;
	ld.u64 	%rd6, [%SP+24];
	ld.u64 	%rd7, [%SP+32];
	setp.ge.u64 	%p1, %rd6, %rd7;
	@%p1 bra 	$L__BB67_6;
	bra.uni 	$L__BB67_1;
$L__BB67_1:
	ld.u64 	%rd17, [%SP+16];
	ld.u64 	%rd18, [%SP+24];
	add.s64 	%rd19, %rd18, %rd17;
	st.u64 	[%SP+24], %rd19;
	ld.u64 	%rd20, [%SP+16];
	ld.u64 	%rd21, [%SP+32];
	add.s64 	%rd22, %rd21, %rd20;
	st.u64 	[%SP+32], %rd22;
	bra.uni 	$L__BB67_2;
$L__BB67_2:
	ld.u64 	%rd23, [%SP+16];
	setp.eq.s64 	%p4, %rd23, 0;
	@%p4 bra 	$L__BB67_5;
	bra.uni 	$L__BB67_3;
$L__BB67_3:
	ld.u64 	%rd24, [%SP+24];
	add.s64 	%rd25, %rd24, -1;
	st.u64 	[%SP+24], %rd25;
	ld.u8 	%rs2, [%rd24+-1];
	ld.u64 	%rd26, [%SP+32];
	add.s64 	%rd27, %rd26, -1;
	st.u64 	[%SP+32], %rd27;
	st.u8 	[%rd26+-1], %rs2;
	bra.uni 	$L__BB67_4;
$L__BB67_4:
	ld.u64 	%rd28, [%SP+16];
	add.s64 	%rd29, %rd28, -1;
	st.u64 	[%SP+16], %rd29;
	bra.uni 	$L__BB67_2;
$L__BB67_5:
	bra.uni 	$L__BB67_13;
$L__BB67_6:
	ld.u64 	%rd8, [%SP+24];
	ld.u64 	%rd9, [%SP+32];
	setp.eq.s64 	%p2, %rd8, %rd9;
	@%p2 bra 	$L__BB67_12;
	bra.uni 	$L__BB67_7;
$L__BB67_7:
	bra.uni 	$L__BB67_8;
$L__BB67_8:
	ld.u64 	%rd10, [%SP+16];
	setp.eq.s64 	%p3, %rd10, 0;
	@%p3 bra 	$L__BB67_11;
	bra.uni 	$L__BB67_9;
$L__BB67_9:
	ld.u64 	%rd11, [%SP+24];
	add.s64 	%rd12, %rd11, 1;
	st.u64 	[%SP+24], %rd12;
	ld.u8 	%rs1, [%rd11];
	ld.u64 	%rd13, [%SP+32];
	add.s64 	%rd14, %rd13, 1;
	st.u64 	[%SP+32], %rd14;
	st.u8 	[%rd13], %rs1;
	bra.uni 	$L__BB67_10;
$L__BB67_10:
	ld.u64 	%rd15, [%SP+16];
	add.s64 	%rd16, %rd15, -1;
	st.u64 	[%SP+16], %rd16;
	bra.uni 	$L__BB67_8;
$L__BB67_11:
	bra.uni 	$L__BB67_12;
$L__BB67_12:
	bra.uni 	$L__BB67_13;
$L__BB67_13:
	ret;

}
	// .globl	rotl64
.visible .func  (.param .b64 func_retval0) rotl64(
	.param .b64 rotl64_param_0,
	.param .b32 rotl64_param_1
)
{
	.local .align 8 .b8 	__local_depot68[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<4>;

	mov.u64 	%SPL, __local_depot68;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [rotl64_param_1];
	ld.param.u64 	%rd1, [rotl64_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	ld.u64 	%rd2, [%SP+0];
	ld.u32 	%r2, [%SP+8];
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	and.b32 	%amt2, %r2, 63;
	shl.b64 	%lhs, %rd2, %amt2;
	sub.u32 	%amt2, 64, %amt2;
	shr.b64 	%rhs, %rd2, %amt2;
	add.u64 	%rd3, %lhs, %rhs;
	}
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	rotr64
.visible .func  (.param .b64 func_retval0) rotr64(
	.param .b64 rotr64_param_0,
	.param .b32 rotr64_param_1
)
{
	.local .align 8 .b8 	__local_depot69[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<4>;

	mov.u64 	%SPL, __local_depot69;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [rotr64_param_1];
	ld.param.u64 	%rd1, [rotr64_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	ld.u64 	%rd2, [%SP+0];
	ld.u32 	%r2, [%SP+8];
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	and.b32 	%amt2, %r2, 63;
	shr.b64 	%lhs, %rd2, %amt2;
	sub.u32 	%amt2, 64, %amt2;
	shl.b64 	%rhs, %rd2, %amt2;
	add.u64 	%rd3, %lhs, %rhs;
	}
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	rotl32
.visible .func  (.param .b32 func_retval0) rotl32(
	.param .b32 rotl32_param_0,
	.param .b32 rotl32_param_1
)
{
	.local .align 4 .b8 	__local_depot70[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<6>;

	mov.u64 	%SPL, __local_depot70;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [rotl32_param_1];
	ld.param.u32 	%r1, [rotl32_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	{
	.reg .b32 %lhs;
	.reg .b32 %rhs;
	.reg .b32 %amt2;
	shl.b32 	%lhs, %r3, %r4;
	sub.s32 	%amt2, 32, %r4;
	shr.b32 	%rhs, %r3, %amt2;
	add.u32 	%r5, %lhs, %rhs;
	}
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	rotr32
.visible .func  (.param .b32 func_retval0) rotr32(
	.param .b32 rotr32_param_0,
	.param .b32 rotr32_param_1
)
{
	.local .align 4 .b8 	__local_depot71[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<6>;

	mov.u64 	%SPL, __local_depot71;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [rotr32_param_1];
	ld.param.u32 	%r1, [rotr32_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	{
	.reg .b32 %lhs;
	.reg .b32 %rhs;
	.reg .b32 %amt2;
	shr.b32 	%lhs, %r3, %r4;
	sub.s32 	%amt2, 32, %r4;
	shl.b32 	%rhs, %r3, %amt2;
	add.u32 	%r5, %lhs, %rhs;
	}
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	rotl_sz
.visible .func  (.param .b64 func_retval0) rotl_sz(
	.param .b64 rotl_sz_param_0,
	.param .b32 rotl_sz_param_1
)
{
	.local .align 8 .b8 	__local_depot72[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<4>;

	mov.u64 	%SPL, __local_depot72;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [rotl_sz_param_1];
	ld.param.u64 	%rd1, [rotl_sz_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	ld.u64 	%rd2, [%SP+0];
	ld.u32 	%r2, [%SP+8];
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	and.b32 	%amt2, %r2, 63;
	shl.b64 	%lhs, %rd2, %amt2;
	sub.u32 	%amt2, 64, %amt2;
	shr.b64 	%rhs, %rd2, %amt2;
	add.u64 	%rd3, %lhs, %rhs;
	}
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	rotr_sz
.visible .func  (.param .b64 func_retval0) rotr_sz(
	.param .b64 rotr_sz_param_0,
	.param .b32 rotr_sz_param_1
)
{
	.local .align 8 .b8 	__local_depot73[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<4>;

	mov.u64 	%SPL, __local_depot73;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [rotr_sz_param_1];
	ld.param.u64 	%rd1, [rotr_sz_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	ld.u64 	%rd2, [%SP+0];
	ld.u32 	%r2, [%SP+8];
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	and.b32 	%amt2, %r2, 63;
	shr.b64 	%lhs, %rd2, %amt2;
	sub.u32 	%amt2, 64, %amt2;
	shl.b64 	%rhs, %rd2, %amt2;
	add.u64 	%rd3, %lhs, %rhs;
	}
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	rotl16
.visible .func  (.param .b32 func_retval0) rotl16(
	.param .b32 rotl16_param_0,
	.param .b32 rotl16_param_1
)
{
	.local .align 4 .b8 	__local_depot74[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;

	mov.u64 	%SPL, __local_depot74;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [rotl16_param_1];
	ld.param.u16 	%rs1, [rotl16_param_0];
	st.u16 	[%SP+0], %rs1;
	st.u32 	[%SP+4], %r1;
	ld.u16 	%r2, [%SP+0];
	ld.u32 	%r3, [%SP+4];
	shl.b32 	%r4, %r2, %r3;
	mov.b32 	%r5, 16;
	sub.s32 	%r6, %r5, %r3;
	shr.u32 	%r7, %r2, %r6;
	or.b32  	%r8, %r4, %r7;
	and.b32  	%r9, %r8, 65535;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	rotr16
.visible .func  (.param .b32 func_retval0) rotr16(
	.param .b32 rotr16_param_0,
	.param .b32 rotr16_param_1
)
{
	.local .align 4 .b8 	__local_depot75[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;

	mov.u64 	%SPL, __local_depot75;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [rotr16_param_1];
	ld.param.u16 	%rs1, [rotr16_param_0];
	st.u16 	[%SP+0], %rs1;
	st.u32 	[%SP+4], %r1;
	ld.u16 	%r2, [%SP+0];
	ld.u32 	%r3, [%SP+4];
	shr.u32 	%r4, %r2, %r3;
	mov.b32 	%r5, 16;
	sub.s32 	%r6, %r5, %r3;
	shl.b32 	%r7, %r2, %r6;
	or.b32  	%r8, %r4, %r7;
	and.b32  	%r9, %r8, 65535;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	rotl8
.visible .func  (.param .b32 func_retval0) rotl8(
	.param .b32 rotl8_param_0,
	.param .b32 rotl8_param_1
)
{
	.local .align 4 .b8 	__local_depot76[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;

	mov.u64 	%SPL, __local_depot76;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [rotl8_param_1];
	ld.param.u8 	%rs1, [rotl8_param_0];
	st.u8 	[%SP+0], %rs1;
	st.u32 	[%SP+4], %r1;
	ld.u8 	%r2, [%SP+0];
	ld.u32 	%r3, [%SP+4];
	shl.b32 	%r4, %r2, %r3;
	mov.b32 	%r5, 8;
	sub.s32 	%r6, %r5, %r3;
	shr.u32 	%r7, %r2, %r6;
	or.b32  	%r8, %r4, %r7;
	and.b32  	%r9, %r8, 255;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	rotr8
.visible .func  (.param .b32 func_retval0) rotr8(
	.param .b32 rotr8_param_0,
	.param .b32 rotr8_param_1
)
{
	.local .align 4 .b8 	__local_depot77[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;

	mov.u64 	%SPL, __local_depot77;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [rotr8_param_1];
	ld.param.u8 	%rs1, [rotr8_param_0];
	st.u8 	[%SP+0], %rs1;
	st.u32 	[%SP+4], %r1;
	ld.u8 	%r2, [%SP+0];
	ld.u32 	%r3, [%SP+4];
	shr.u32 	%r4, %r2, %r3;
	mov.b32 	%r5, 8;
	sub.s32 	%r6, %r5, %r3;
	shl.b32 	%r7, %r2, %r6;
	or.b32  	%r8, %r4, %r7;
	and.b32  	%r9, %r8, 255;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	bswap_16
.visible .func  (.param .b32 func_retval0) bswap_16(
	.param .b32 bswap_16_param_0
)
{
	.local .align 2 .b8 	__local_depot78[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<10>;

	mov.u64 	%SPL, __local_depot78;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u16 	%rs1, [bswap_16_param_0];
	st.u16 	[%SP+0], %rs1;
	mov.u16 	%rs2, 255;
	st.u16 	[%SP+2], %rs2;
	ld.u16 	%r1, [%SP+0];
	ld.u16 	%r2, [%SP+2];
	shl.b32 	%r3, %r2, 8;
	and.b32  	%r4, %r1, %r3;
	shr.u32 	%r5, %r4, 8;
	and.b32  	%r6, %r1, %r2;
	shl.b32 	%r7, %r6, 8;
	or.b32  	%r8, %r5, %r7;
	and.b32  	%r9, %r8, 65535;
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	bswap_32
.visible .func  (.param .b32 func_retval0) bswap_32(
	.param .b32 bswap_32_param_0
)
{
	.local .align 4 .b8 	__local_depot79[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<18>;

	mov.u64 	%SPL, __local_depot79;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [bswap_32_param_0];
	st.u32 	[%SP+0], %r1;
	mov.b32 	%r2, 255;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	shl.b32 	%r5, %r4, 24;
	and.b32  	%r6, %r3, %r5;
	shr.u32 	%r7, %r6, 24;
	shl.b32 	%r8, %r4, 16;
	and.b32  	%r9, %r3, %r8;
	shr.u32 	%r10, %r9, 8;
	or.b32  	%r11, %r7, %r10;
	shl.b32 	%r12, %r3, 8;
	and.b32  	%r13, %r8, %r12;
	or.b32  	%r14, %r11, %r13;
	and.b32  	%r15, %r3, %r4;
	shl.b32 	%r16, %r15, 24;
	or.b32  	%r17, %r14, %r16;
	st.param.b32 	[func_retval0+0], %r17;
	ret;

}
	// .globl	bswap_64
.visible .func  (.param .b64 func_retval0) bswap_64(
	.param .b64 bswap_64_param_0
)
{
	.local .align 8 .b8 	__local_depot80[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<32>;

	mov.u64 	%SPL, __local_depot80;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [bswap_64_param_0];
	st.u64 	[%SP+0], %rd1;
	mov.u64 	%rd2, 255;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	ld.u64 	%rd4, [%SP+8];
	shl.b64 	%rd5, %rd4, 56;
	and.b64  	%rd6, %rd3, %rd5;
	shr.u64 	%rd7, %rd6, 56;
	shl.b64 	%rd8, %rd4, 48;
	and.b64  	%rd9, %rd3, %rd8;
	shr.u64 	%rd10, %rd9, 40;
	or.b64  	%rd11, %rd7, %rd10;
	shl.b64 	%rd12, %rd4, 40;
	and.b64  	%rd13, %rd3, %rd12;
	shr.u64 	%rd14, %rd13, 24;
	or.b64  	%rd15, %rd11, %rd14;
	shl.b64 	%rd16, %rd4, 32;
	and.b64  	%rd17, %rd3, %rd16;
	shr.u64 	%rd18, %rd17, 8;
	or.b64  	%rd19, %rd15, %rd18;
	shl.b64 	%rd20, %rd3, 8;
	and.b64  	%rd21, %rd16, %rd20;
	or.b64  	%rd22, %rd19, %rd21;
	shl.b64 	%rd23, %rd3, 24;
	and.b64  	%rd24, %rd12, %rd23;
	or.b64  	%rd25, %rd22, %rd24;
	shl.b64 	%rd26, %rd3, 40;
	and.b64  	%rd27, %rd8, %rd26;
	or.b64  	%rd28, %rd25, %rd27;
	and.b64  	%rd29, %rd3, %rd4;
	shl.b64 	%rd30, %rd29, 56;
	or.b64  	%rd31, %rd28, %rd30;
	st.param.b64 	[func_retval0+0], %rd31;
	ret;

}
	// .globl	ffs
.visible .func  (.param .b32 func_retval0) ffs(
	.param .b32 ffs_param_0
)
{
	.local .align 4 .b8 	__local_depot81[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<14>;

	mov.u64 	%SPL, __local_depot81;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [ffs_param_0];
	st.u32 	[%SP+4], %r1;
	mov.b32 	%r2, 0;
	st.u32 	[%SP+8], %r2;
	bra.uni 	$L__BB81_1;
$L__BB81_1:
	ld.u32 	%r3, [%SP+8];
	setp.gt.u32 	%p1, %r3, 31;
	@%p1 bra 	$L__BB81_6;
	bra.uni 	$L__BB81_2;
$L__BB81_2:
	ld.u32 	%r5, [%SP+4];
	ld.u32 	%r6, [%SP+8];
	shr.u32 	%r7, %r5, %r6;
	and.b32  	%r8, %r7, 1;
	setp.eq.b32 	%p2, %r8, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB81_4;
	bra.uni 	$L__BB81_3;
$L__BB81_3:
	ld.u32 	%r11, [%SP+8];
	add.s32 	%r12, %r11, 1;
	st.u32 	[%SP+0], %r12;
	bra.uni 	$L__BB81_7;
$L__BB81_4:
	bra.uni 	$L__BB81_5;
$L__BB81_5:
	ld.u32 	%r9, [%SP+8];
	add.s32 	%r10, %r9, 1;
	st.u32 	[%SP+8], %r10;
	bra.uni 	$L__BB81_1;
$L__BB81_6:
	mov.b32 	%r4, 0;
	st.u32 	[%SP+0], %r4;
	bra.uni 	$L__BB81_7;
$L__BB81_7:
	ld.u32 	%r13, [%SP+0];
	st.param.b32 	[func_retval0+0], %r13;
	ret;

}
	// .globl	libiberty_ffs
.visible .func  (.param .b32 func_retval0) libiberty_ffs(
	.param .b32 libiberty_ffs_param_0
)
{
	.local .align 4 .b8 	__local_depot82[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<13>;

	mov.u64 	%SPL, __local_depot82;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [libiberty_ffs_param_0];
	st.u32 	[%SP+4], %r1;
	ld.u32 	%r2, [%SP+4];
	setp.ne.s32 	%p1, %r2, 0;
	@%p1 bra 	$L__BB82_2;
	bra.uni 	$L__BB82_1;
$L__BB82_1:
	mov.b32 	%r11, 0;
	st.u32 	[%SP+0], %r11;
	bra.uni 	$L__BB82_7;
$L__BB82_2:
	mov.b32 	%r3, 1;
	st.u32 	[%SP+8], %r3;
	bra.uni 	$L__BB82_3;
$L__BB82_3:
	ld.u32 	%r4, [%SP+4];
	and.b32  	%r5, %r4, 1;
	setp.eq.b32 	%p2, %r5, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB82_6;
	bra.uni 	$L__BB82_4;
$L__BB82_4:
	ld.u32 	%r7, [%SP+4];
	shr.s32 	%r8, %r7, 1;
	st.u32 	[%SP+4], %r8;
	bra.uni 	$L__BB82_5;
$L__BB82_5:
	ld.u32 	%r9, [%SP+8];
	add.s32 	%r10, %r9, 1;
	st.u32 	[%SP+8], %r10;
	bra.uni 	$L__BB82_3;
$L__BB82_6:
	ld.u32 	%r6, [%SP+8];
	st.u32 	[%SP+0], %r6;
	bra.uni 	$L__BB82_7;
$L__BB82_7:
	ld.u32 	%r12, [%SP+0];
	st.param.b32 	[func_retval0+0], %r12;
	ret;

}
	// .globl	gl_isinff
.visible .func  (.param .b32 func_retval0) gl_isinff(
	.param .b32 gl_isinff_param_0
)
{
	.local .align 4 .b8 	__local_depot83[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<2>;
	.reg .f32 	%f<4>;

	mov.u64 	%SPL, __local_depot83;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f1, [gl_isinff_param_0];
	st.f32 	[%SP+0], %f1;
	ld.f32 	%f2, [%SP+0];
	setp.lt.f32 	%p4, %f2, 0fFF7FFFFF;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB83_2;
	bra.uni 	$L__BB83_1;
$L__BB83_1:
	ld.f32 	%f3, [%SP+0];
	setp.gt.f32 	%p1, %f3, 0f7F7FFFFF;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB83_2;
$L__BB83_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r1, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r1;
	ret;

}
	// .globl	gl_isinfd
.visible .func  (.param .b32 func_retval0) gl_isinfd(
	.param .b64 gl_isinfd_param_0
)
{
	.local .align 8 .b8 	__local_depot84[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<2>;
	.reg .f64 	%fd<4>;

	mov.u64 	%SPL, __local_depot84;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd1, [gl_isinfd_param_0];
	st.f64 	[%SP+0], %fd1;
	ld.f64 	%fd2, [%SP+0];
	setp.lt.f64 	%p4, %fd2, 0dFFEFFFFFFFFFFFFF;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB84_2;
	bra.uni 	$L__BB84_1;
$L__BB84_1:
	ld.f64 	%fd3, [%SP+0];
	setp.gt.f64 	%p1, %fd3, 0d7FEFFFFFFFFFFFFF;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB84_2;
$L__BB84_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r1, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r1;
	ret;

}
	// .globl	gl_isinfl
.visible .func  (.param .b32 func_retval0) gl_isinfl(
	.param .b64 gl_isinfl_param_0
)
{
	.local .align 8 .b8 	__local_depot85[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<2>;
	.reg .f64 	%fd<4>;

	mov.u64 	%SPL, __local_depot85;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd1, [gl_isinfl_param_0];
	st.f64 	[%SP+0], %fd1;
	ld.f64 	%fd2, [%SP+0];
	setp.lt.f64 	%p4, %fd2, 0dFFEFFFFFFFFFFFFF;
	mov.pred 	%p3, -1;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB85_2;
	bra.uni 	$L__BB85_1;
$L__BB85_1:
	ld.f64 	%fd3, [%SP+0];
	setp.gt.f64 	%p1, %fd3, 0d7FEFFFFFFFFFFFFF;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB85_2;
$L__BB85_2:
	mov.pred 	%p2, %p5;
	selp.u32 	%r1, 1, 0, %p2;
	st.param.b32 	[func_retval0+0], %r1;
	ret;

}
	// .globl	_Qp_itoq
.visible .func _Qp_itoq(
	.param .b64 _Qp_itoq_param_0,
	.param .b32 _Qp_itoq_param_1
)
{
	.local .align 8 .b8 	__local_depot86[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<3>;
	.reg .f64 	%fd<2>;

	mov.u64 	%SPL, __local_depot86;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [_Qp_itoq_param_1];
	ld.param.u64 	%rd1, [_Qp_itoq_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	ld.u32 	%r2, [%SP+8];
	cvt.rn.f64.s32 	%fd1, %r2;
	ld.u64 	%rd2, [%SP+0];
	st.f64 	[%rd2], %fd1;
	ret;

}
	// .globl	ldexpf
.visible .func  (.param .b32 func_retval0) ldexpf(
	.param .b32 ldexpf_param_0,
	.param .b32 ldexpf_param_1
)
{
	.local .align 4 .b8 	__local_depot87[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<12>;

	mov.u64 	%SPL, __local_depot87;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [ldexpf_param_1];
	ld.param.f32 	%f1, [ldexpf_param_0];
	st.f32 	[%SP+0], %f1;
	st.u32 	[%SP+4], %r1;
	ld.f32 	%f2, [%SP+0];
	setp.nan.f32 	%p1, %f2, %f2;
	@%p1 bra 	$L__BB87_9;
	bra.uni 	$L__BB87_1;
$L__BB87_1:
	ld.f32 	%f3, [%SP+0];
	add.rn.f32 	%f4, %f3, %f3;
	setp.eq.f32 	%p2, %f4, %f3;
	@%p2 bra 	$L__BB87_9;
	bra.uni 	$L__BB87_2;
$L__BB87_2:
	ld.u32 	%r2, [%SP+4];
	setp.lt.s32 	%p3, %r2, 0;
	selp.f32 	%f5, 0f3F000000, 0f40000000, %p3;
	st.f32 	[%SP+8], %f5;
	bra.uni 	$L__BB87_3;
$L__BB87_3:
	ld.u32 	%r3, [%SP+4];
	shr.u32 	%r4, %r3, 31;
	add.s32 	%r5, %r3, %r4;
	and.b32  	%r6, %r5, -2;
	sub.s32 	%r7, %r3, %r6;
	setp.eq.s32 	%p4, %r7, 0;
	@%p4 bra 	$L__BB87_5;
	bra.uni 	$L__BB87_4;
$L__BB87_4:
	ld.f32 	%f6, [%SP+8];
	ld.f32 	%f7, [%SP+0];
	mul.rn.f32 	%f8, %f7, %f6;
	st.f32 	[%SP+0], %f8;
	bra.uni 	$L__BB87_5;
$L__BB87_5:
	ld.u32 	%r8, [%SP+4];
	shr.u32 	%r9, %r8, 31;
	add.s32 	%r10, %r8, %r9;
	shr.s32 	%r11, %r10, 1;
	st.u32 	[%SP+4], %r11;
	ld.u32 	%r12, [%SP+4];
	setp.ne.s32 	%p5, %r12, 0;
	@%p5 bra 	$L__BB87_7;
	bra.uni 	$L__BB87_6;
$L__BB87_6:
	bra.uni 	$L__BB87_8;
$L__BB87_7:
	ld.f32 	%f9, [%SP+8];
	mul.rn.f32 	%f10, %f9, %f9;
	st.f32 	[%SP+8], %f10;
	bra.uni 	$L__BB87_3;
$L__BB87_8:
	bra.uni 	$L__BB87_9;
$L__BB87_9:
	ld.f32 	%f11, [%SP+0];
	st.param.f32 	[func_retval0+0], %f11;
	ret;

}
	// .globl	ldexp
.visible .func  (.param .b64 func_retval0) ldexp(
	.param .b64 ldexp_param_0,
	.param .b32 ldexp_param_1
)
{
	.local .align 8 .b8 	__local_depot88[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<12>;

	mov.u64 	%SPL, __local_depot88;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [ldexp_param_1];
	ld.param.f64 	%fd1, [ldexp_param_0];
	st.f64 	[%SP+0], %fd1;
	st.u32 	[%SP+8], %r1;
	ld.f64 	%fd2, [%SP+0];
	setp.nan.f64 	%p1, %fd2, %fd2;
	@%p1 bra 	$L__BB88_9;
	bra.uni 	$L__BB88_1;
$L__BB88_1:
	ld.f64 	%fd3, [%SP+0];
	add.rn.f64 	%fd4, %fd3, %fd3;
	setp.eq.f64 	%p2, %fd4, %fd3;
	@%p2 bra 	$L__BB88_9;
	bra.uni 	$L__BB88_2;
$L__BB88_2:
	ld.u32 	%r2, [%SP+8];
	setp.lt.s32 	%p3, %r2, 0;
	selp.f64 	%fd5, 0d3FE0000000000000, 0d4000000000000000, %p3;
	st.f64 	[%SP+16], %fd5;
	bra.uni 	$L__BB88_3;
$L__BB88_3:
	ld.u32 	%r3, [%SP+8];
	shr.u32 	%r4, %r3, 31;
	add.s32 	%r5, %r3, %r4;
	and.b32  	%r6, %r5, -2;
	sub.s32 	%r7, %r3, %r6;
	setp.eq.s32 	%p4, %r7, 0;
	@%p4 bra 	$L__BB88_5;
	bra.uni 	$L__BB88_4;
$L__BB88_4:
	ld.f64 	%fd6, [%SP+16];
	ld.f64 	%fd7, [%SP+0];
	mul.rn.f64 	%fd8, %fd7, %fd6;
	st.f64 	[%SP+0], %fd8;
	bra.uni 	$L__BB88_5;
$L__BB88_5:
	ld.u32 	%r8, [%SP+8];
	shr.u32 	%r9, %r8, 31;
	add.s32 	%r10, %r8, %r9;
	shr.s32 	%r11, %r10, 1;
	st.u32 	[%SP+8], %r11;
	ld.u32 	%r12, [%SP+8];
	setp.ne.s32 	%p5, %r12, 0;
	@%p5 bra 	$L__BB88_7;
	bra.uni 	$L__BB88_6;
$L__BB88_6:
	bra.uni 	$L__BB88_8;
$L__BB88_7:
	ld.f64 	%fd9, [%SP+16];
	mul.rn.f64 	%fd10, %fd9, %fd9;
	st.f64 	[%SP+16], %fd10;
	bra.uni 	$L__BB88_3;
$L__BB88_8:
	bra.uni 	$L__BB88_9;
$L__BB88_9:
	ld.f64 	%fd11, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd11;
	ret;

}
	// .globl	ldexpl
.visible .func  (.param .b64 func_retval0) ldexpl(
	.param .b64 ldexpl_param_0,
	.param .b32 ldexpl_param_1
)
{
	.local .align 8 .b8 	__local_depot89[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<12>;

	mov.u64 	%SPL, __local_depot89;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [ldexpl_param_1];
	ld.param.f64 	%fd1, [ldexpl_param_0];
	st.f64 	[%SP+0], %fd1;
	st.u32 	[%SP+8], %r1;
	ld.f64 	%fd2, [%SP+0];
	setp.nan.f64 	%p1, %fd2, %fd2;
	@%p1 bra 	$L__BB89_9;
	bra.uni 	$L__BB89_1;
$L__BB89_1:
	ld.f64 	%fd3, [%SP+0];
	add.rn.f64 	%fd4, %fd3, %fd3;
	setp.eq.f64 	%p2, %fd4, %fd3;
	@%p2 bra 	$L__BB89_9;
	bra.uni 	$L__BB89_2;
$L__BB89_2:
	ld.u32 	%r2, [%SP+8];
	setp.lt.s32 	%p3, %r2, 0;
	selp.f64 	%fd5, 0d3FE0000000000000, 0d4000000000000000, %p3;
	st.f64 	[%SP+16], %fd5;
	bra.uni 	$L__BB89_3;
$L__BB89_3:
	ld.u32 	%r3, [%SP+8];
	shr.u32 	%r4, %r3, 31;
	add.s32 	%r5, %r3, %r4;
	and.b32  	%r6, %r5, -2;
	sub.s32 	%r7, %r3, %r6;
	setp.eq.s32 	%p4, %r7, 0;
	@%p4 bra 	$L__BB89_5;
	bra.uni 	$L__BB89_4;
$L__BB89_4:
	ld.f64 	%fd6, [%SP+16];
	ld.f64 	%fd7, [%SP+0];
	mul.rn.f64 	%fd8, %fd7, %fd6;
	st.f64 	[%SP+0], %fd8;
	bra.uni 	$L__BB89_5;
$L__BB89_5:
	ld.u32 	%r8, [%SP+8];
	shr.u32 	%r9, %r8, 31;
	add.s32 	%r10, %r8, %r9;
	shr.s32 	%r11, %r10, 1;
	st.u32 	[%SP+8], %r11;
	ld.u32 	%r12, [%SP+8];
	setp.ne.s32 	%p5, %r12, 0;
	@%p5 bra 	$L__BB89_7;
	bra.uni 	$L__BB89_6;
$L__BB89_6:
	bra.uni 	$L__BB89_8;
$L__BB89_7:
	ld.f64 	%fd9, [%SP+16];
	mul.rn.f64 	%fd10, %fd9, %fd9;
	st.f64 	[%SP+16], %fd10;
	bra.uni 	$L__BB89_3;
$L__BB89_8:
	bra.uni 	$L__BB89_9;
$L__BB89_9:
	ld.f64 	%fd11, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd11;
	ret;

}
	// .globl	memxor
.visible .func  (.param .b64 func_retval0) memxor(
	.param .b64 memxor_param_0,
	.param .b64 memxor_param_1,
	.param .b64 memxor_param_2
)
{
	.local .align 8 .b8 	__local_depot90[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<14>;

	mov.u64 	%SPL, __local_depot90;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [memxor_param_2];
	ld.param.u64 	%rd2, [memxor_param_1];
	ld.param.u64 	%rd1, [memxor_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+8];
	st.u64 	[%SP+24], %rd4;
	ld.u64 	%rd5, [%SP+0];
	st.u64 	[%SP+32], %rd5;
	bra.uni 	$L__BB90_1;
$L__BB90_1:
	ld.u64 	%rd6, [%SP+16];
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB90_4;
	bra.uni 	$L__BB90_2;
$L__BB90_2:
	ld.u64 	%rd8, [%SP+24];
	add.s64 	%rd9, %rd8, 1;
	st.u64 	[%SP+24], %rd9;
	ld.u8 	%r1, [%rd8];
	ld.u64 	%rd10, [%SP+32];
	add.s64 	%rd11, %rd10, 1;
	st.u64 	[%SP+32], %rd11;
	ld.u8 	%r2, [%rd10];
	xor.b32  	%r3, %r2, %r1;
	st.u8 	[%rd10], %r3;
	bra.uni 	$L__BB90_3;
$L__BB90_3:
	ld.u64 	%rd12, [%SP+16];
	add.s64 	%rd13, %rd12, -1;
	st.u64 	[%SP+16], %rd13;
	bra.uni 	$L__BB90_1;
$L__BB90_4:
	ld.u64 	%rd7, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd7;
	ret;

}
	// .globl	strncat
.visible .func  (.param .b64 func_retval0) strncat(
	.param .b64 strncat_param_0,
	.param .b64 strncat_param_1,
	.param .b64 strncat_param_2
)
{
	.local .align 8 .b8 	__local_depot91[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<20>;

	mov.u64 	%SPL, __local_depot91;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [strncat_param_2];
	ld.param.u64 	%rd2, [strncat_param_1];
	ld.param.u64 	%rd1, [strncat_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+0];
	{ // callseq 8, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd4;
	.param .b64 retval0;
	call.uni (retval0), 
	strlen, 
	(
	param0
	);
	ld.param.b64 	%rd5, [retval0+0];
	} // callseq 8
	add.s64 	%rd7, %rd4, %rd5;
	st.u64 	[%SP+24], %rd7;
	bra.uni 	$L__BB91_1;
$L__BB91_1:
	ld.u64 	%rd8, [%SP+16];
	setp.eq.s64 	%p4, %rd8, 0;
	mov.pred 	%p3, 0;
	mov.pred 	%p6, %p3;
	@%p4 bra 	$L__BB91_3;
	bra.uni 	$L__BB91_2;
$L__BB91_2:
	ld.u64 	%rd9, [%SP+8];
	ld.u8 	%rs1, [%rd9];
	ld.u64 	%rd10, [%SP+24];
	st.u8 	[%rd10], %rs1;
	cvt.u32.u16 	%r1, %rs1;
	and.b32  	%r2, %r1, 255;
	setp.ne.s32 	%p1, %r2, 0;
	mov.pred 	%p6, %p1;
	bra.uni 	$L__BB91_3;
$L__BB91_3:
	mov.pred 	%p2, %p6;
	@!%p2 bra 	$L__BB91_6;
	bra.uni 	$L__BB91_4;
$L__BB91_4:
	bra.uni 	$L__BB91_5;
$L__BB91_5:
	ld.u64 	%rd14, [%SP+8];
	add.s64 	%rd15, %rd14, 1;
	st.u64 	[%SP+8], %rd15;
	ld.u64 	%rd16, [%SP+24];
	add.s64 	%rd17, %rd16, 1;
	st.u64 	[%SP+24], %rd17;
	ld.u64 	%rd18, [%SP+16];
	add.s64 	%rd19, %rd18, -1;
	st.u64 	[%SP+16], %rd19;
	bra.uni 	$L__BB91_1;
$L__BB91_6:
	ld.u64 	%rd11, [%SP+16];
	setp.ne.s64 	%p5, %rd11, 0;
	@%p5 bra 	$L__BB91_8;
	bra.uni 	$L__BB91_7;
$L__BB91_7:
	ld.u64 	%rd12, [%SP+24];
	mov.u16 	%rs2, 0;
	st.u8 	[%rd12], %rs2;
	bra.uni 	$L__BB91_8;
$L__BB91_8:
	ld.u64 	%rd13, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd13;
	ret;

}
	// .globl	strnlen
.visible .func  (.param .b64 func_retval0) strnlen(
	.param .b64 strnlen_param_0,
	.param .b64 strnlen_param_1
)
{
	.local .align 8 .b8 	__local_depot92[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<12>;

	mov.u64 	%SPL, __local_depot92;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [strnlen_param_1];
	ld.param.u64 	%rd1, [strnlen_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	mov.u64 	%rd3, 0;
	st.u64 	[%SP+16], %rd3;
	bra.uni 	$L__BB92_1;
$L__BB92_1:
	ld.u64 	%rd4, [%SP+16];
	ld.u64 	%rd5, [%SP+8];
	setp.ge.u64 	%p4, %rd4, %rd5;
	mov.pred 	%p3, 0;
	mov.pred 	%p5, %p3;
	@%p4 bra 	$L__BB92_3;
	bra.uni 	$L__BB92_2;
$L__BB92_2:
	ld.u64 	%rd6, [%SP+0];
	ld.u64 	%rd7, [%SP+16];
	add.s64 	%rd8, %rd6, %rd7;
	ld.s8 	%r1, [%rd8];
	setp.ne.s32 	%p1, %r1, 0;
	mov.pred 	%p5, %p1;
	bra.uni 	$L__BB92_3;
$L__BB92_3:
	mov.pred 	%p2, %p5;
	@!%p2 bra 	$L__BB92_6;
	bra.uni 	$L__BB92_4;
$L__BB92_4:
	bra.uni 	$L__BB92_5;
$L__BB92_5:
	ld.u64 	%rd10, [%SP+16];
	add.s64 	%rd11, %rd10, 1;
	st.u64 	[%SP+16], %rd11;
	bra.uni 	$L__BB92_1;
$L__BB92_6:
	ld.u64 	%rd9, [%SP+16];
	st.param.b64 	[func_retval0+0], %rd9;
	ret;

}
	// .globl	strpbrk
.visible .func  (.param .b64 func_retval0) strpbrk(
	.param .b64 strpbrk_param_0,
	.param .b64 strpbrk_param_1
)
{
	.local .align 8 .b8 	__local_depot93[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;

	mov.u64 	%SPL, __local_depot93;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [strpbrk_param_1];
	ld.param.u64 	%rd1, [strpbrk_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	bra.uni 	$L__BB93_1;
$L__BB93_1:
	ld.u64 	%rd3, [%SP+8];
	ld.s8 	%r1, [%rd3];
	setp.eq.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB93_8;
	bra.uni 	$L__BB93_2;
$L__BB93_2:
	ld.u64 	%rd5, [%SP+16];
	st.u64 	[%SP+24], %rd5;
	bra.uni 	$L__BB93_3;
$L__BB93_3:
	ld.u64 	%rd6, [%SP+24];
	ld.s8 	%r2, [%rd6];
	setp.eq.s32 	%p2, %r2, 0;
	@%p2 bra 	$L__BB93_7;
	bra.uni 	$L__BB93_4;
$L__BB93_4:
	ld.u64 	%rd9, [%SP+24];
	add.s64 	%rd10, %rd9, 1;
	st.u64 	[%SP+24], %rd10;
	ld.s8 	%r3, [%rd9];
	ld.u64 	%rd11, [%SP+8];
	ld.s8 	%r4, [%rd11];
	setp.ne.s32 	%p3, %r3, %r4;
	@%p3 bra 	$L__BB93_6;
	bra.uni 	$L__BB93_5;
$L__BB93_5:
	ld.u64 	%rd12, [%SP+8];
	st.u64 	[%SP+0], %rd12;
	bra.uni 	$L__BB93_9;
$L__BB93_6:
	bra.uni 	$L__BB93_3;
$L__BB93_7:
	ld.u64 	%rd7, [%SP+8];
	add.s64 	%rd8, %rd7, 1;
	st.u64 	[%SP+8], %rd8;
	bra.uni 	$L__BB93_1;
$L__BB93_8:
	mov.u64 	%rd4, 0;
	st.u64 	[%SP+0], %rd4;
	bra.uni 	$L__BB93_9;
$L__BB93_9:
	ld.u64 	%rd13, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd13;
	ret;

}
	// .globl	strrchr
.visible .func  (.param .b64 func_retval0) strrchr(
	.param .b64 strrchr_param_0,
	.param .b32 strrchr_param_1
)
{
	.local .align 8 .b8 	__local_depot94[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<8>;

	mov.u64 	%SPL, __local_depot94;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [strrchr_param_1];
	ld.param.u64 	%rd1, [strrchr_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	mov.u64 	%rd2, 0;
	st.u64 	[%SP+16], %rd2;
	bra.uni 	$L__BB94_1;
$L__BB94_1:
	ld.u64 	%rd3, [%SP+0];
	ld.s8 	%r2, [%rd3];
	ld.u32 	%r3, [%SP+8];
	setp.ne.s32 	%p1, %r2, %r3;
	@%p1 bra 	$L__BB94_3;
	bra.uni 	$L__BB94_2;
$L__BB94_2:
	ld.u64 	%rd4, [%SP+0];
	st.u64 	[%SP+16], %rd4;
	bra.uni 	$L__BB94_3;
$L__BB94_3:
	bra.uni 	$L__BB94_4;
$L__BB94_4:
	ld.u64 	%rd5, [%SP+0];
	add.s64 	%rd6, %rd5, 1;
	st.u64 	[%SP+0], %rd6;
	ld.u8 	%rs1, [%rd5];
	setp.ne.s16 	%p2, %rs1, 0;
	@%p2 bra 	$L__BB94_1;
	bra.uni 	$L__BB94_5;
$L__BB94_5:
	ld.u64 	%rd7, [%SP+16];
	st.param.b64 	[func_retval0+0], %rd7;
	ret;

}
	// .globl	strstr
.visible .func  (.param .b64 func_retval0) strstr(
	.param .b64 strstr_param_0,
	.param .b64 strstr_param_1
)
{
	.local .align 8 .b8 	__local_depot95[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<21>;

	mov.u64 	%SPL, __local_depot95;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [strstr_param_1];
	ld.param.u64 	%rd1, [strstr_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	ld.u64 	%rd3, [%SP+8];
	st.u64 	[%SP+24], %rd3;
	ld.u64 	%rd4, [%SP+16];
	{ // callseq 9, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd4;
	.param .b64 retval0;
	call.uni (retval0), 
	strlen, 
	(
	param0
	);
	ld.param.b64 	%rd5, [retval0+0];
	} // callseq 9
	st.u64 	[%SP+32], %rd5;
	ld.u64 	%rd7, [%SP+32];
	setp.ne.s64 	%p1, %rd7, 0;
	@%p1 bra 	$L__BB95_2;
	bra.uni 	$L__BB95_1;
$L__BB95_1:
	ld.u64 	%rd8, [%SP+8];
	st.u64 	[%SP+0], %rd8;
	bra.uni 	$L__BB95_9;
$L__BB95_2:
	bra.uni 	$L__BB95_3;
$L__BB95_3:
	ld.u64 	%rd9, [%SP+24];
	ld.u64 	%rd10, [%SP+16];
	ld.s8 	%r1, [%rd10];
	{ // callseq 10, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd9;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r1;
	.param .b64 retval0;
	call.uni (retval0), 
	strchr, 
	(
	param0, 
	param1
	);
	ld.param.b64 	%rd11, [retval0+0];
	} // callseq 10
	st.u64 	[%SP+24], %rd11;
	setp.eq.s64 	%p2, %rd11, 0;
	@%p2 bra 	$L__BB95_8;
	bra.uni 	$L__BB95_4;
$L__BB95_4:
	ld.u64 	%rd14, [%SP+24];
	ld.u64 	%rd15, [%SP+16];
	ld.u64 	%rd16, [%SP+32];
	{ // callseq 11, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd14;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd15;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd16;
	.param .b32 retval0;
	call.uni (retval0), 
	strncmp, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r2, [retval0+0];
	} // callseq 11
	setp.ne.s32 	%p3, %r2, 0;
	@%p3 bra 	$L__BB95_6;
	bra.uni 	$L__BB95_5;
$L__BB95_5:
	ld.u64 	%rd19, [%SP+24];
	st.u64 	[%SP+0], %rd19;
	bra.uni 	$L__BB95_9;
$L__BB95_6:
	bra.uni 	$L__BB95_7;
$L__BB95_7:
	ld.u64 	%rd17, [%SP+24];
	add.s64 	%rd18, %rd17, 1;
	st.u64 	[%SP+24], %rd18;
	bra.uni 	$L__BB95_3;
$L__BB95_8:
	mov.u64 	%rd13, 0;
	st.u64 	[%SP+0], %rd13;
	bra.uni 	$L__BB95_9;
$L__BB95_9:
	ld.u64 	%rd20, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd20;
	ret;

}
	// .globl	copysign
.visible .func  (.param .b64 func_retval0) copysign(
	.param .b64 copysign_param_0,
	.param .b64 copysign_param_1
)
{
	.local .align 8 .b8 	__local_depot96[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<5>;
	.reg .f64 	%fd<11>;

	mov.u64 	%SPL, __local_depot96;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd2, [copysign_param_1];
	ld.param.f64 	%fd1, [copysign_param_0];
	st.f64 	[%SP+8], %fd1;
	st.f64 	[%SP+16], %fd2;
	ld.f64 	%fd3, [%SP+8];
	setp.geu.f64 	%p1, %fd3, 0d0000000000000000;
	@%p1 bra 	$L__BB96_2;
	bra.uni 	$L__BB96_1;
$L__BB96_1:
	ld.f64 	%fd4, [%SP+16];
	setp.gt.f64 	%p2, %fd4, 0d0000000000000000;
	@%p2 bra 	$L__BB96_4;
	bra.uni 	$L__BB96_2;
$L__BB96_2:
	ld.f64 	%fd5, [%SP+8];
	setp.leu.f64 	%p3, %fd5, 0d0000000000000000;
	@%p3 bra 	$L__BB96_5;
	bra.uni 	$L__BB96_3;
$L__BB96_3:
	ld.f64 	%fd6, [%SP+16];
	setp.geu.f64 	%p4, %fd6, 0d0000000000000000;
	@%p4 bra 	$L__BB96_5;
	bra.uni 	$L__BB96_4;
$L__BB96_4:
	ld.f64 	%fd8, [%SP+8];
	neg.f64 	%fd9, %fd8;
	st.f64 	[%SP+0], %fd9;
	bra.uni 	$L__BB96_6;
$L__BB96_5:
	ld.f64 	%fd7, [%SP+8];
	st.f64 	[%SP+0], %fd7;
	bra.uni 	$L__BB96_6;
$L__BB96_6:
	ld.f64 	%fd10, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd10;
	ret;

}
	// .globl	memmem
.visible .func  (.param .b64 func_retval0) memmem(
	.param .b64 memmem_param_0,
	.param .b64 memmem_param_1,
	.param .b64 memmem_param_2,
	.param .b64 memmem_param_3
)
{
	.local .align 8 .b8 	__local_depot97[56];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<31>;

	mov.u64 	%SPL, __local_depot97;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd4, [memmem_param_3];
	ld.param.u64 	%rd3, [memmem_param_2];
	ld.param.u64 	%rd2, [memmem_param_1];
	ld.param.u64 	%rd1, [memmem_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	st.u64 	[%SP+24], %rd3;
	st.u64 	[%SP+32], %rd4;
	ld.u64 	%rd5, [%SP+8];
	ld.u64 	%rd6, [%SP+16];
	add.s64 	%rd7, %rd5, %rd6;
	ld.u64 	%rd8, [%SP+32];
	sub.s64 	%rd9, %rd7, %rd8;
	st.u64 	[%SP+48], %rd9;
	ld.u64 	%rd10, [%SP+32];
	setp.ne.s64 	%p1, %rd10, 0;
	@%p1 bra 	$L__BB97_2;
	bra.uni 	$L__BB97_1;
$L__BB97_1:
	ld.u64 	%rd29, [%SP+8];
	st.u64 	[%SP+0], %rd29;
	bra.uni 	$L__BB97_12;
$L__BB97_2:
	ld.u64 	%rd11, [%SP+16];
	ld.u64 	%rd12, [%SP+32];
	setp.ge.u64 	%p2, %rd11, %rd12;
	@%p2 bra 	$L__BB97_4;
	bra.uni 	$L__BB97_3;
$L__BB97_3:
	mov.u64 	%rd28, 0;
	st.u64 	[%SP+0], %rd28;
	bra.uni 	$L__BB97_12;
$L__BB97_4:
	ld.u64 	%rd13, [%SP+8];
	st.u64 	[%SP+40], %rd13;
	bra.uni 	$L__BB97_5;
$L__BB97_5:
	ld.u64 	%rd14, [%SP+40];
	ld.u64 	%rd15, [%SP+48];
	setp.gt.u64 	%p3, %rd14, %rd15;
	@%p3 bra 	$L__BB97_11;
	bra.uni 	$L__BB97_6;
$L__BB97_6:
	ld.u64 	%rd17, [%SP+40];
	ld.s8 	%r1, [%rd17];
	ld.u64 	%rd18, [%SP+24];
	ld.s8 	%r2, [%rd18];
	setp.ne.s32 	%p4, %r1, %r2;
	@%p4 bra 	$L__BB97_9;
	bra.uni 	$L__BB97_7;
$L__BB97_7:
	ld.u64 	%rd19, [%SP+40];
	add.s64 	%rd20, %rd19, 1;
	ld.u64 	%rd21, [%SP+24];
	add.s64 	%rd22, %rd21, 1;
	ld.u64 	%rd23, [%SP+32];
	add.s64 	%rd24, %rd23, -1;
	{ // callseq 12, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd20;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd22;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	memcmp, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r3, [retval0+0];
	} // callseq 12
	setp.ne.s32 	%p5, %r3, 0;
	@%p5 bra 	$L__BB97_9;
	bra.uni 	$L__BB97_8;
$L__BB97_8:
	ld.u64 	%rd25, [%SP+40];
	st.u64 	[%SP+0], %rd25;
	bra.uni 	$L__BB97_12;
$L__BB97_9:
	bra.uni 	$L__BB97_10;
$L__BB97_10:
	ld.u64 	%rd26, [%SP+40];
	add.s64 	%rd27, %rd26, 1;
	st.u64 	[%SP+40], %rd27;
	bra.uni 	$L__BB97_5;
$L__BB97_11:
	mov.u64 	%rd16, 0;
	st.u64 	[%SP+0], %rd16;
	bra.uni 	$L__BB97_12;
$L__BB97_12:
	ld.u64 	%rd30, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd30;
	ret;

}
	// .globl	mempcpy
.visible .func  (.param .b64 func_retval0) mempcpy(
	.param .b64 mempcpy_param_0,
	.param .b64 mempcpy_param_1,
	.param .b64 mempcpy_param_2
)
{
	.local .align 8 .b8 	__local_depot98[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<15>;

	mov.u64 	%SPL, __local_depot98;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd8, [mempcpy_param_2];
	ld.param.u64 	%rd7, [mempcpy_param_1];
	ld.param.u64 	%rd6, [mempcpy_param_0];
	st.u64 	[%SP+0], %rd6;
	st.u64 	[%SP+8], %rd7;
	st.u64 	[%SP+16], %rd8;
	ld.u64 	%rd1, [%SP+0];
	ld.u64 	%rd2, [%SP+8];
	ld.u64 	%rd3, [%SP+16];
	setp.eq.s64 	%p1, %rd3, 0;
	mov.u64 	%rd9, 0;
	mov.u64 	%rd14, %rd9;
	@%p1 bra 	$L__BB98_2;
	bra.uni 	$L__BB98_1;
$L__BB98_1:
	mov.u64 	%rd4, %rd14;
	add.s64 	%rd10, %rd2, %rd4;
	ld.u8 	%rs1, [%rd10];
	add.s64 	%rd11, %rd1, %rd4;
	st.u8 	[%rd11], %rs1;
	add.s64 	%rd5, %rd4, 1;
	setp.lt.u64 	%p2, %rd5, %rd3;
	mov.u64 	%rd14, %rd5;
	@%p2 bra 	$L__BB98_1;
	bra.uni 	$L__BB98_2;
$L__BB98_2:
	ld.u64 	%rd12, [%SP+16];
	add.s64 	%rd13, %rd1, %rd12;
	st.param.b64 	[func_retval0+0], %rd13;
	ret;

}
	// .globl	frexp
.visible .func  (.param .b64 func_retval0) frexp(
	.param .b64 frexp_param_0,
	.param .b64 frexp_param_1
)
{
	.local .align 8 .b8 	__local_depot99[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<8>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<3>;
	.reg .f64 	%fd<17>;

	mov.u64 	%SPL, __local_depot99;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [frexp_param_1];
	ld.param.f64 	%fd1, [frexp_param_0];
	st.f64 	[%SP+0], %fd1;
	st.u64 	[%SP+8], %rd1;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+20], %r1;
	st.u32 	[%SP+16], %r1;
	ld.f64 	%fd2, [%SP+0];
	setp.geu.f64 	%p1, %fd2, 0d0000000000000000;
	@%p1 bra 	$L__BB99_2;
	bra.uni 	$L__BB99_1;
$L__BB99_1:
	ld.f64 	%fd3, [%SP+0];
	neg.f64 	%fd4, %fd3;
	st.f64 	[%SP+0], %fd4;
	mov.b32 	%r2, 1;
	st.u32 	[%SP+16], %r2;
	bra.uni 	$L__BB99_2;
$L__BB99_2:
	ld.f64 	%fd5, [%SP+0];
	setp.ltu.f64 	%p2, %fd5, 0d3FF0000000000000;
	@%p2 bra 	$L__BB99_7;
	bra.uni 	$L__BB99_3;
$L__BB99_3:
	bra.uni 	$L__BB99_4;
$L__BB99_4:
	ld.f64 	%fd11, [%SP+0];
	setp.ltu.f64 	%p6, %fd11, 0d3FF0000000000000;
	@%p6 bra 	$L__BB99_6;
	bra.uni 	$L__BB99_5;
$L__BB99_5:
	ld.u32 	%r7, [%SP+20];
	add.s32 	%r8, %r7, 1;
	st.u32 	[%SP+20], %r8;
	ld.f64 	%fd15, [%SP+0];
	mul.rn.f64 	%fd16, %fd15, 0d3FE0000000000000;
	st.f64 	[%SP+0], %fd16;
	bra.uni 	$L__BB99_4;
$L__BB99_6:
	bra.uni 	$L__BB99_14;
$L__BB99_7:
	ld.f64 	%fd6, [%SP+0];
	setp.geu.f64 	%p3, %fd6, 0d3FE0000000000000;
	@%p3 bra 	$L__BB99_13;
	bra.uni 	$L__BB99_8;
$L__BB99_8:
	ld.f64 	%fd7, [%SP+0];
	setp.eq.f64 	%p4, %fd7, 0d0000000000000000;
	@%p4 bra 	$L__BB99_13;
	bra.uni 	$L__BB99_9;
$L__BB99_9:
	bra.uni 	$L__BB99_10;
$L__BB99_10:
	ld.f64 	%fd8, [%SP+0];
	setp.geu.f64 	%p5, %fd8, 0d3FE0000000000000;
	@%p5 bra 	$L__BB99_12;
	bra.uni 	$L__BB99_11;
$L__BB99_11:
	ld.u32 	%r3, [%SP+20];
	add.s32 	%r4, %r3, -1;
	st.u32 	[%SP+20], %r4;
	ld.f64 	%fd9, [%SP+0];
	add.rn.f64 	%fd10, %fd9, %fd9;
	st.f64 	[%SP+0], %fd10;
	bra.uni 	$L__BB99_10;
$L__BB99_12:
	bra.uni 	$L__BB99_13;
$L__BB99_13:
	bra.uni 	$L__BB99_14;
$L__BB99_14:
	ld.u32 	%r5, [%SP+20];
	ld.u64 	%rd2, [%SP+8];
	st.u32 	[%rd2], %r5;
	ld.u32 	%r6, [%SP+16];
	setp.eq.s32 	%p7, %r6, 0;
	@%p7 bra 	$L__BB99_16;
	bra.uni 	$L__BB99_15;
$L__BB99_15:
	ld.f64 	%fd12, [%SP+0];
	neg.f64 	%fd13, %fd12;
	st.f64 	[%SP+0], %fd13;
	bra.uni 	$L__BB99_16;
$L__BB99_16:
	ld.f64 	%fd14, [%SP+0];
	st.param.f64 	[func_retval0+0], %fd14;
	ret;

}
	// .globl	__muldi3
.visible .func  (.param .b64 func_retval0) __muldi3(
	.param .b64 __muldi3_param_0,
	.param .b64 __muldi3_param_1
)
{
	.local .align 8 .b8 	__local_depot100[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b64 	%rd<16>;

	mov.u64 	%SPL, __local_depot100;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [__muldi3_param_1];
	ld.param.u64 	%rd1, [__muldi3_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	mov.u64 	%rd3, 0;
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+0];
	st.u64 	[%SP+24], %rd4;
	bra.uni 	$L__BB100_1;
$L__BB100_1:
	ld.u64 	%rd5, [%SP+24];
	setp.eq.s64 	%p1, %rd5, 0;
	@%p1 bra 	$L__BB100_5;
	bra.uni 	$L__BB100_2;
$L__BB100_2:
	ld.u64 	%rd7, [%SP+24];
	and.b64  	%rd8, %rd7, 1;
	setp.eq.b64 	%p2, %rd8, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB100_4;
	bra.uni 	$L__BB100_3;
$L__BB100_3:
	ld.u64 	%rd9, [%SP+8];
	ld.u64 	%rd10, [%SP+16];
	add.s64 	%rd11, %rd10, %rd9;
	st.u64 	[%SP+16], %rd11;
	bra.uni 	$L__BB100_4;
$L__BB100_4:
	ld.u64 	%rd12, [%SP+8];
	shl.b64 	%rd13, %rd12, 1;
	st.u64 	[%SP+8], %rd13;
	ld.u64 	%rd14, [%SP+24];
	shr.u64 	%rd15, %rd14, 1;
	st.u64 	[%SP+24], %rd15;
	bra.uni 	$L__BB100_1;
$L__BB100_5:
	ld.u64 	%rd6, [%SP+16];
	st.param.b64 	[func_retval0+0], %rd6;
	ret;

}
	// .globl	udivmodsi4
.visible .func  (.param .b32 func_retval0) udivmodsi4(
	.param .b32 udivmodsi4_param_0,
	.param .b32 udivmodsi4_param_1,
	.param .b64 udivmodsi4_param_2
)
{
	.local .align 8 .b8 	__local_depot101[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<5>;

	mov.u64 	%SPL, __local_depot101;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [udivmodsi4_param_2];
	ld.param.u32 	%r2, [udivmodsi4_param_1];
	ld.param.u32 	%r1, [udivmodsi4_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u64 	[%SP+16], %rd1;
	mov.b32 	%r3, 1;
	st.u32 	[%SP+24], %r3;
	mov.b32 	%r4, 0;
	st.u32 	[%SP+28], %r4;
	bra.uni 	$L__BB101_1;
$L__BB101_1:
	ld.u32 	%r5, [%SP+8];
	ld.u32 	%r6, [%SP+4];
	setp.ge.u32 	%p4, %r5, %r6;
	mov.pred 	%p3, 0;
	mov.pred 	%p10, %p3;
	@%p4 bra 	$L__BB101_4;
	bra.uni 	$L__BB101_2;
$L__BB101_2:
	ld.u32 	%r7, [%SP+24];
	setp.eq.s32 	%p6, %r7, 0;
	mov.pred 	%p5, 0;
	mov.pred 	%p10, %p5;
	@%p6 bra 	$L__BB101_4;
	bra.uni 	$L__BB101_3;
$L__BB101_3:
	ld.u32 	%rd2, [%SP+8];
	and.b64  	%rd3, %rd2, 2147483648;
	setp.eq.s64 	%p1, %rd3, 0;
	mov.pred 	%p10, %p1;
	bra.uni 	$L__BB101_4;
$L__BB101_4:
	mov.pred 	%p2, %p10;
	@!%p2 bra 	$L__BB101_6;
	bra.uni 	$L__BB101_5;
$L__BB101_5:
	ld.u32 	%r24, [%SP+8];
	shl.b32 	%r25, %r24, 1;
	st.u32 	[%SP+8], %r25;
	ld.u32 	%r26, [%SP+24];
	shl.b32 	%r27, %r26, 1;
	st.u32 	[%SP+24], %r27;
	bra.uni 	$L__BB101_1;
$L__BB101_6:
	bra.uni 	$L__BB101_7;
$L__BB101_7:
	ld.u32 	%r8, [%SP+24];
	setp.eq.s32 	%p7, %r8, 0;
	@%p7 bra 	$L__BB101_11;
	bra.uni 	$L__BB101_8;
$L__BB101_8:
	ld.u32 	%r12, [%SP+4];
	ld.u32 	%r13, [%SP+8];
	setp.lt.u32 	%p9, %r12, %r13;
	@%p9 bra 	$L__BB101_10;
	bra.uni 	$L__BB101_9;
$L__BB101_9:
	ld.u32 	%r14, [%SP+8];
	ld.u32 	%r15, [%SP+4];
	sub.s32 	%r16, %r15, %r14;
	st.u32 	[%SP+4], %r16;
	ld.u32 	%r17, [%SP+24];
	ld.u32 	%r18, [%SP+28];
	or.b32  	%r19, %r18, %r17;
	st.u32 	[%SP+28], %r19;
	bra.uni 	$L__BB101_10;
$L__BB101_10:
	ld.u32 	%r20, [%SP+24];
	shr.u32 	%r21, %r20, 1;
	st.u32 	[%SP+24], %r21;
	ld.u32 	%r22, [%SP+8];
	shr.u32 	%r23, %r22, 1;
	st.u32 	[%SP+8], %r23;
	bra.uni 	$L__BB101_7;
$L__BB101_11:
	ld.u64 	%rd4, [%SP+16];
	setp.eq.s64 	%p8, %rd4, 0;
	@%p8 bra 	$L__BB101_13;
	bra.uni 	$L__BB101_12;
$L__BB101_12:
	ld.u32 	%r10, [%SP+4];
	st.u32 	[%SP+0], %r10;
	bra.uni 	$L__BB101_14;
$L__BB101_13:
	ld.u32 	%r9, [%SP+28];
	st.u32 	[%SP+0], %r9;
	bra.uni 	$L__BB101_14;
$L__BB101_14:
	ld.u32 	%r11, [%SP+0];
	st.param.b32 	[func_retval0+0], %r11;
	ret;

}
	// .globl	__clrsbqi2
.visible .func  (.param .b32 func_retval0) __clrsbqi2(
	.param .b32 __clrsbqi2_param_0
)
{
	.local .align 4 .b8 	__local_depot102[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<12>;

	mov.u64 	%SPL, __local_depot102;
	cvta.local.u64 	%SP, %SPL;
	ld.param.s8 	%rs1, [__clrsbqi2_param_0];
	st.u8 	[%SP+4], %rs1;
	ld.s8 	%r1, [%SP+4];
	setp.gt.s32 	%p1, %r1, -1;
	@%p1 bra 	$L__BB102_2;
	bra.uni 	$L__BB102_1;
$L__BB102_1:
	ld.u8 	%r2, [%SP+4];
	not.b32 	%r3, %r2;
	st.u8 	[%SP+4], %r3;
	bra.uni 	$L__BB102_2;
$L__BB102_2:
	ld.s8 	%r4, [%SP+4];
	setp.ne.s32 	%p2, %r4, 0;
	@%p2 bra 	$L__BB102_4;
	bra.uni 	$L__BB102_3;
$L__BB102_3:
	mov.b32 	%r10, 7;
	st.u32 	[%SP+0], %r10;
	bra.uni 	$L__BB102_5;
$L__BB102_4:
	ld.s8 	%r5, [%SP+4];
	shl.b32 	%r6, %r5, 8;
	clz.b32 	%r7, %r6;
	st.u32 	[%SP+8], %r7;
	ld.u32 	%r8, [%SP+8];
	add.s32 	%r9, %r8, -1;
	st.u32 	[%SP+0], %r9;
	bra.uni 	$L__BB102_5;
$L__BB102_5:
	ld.u32 	%r11, [%SP+0];
	st.param.b32 	[func_retval0+0], %r11;
	ret;

}
	// .globl	__clrsbdi2
.visible .func  (.param .b32 func_retval0) __clrsbdi2(
	.param .b64 __clrsbdi2_param_0
)
{
	.local .align 8 .b8 	__local_depot103[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;

	mov.u64 	%SPL, __local_depot103;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [__clrsbdi2_param_0];
	st.u64 	[%SP+8], %rd1;
	ld.u64 	%rd2, [%SP+8];
	setp.gt.s64 	%p1, %rd2, -1;
	@%p1 bra 	$L__BB103_2;
	bra.uni 	$L__BB103_1;
$L__BB103_1:
	ld.u64 	%rd3, [%SP+8];
	not.b64 	%rd4, %rd3;
	st.u64 	[%SP+8], %rd4;
	bra.uni 	$L__BB103_2;
$L__BB103_2:
	ld.u64 	%rd5, [%SP+8];
	setp.ne.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB103_4;
	bra.uni 	$L__BB103_3;
$L__BB103_3:
	mov.b32 	%r4, 63;
	st.u32 	[%SP+0], %r4;
	bra.uni 	$L__BB103_5;
$L__BB103_4:
	ld.u64 	%rd6, [%SP+8];
	clz.b64 	%r1, %rd6;
	cvt.u64.u32 	%rd7, %r1;
	st.u32 	[%SP+16], %rd7;
	ld.u32 	%r2, [%SP+16];
	add.s32 	%r3, %r2, -1;
	st.u32 	[%SP+0], %r3;
	bra.uni 	$L__BB103_5;
$L__BB103_5:
	ld.u32 	%r5, [%SP+0];
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	__mulsi3
.visible .func  (.param .b32 func_retval0) __mulsi3(
	.param .b32 __mulsi3_param_0,
	.param .b32 __mulsi3_param_1
)
{
	.local .align 4 .b8 	__local_depot104[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<15>;

	mov.u64 	%SPL, __local_depot104;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [__mulsi3_param_1];
	ld.param.u32 	%r1, [__mulsi3_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	mov.b32 	%r3, 0;
	st.u32 	[%SP+8], %r3;
	bra.uni 	$L__BB104_1;
$L__BB104_1:
	ld.u32 	%r4, [%SP+0];
	setp.eq.s32 	%p1, %r4, 0;
	@%p1 bra 	$L__BB104_5;
	bra.uni 	$L__BB104_2;
$L__BB104_2:
	ld.u32 	%r6, [%SP+0];
	and.b32  	%r7, %r6, 1;
	setp.eq.b32 	%p2, %r7, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB104_4;
	bra.uni 	$L__BB104_3;
$L__BB104_3:
	ld.u32 	%r8, [%SP+4];
	ld.u32 	%r9, [%SP+8];
	add.s32 	%r10, %r9, %r8;
	st.u32 	[%SP+8], %r10;
	bra.uni 	$L__BB104_4;
$L__BB104_4:
	ld.u32 	%r11, [%SP+0];
	shr.u32 	%r12, %r11, 1;
	st.u32 	[%SP+0], %r12;
	ld.u32 	%r13, [%SP+4];
	shl.b32 	%r14, %r13, 1;
	st.u32 	[%SP+4], %r14;
	bra.uni 	$L__BB104_1;
$L__BB104_5:
	ld.u32 	%r5, [%SP+8];
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	__cmovd
.visible .func __cmovd(
	.param .b64 __cmovd_param_0,
	.param .b64 __cmovd_param_1,
	.param .b32 __cmovd_param_2
)
{
	.local .align 8 .b8 	__local_depot105[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<28>;

	mov.u64 	%SPL, __local_depot105;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__cmovd_param_2];
	ld.param.u64 	%rd2, [__cmovd_param_1];
	ld.param.u64 	%rd1, [__cmovd_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u32 	[%SP+16], %r1;
	ld.u32 	%r2, [%SP+16];
	shr.u32 	%r3, %r2, 3;
	st.u32 	[%SP+24], %r3;
	ld.u32 	%r4, [%SP+16];
	and.b32  	%r5, %r4, -8;
	st.u32 	[%SP+28], %r5;
	ld.u64 	%rd3, [%SP+0];
	st.u64 	[%SP+32], %rd3;
	ld.u64 	%rd4, [%SP+8];
	st.u64 	[%SP+40], %rd4;
	ld.u64 	%rd5, [%SP+32];
	ld.u64 	%rd6, [%SP+40];
	setp.lt.u64 	%p1, %rd5, %rd6;
	@%p1 bra 	$L__BB105_2;
	bra.uni 	$L__BB105_1;
$L__BB105_1:
	ld.u64 	%rd7, [%SP+32];
	ld.u64 	%rd8, [%SP+40];
	ld.u32 	%rd9, [%SP+16];
	add.s64 	%rd10, %rd8, %rd9;
	setp.le.u64 	%p2, %rd7, %rd10;
	@%p2 bra 	$L__BB105_10;
	bra.uni 	$L__BB105_2;
$L__BB105_2:
	mov.b32 	%r8, 0;
	st.u32 	[%SP+20], %r8;
	bra.uni 	$L__BB105_3;
$L__BB105_3:
	ld.u32 	%r9, [%SP+20];
	ld.u32 	%r10, [%SP+24];
	setp.ge.u32 	%p4, %r9, %r10;
	@%p4 bra 	$L__BB105_6;
	bra.uni 	$L__BB105_4;
$L__BB105_4:
	ld.u64 	%rd21, [%SP+8];
	ld.u32 	%rd22, [%SP+20];
	shl.b64 	%rd23, %rd22, 3;
	add.s64 	%rd24, %rd21, %rd23;
	ld.u64 	%rd25, [%rd24];
	ld.u64 	%rd26, [%SP+0];
	add.s64 	%rd27, %rd26, %rd23;
	st.u64 	[%rd27], %rd25;
	bra.uni 	$L__BB105_5;
$L__BB105_5:
	ld.u32 	%r15, [%SP+20];
	add.s32 	%r16, %r15, 1;
	st.u32 	[%SP+20], %r16;
	bra.uni 	$L__BB105_3;
$L__BB105_6:
	bra.uni 	$L__BB105_7;
$L__BB105_7:
	ld.u32 	%r11, [%SP+16];
	ld.u32 	%r12, [%SP+28];
	setp.le.u32 	%p5, %r11, %r12;
	@%p5 bra 	$L__BB105_9;
	bra.uni 	$L__BB105_8;
$L__BB105_8:
	ld.u64 	%rd16, [%SP+40];
	ld.u32 	%rd17, [%SP+28];
	add.s64 	%rd18, %rd16, %rd17;
	ld.u8 	%rs2, [%rd18];
	ld.u64 	%rd19, [%SP+32];
	add.s64 	%rd20, %rd19, %rd17;
	st.u8 	[%rd20], %rs2;
	ld.u32 	%r13, [%SP+28];
	add.s32 	%r14, %r13, 1;
	st.u32 	[%SP+28], %r14;
	bra.uni 	$L__BB105_7;
$L__BB105_9:
	bra.uni 	$L__BB105_14;
$L__BB105_10:
	bra.uni 	$L__BB105_11;
$L__BB105_11:
	ld.u32 	%r6, [%SP+16];
	add.s32 	%r7, %r6, -1;
	st.u32 	[%SP+16], %r7;
	setp.eq.s32 	%p3, %r6, 0;
	@%p3 bra 	$L__BB105_13;
	bra.uni 	$L__BB105_12;
$L__BB105_12:
	ld.u64 	%rd11, [%SP+40];
	ld.u32 	%rd12, [%SP+16];
	add.s64 	%rd13, %rd11, %rd12;
	ld.u8 	%rs1, [%rd13];
	ld.u64 	%rd14, [%SP+32];
	add.s64 	%rd15, %rd14, %rd12;
	st.u8 	[%rd15], %rs1;
	bra.uni 	$L__BB105_11;
$L__BB105_13:
	bra.uni 	$L__BB105_14;
$L__BB105_14:
	ret;

}
	// .globl	__cmovh
.visible .func __cmovh(
	.param .b64 __cmovh_param_0,
	.param .b64 __cmovh_param_1,
	.param .b32 __cmovh_param_2
)
{
	.local .align 8 .b8 	__local_depot106[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<27>;

	mov.u64 	%SPL, __local_depot106;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__cmovh_param_2];
	ld.param.u64 	%rd2, [__cmovh_param_1];
	ld.param.u64 	%rd1, [__cmovh_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u32 	[%SP+16], %r1;
	ld.u32 	%r2, [%SP+16];
	shr.u32 	%r3, %r2, 1;
	st.u32 	[%SP+24], %r3;
	ld.u64 	%rd3, [%SP+0];
	st.u64 	[%SP+32], %rd3;
	ld.u64 	%rd4, [%SP+8];
	st.u64 	[%SP+40], %rd4;
	ld.u64 	%rd5, [%SP+32];
	ld.u64 	%rd6, [%SP+40];
	setp.lt.u64 	%p1, %rd5, %rd6;
	@%p1 bra 	$L__BB106_2;
	bra.uni 	$L__BB106_1;
$L__BB106_1:
	ld.u64 	%rd7, [%SP+32];
	ld.u64 	%rd8, [%SP+40];
	ld.u32 	%rd9, [%SP+16];
	add.s64 	%rd10, %rd8, %rd9;
	setp.le.u64 	%p2, %rd7, %rd10;
	@%p2 bra 	$L__BB106_9;
	bra.uni 	$L__BB106_2;
$L__BB106_2:
	mov.b32 	%r6, 0;
	st.u32 	[%SP+20], %r6;
	bra.uni 	$L__BB106_3;
$L__BB106_3:
	ld.u32 	%r7, [%SP+20];
	ld.u32 	%r8, [%SP+24];
	setp.ge.u32 	%p4, %r7, %r8;
	@%p4 bra 	$L__BB106_6;
	bra.uni 	$L__BB106_4;
$L__BB106_4:
	ld.u64 	%rd21, [%SP+8];
	ld.u32 	%rd22, [%SP+20];
	shl.b64 	%rd23, %rd22, 1;
	add.s64 	%rd24, %rd21, %rd23;
	ld.u16 	%rs3, [%rd24];
	ld.u64 	%rd25, [%SP+0];
	add.s64 	%rd26, %rd25, %rd23;
	st.u16 	[%rd26], %rs3;
	bra.uni 	$L__BB106_5;
$L__BB106_5:
	ld.u32 	%r13, [%SP+20];
	add.s32 	%r14, %r13, 1;
	st.u32 	[%SP+20], %r14;
	bra.uni 	$L__BB106_3;
$L__BB106_6:
	ld.u32 	%r9, [%SP+16];
	and.b32  	%r10, %r9, 1;
	setp.eq.b32 	%p5, %r10, 1;
	mov.pred 	%p6, 0;
	xor.pred  	%p7, %p5, %p6;
	not.pred 	%p8, %p7;
	@%p8 bra 	$L__BB106_8;
	bra.uni 	$L__BB106_7;
$L__BB106_7:
	ld.u64 	%rd16, [%SP+40];
	ld.u32 	%r11, [%SP+16];
	add.s32 	%r12, %r11, -1;
	cvt.u64.u32 	%rd17, %r12;
	add.s64 	%rd18, %rd16, %rd17;
	ld.u8 	%rs2, [%rd18];
	ld.u64 	%rd19, [%SP+32];
	add.s64 	%rd20, %rd19, %rd17;
	st.u8 	[%rd20], %rs2;
	bra.uni 	$L__BB106_8;
$L__BB106_8:
	bra.uni 	$L__BB106_13;
$L__BB106_9:
	bra.uni 	$L__BB106_10;
$L__BB106_10:
	ld.u32 	%r4, [%SP+16];
	add.s32 	%r5, %r4, -1;
	st.u32 	[%SP+16], %r5;
	setp.eq.s32 	%p3, %r4, 0;
	@%p3 bra 	$L__BB106_12;
	bra.uni 	$L__BB106_11;
$L__BB106_11:
	ld.u64 	%rd11, [%SP+40];
	ld.u32 	%rd12, [%SP+16];
	add.s64 	%rd13, %rd11, %rd12;
	ld.u8 	%rs1, [%rd13];
	ld.u64 	%rd14, [%SP+32];
	add.s64 	%rd15, %rd14, %rd12;
	st.u8 	[%rd15], %rs1;
	bra.uni 	$L__BB106_10;
$L__BB106_12:
	bra.uni 	$L__BB106_13;
$L__BB106_13:
	ret;

}
	// .globl	__cmovw
.visible .func __cmovw(
	.param .b64 __cmovw_param_0,
	.param .b64 __cmovw_param_1,
	.param .b32 __cmovw_param_2
)
{
	.local .align 8 .b8 	__local_depot107[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<27>;

	mov.u64 	%SPL, __local_depot107;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__cmovw_param_2];
	ld.param.u64 	%rd2, [__cmovw_param_1];
	ld.param.u64 	%rd1, [__cmovw_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u32 	[%SP+16], %r1;
	ld.u32 	%r2, [%SP+16];
	shr.u32 	%r3, %r2, 2;
	st.u32 	[%SP+24], %r3;
	ld.u32 	%r4, [%SP+16];
	and.b32  	%r5, %r4, -4;
	st.u32 	[%SP+28], %r5;
	ld.u64 	%rd3, [%SP+0];
	st.u64 	[%SP+32], %rd3;
	ld.u64 	%rd4, [%SP+8];
	st.u64 	[%SP+40], %rd4;
	ld.u64 	%rd5, [%SP+32];
	ld.u64 	%rd6, [%SP+40];
	setp.lt.u64 	%p1, %rd5, %rd6;
	@%p1 bra 	$L__BB107_2;
	bra.uni 	$L__BB107_1;
$L__BB107_1:
	ld.u64 	%rd7, [%SP+32];
	ld.u64 	%rd8, [%SP+40];
	ld.u32 	%rd9, [%SP+16];
	add.s64 	%rd10, %rd8, %rd9;
	setp.le.u64 	%p2, %rd7, %rd10;
	@%p2 bra 	$L__BB107_10;
	bra.uni 	$L__BB107_2;
$L__BB107_2:
	mov.b32 	%r8, 0;
	st.u32 	[%SP+20], %r8;
	bra.uni 	$L__BB107_3;
$L__BB107_3:
	ld.u32 	%r9, [%SP+20];
	ld.u32 	%r10, [%SP+24];
	setp.ge.u32 	%p4, %r9, %r10;
	@%p4 bra 	$L__BB107_6;
	bra.uni 	$L__BB107_4;
$L__BB107_4:
	ld.u64 	%rd21, [%SP+8];
	ld.u32 	%rd22, [%SP+20];
	shl.b64 	%rd23, %rd22, 2;
	add.s64 	%rd24, %rd21, %rd23;
	ld.u32 	%r15, [%rd24];
	ld.u64 	%rd25, [%SP+0];
	add.s64 	%rd26, %rd25, %rd23;
	st.u32 	[%rd26], %r15;
	bra.uni 	$L__BB107_5;
$L__BB107_5:
	ld.u32 	%r16, [%SP+20];
	add.s32 	%r17, %r16, 1;
	st.u32 	[%SP+20], %r17;
	bra.uni 	$L__BB107_3;
$L__BB107_6:
	bra.uni 	$L__BB107_7;
$L__BB107_7:
	ld.u32 	%r11, [%SP+16];
	ld.u32 	%r12, [%SP+28];
	setp.le.u32 	%p5, %r11, %r12;
	@%p5 bra 	$L__BB107_9;
	bra.uni 	$L__BB107_8;
$L__BB107_8:
	ld.u64 	%rd16, [%SP+40];
	ld.u32 	%rd17, [%SP+28];
	add.s64 	%rd18, %rd16, %rd17;
	ld.u8 	%rs2, [%rd18];
	ld.u64 	%rd19, [%SP+32];
	add.s64 	%rd20, %rd19, %rd17;
	st.u8 	[%rd20], %rs2;
	ld.u32 	%r13, [%SP+28];
	add.s32 	%r14, %r13, 1;
	st.u32 	[%SP+28], %r14;
	bra.uni 	$L__BB107_7;
$L__BB107_9:
	bra.uni 	$L__BB107_14;
$L__BB107_10:
	bra.uni 	$L__BB107_11;
$L__BB107_11:
	ld.u32 	%r6, [%SP+16];
	add.s32 	%r7, %r6, -1;
	st.u32 	[%SP+16], %r7;
	setp.eq.s32 	%p3, %r6, 0;
	@%p3 bra 	$L__BB107_13;
	bra.uni 	$L__BB107_12;
$L__BB107_12:
	ld.u64 	%rd11, [%SP+40];
	ld.u32 	%rd12, [%SP+16];
	add.s64 	%rd13, %rd11, %rd12;
	ld.u8 	%rs1, [%rd13];
	ld.u64 	%rd14, [%SP+32];
	add.s64 	%rd15, %rd14, %rd12;
	st.u8 	[%rd15], %rs1;
	bra.uni 	$L__BB107_11;
$L__BB107_13:
	bra.uni 	$L__BB107_14;
$L__BB107_14:
	ret;

}
	// .globl	__modi
.visible .func  (.param .b32 func_retval0) __modi(
	.param .b32 __modi_param_0,
	.param .b32 __modi_param_1
)
{
	.local .align 4 .b8 	__local_depot108[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<6>;

	mov.u64 	%SPL, __local_depot108;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [__modi_param_1];
	ld.param.u32 	%r1, [__modi_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	rem.s32 	%r5, %r3, %r4;
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	__uitod
.visible .func  (.param .b64 func_retval0) __uitod(
	.param .b32 __uitod_param_0
)
{
	.local .align 4 .b8 	__local_depot109[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<3>;
	.reg .f64 	%fd<2>;

	mov.u64 	%SPL, __local_depot109;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__uitod_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	cvt.rn.f64.u32 	%fd1, %r2;
	st.param.f64 	[func_retval0+0], %fd1;
	ret;

}
	// .globl	__uitof
.visible .func  (.param .b32 func_retval0) __uitof(
	.param .b32 __uitof_param_0
)
{
	.local .align 4 .b8 	__local_depot110[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<3>;
	.reg .f32 	%f<2>;

	mov.u64 	%SPL, __local_depot110;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__uitof_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	cvt.rn.f32.u32 	%f1, %r2;
	st.param.f32 	[func_retval0+0], %f1;
	ret;

}
	// .globl	__ulltod
.visible .func  (.param .b64 func_retval0) __ulltod(
	.param .b64 __ulltod_param_0
)
{
	.local .align 8 .b8 	__local_depot111[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<3>;
	.reg .f64 	%fd<2>;

	mov.u64 	%SPL, __local_depot111;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [__ulltod_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	cvt.rn.f64.u64 	%fd1, %rd2;
	st.param.f64 	[func_retval0+0], %fd1;
	ret;

}
	// .globl	__ulltof
.visible .func  (.param .b32 func_retval0) __ulltof(
	.param .b64 __ulltof_param_0
)
{
	.local .align 8 .b8 	__local_depot112[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<3>;

	mov.u64 	%SPL, __local_depot112;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [__ulltof_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	cvt.rn.f32.u64 	%f1, %rd2;
	st.param.f32 	[func_retval0+0], %f1;
	ret;

}
	// .globl	__umodi
.visible .func  (.param .b32 func_retval0) __umodi(
	.param .b32 __umodi_param_0,
	.param .b32 __umodi_param_1
)
{
	.local .align 4 .b8 	__local_depot113[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<6>;

	mov.u64 	%SPL, __local_depot113;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [__umodi_param_1];
	ld.param.u32 	%r1, [__umodi_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	rem.u32 	%r5, %r3, %r4;
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	__clzhi2
.visible .func  (.param .b32 func_retval0) __clzhi2(
	.param .b32 __clzhi2_param_0
)
{
	.local .align 4 .b8 	__local_depot114[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<12>;

	mov.u64 	%SPL, __local_depot114;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u16 	%rs1, [__clzhi2_param_0];
	st.u16 	[%SP+0], %rs1;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+4], %r1;
	bra.uni 	$L__BB114_1;
$L__BB114_1:
	ld.u32 	%r2, [%SP+4];
	setp.gt.s32 	%p1, %r2, 15;
	@%p1 bra 	$L__BB114_6;
	bra.uni 	$L__BB114_2;
$L__BB114_2:
	ld.u16 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	mov.b32 	%r5, 15;
	sub.s32 	%r6, %r5, %r4;
	shr.u32 	%r7, %r3, %r6;
	and.b32  	%r8, %r7, 1;
	setp.eq.b32 	%p2, %r8, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB114_4;
	bra.uni 	$L__BB114_3;
$L__BB114_3:
	bra.uni 	$L__BB114_6;
$L__BB114_4:
	bra.uni 	$L__BB114_5;
$L__BB114_5:
	ld.u32 	%r9, [%SP+4];
	add.s32 	%r10, %r9, 1;
	st.u32 	[%SP+4], %r10;
	bra.uni 	$L__BB114_1;
$L__BB114_6:
	ld.u32 	%r11, [%SP+4];
	st.param.b32 	[func_retval0+0], %r11;
	ret;

}
	// .globl	__ctzhi2
.visible .func  (.param .b32 func_retval0) __ctzhi2(
	.param .b32 __ctzhi2_param_0
)
{
	.local .align 4 .b8 	__local_depot115[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;

	mov.u64 	%SPL, __local_depot115;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u16 	%rs1, [__ctzhi2_param_0];
	st.u16 	[%SP+0], %rs1;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+4], %r1;
	bra.uni 	$L__BB115_1;
$L__BB115_1:
	ld.u32 	%r2, [%SP+4];
	setp.gt.s32 	%p1, %r2, 15;
	@%p1 bra 	$L__BB115_6;
	bra.uni 	$L__BB115_2;
$L__BB115_2:
	ld.u16 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+4];
	shr.u32 	%r5, %r3, %r4;
	and.b32  	%r6, %r5, 1;
	setp.eq.b32 	%p2, %r6, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB115_4;
	bra.uni 	$L__BB115_3;
$L__BB115_3:
	bra.uni 	$L__BB115_6;
$L__BB115_4:
	bra.uni 	$L__BB115_5;
$L__BB115_5:
	ld.u32 	%r7, [%SP+4];
	add.s32 	%r8, %r7, 1;
	st.u32 	[%SP+4], %r8;
	bra.uni 	$L__BB115_1;
$L__BB115_6:
	ld.u32 	%r9, [%SP+4];
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	__fixunssfsi
.visible .func  (.param .b64 func_retval0) __fixunssfsi(
	.param .b32 __fixunssfsi_param_0
)
{
	.local .align 8 .b8 	__local_depot116[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .f32 	%f<6>;
	.reg .b64 	%rd<5>;

	mov.u64 	%SPL, __local_depot116;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f1, [__fixunssfsi_param_0];
	st.f32 	[%SP+8], %f1;
	ld.f32 	%f2, [%SP+8];
	setp.ltu.f32 	%p1, %f2, 0f47000000;
	@%p1 bra 	$L__BB116_2;
	bra.uni 	$L__BB116_1;
$L__BB116_1:
	ld.f32 	%f4, [%SP+8];
	add.rn.f32 	%f5, %f4, 0fC7000000;
	cvt.rzi.s64.f32 	%rd2, %f5;
	add.s64 	%rd3, %rd2, 32768;
	st.u64 	[%SP+0], %rd3;
	bra.uni 	$L__BB116_3;
$L__BB116_2:
	ld.f32 	%f3, [%SP+8];
	cvt.rzi.s64.f32 	%rd1, %f3;
	st.u64 	[%SP+0], %rd1;
	bra.uni 	$L__BB116_3;
$L__BB116_3:
	ld.u64 	%rd4, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd4;
	ret;

}
	// .globl	__parityhi2
.visible .func  (.param .b32 func_retval0) __parityhi2(
	.param .b32 __parityhi2_param_0
)
{
	.local .align 4 .b8 	__local_depot117[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<13>;

	mov.u64 	%SPL, __local_depot117;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u16 	%rs1, [__parityhi2_param_0];
	st.u16 	[%SP+0], %rs1;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+8], %r1;
	st.u32 	[%SP+4], %r1;
	bra.uni 	$L__BB117_1;
$L__BB117_1:
	ld.u32 	%r2, [%SP+4];
	setp.gt.s32 	%p1, %r2, 15;
	@%p1 bra 	$L__BB117_6;
	bra.uni 	$L__BB117_2;
$L__BB117_2:
	ld.u16 	%r5, [%SP+0];
	ld.u32 	%r6, [%SP+4];
	shr.u32 	%r7, %r5, %r6;
	and.b32  	%r8, %r7, 1;
	setp.eq.b32 	%p2, %r8, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB117_4;
	bra.uni 	$L__BB117_3;
$L__BB117_3:
	ld.u32 	%r9, [%SP+8];
	add.s32 	%r10, %r9, 1;
	st.u32 	[%SP+8], %r10;
	bra.uni 	$L__BB117_4;
$L__BB117_4:
	bra.uni 	$L__BB117_5;
$L__BB117_5:
	ld.u32 	%r11, [%SP+4];
	add.s32 	%r12, %r11, 1;
	st.u32 	[%SP+4], %r12;
	bra.uni 	$L__BB117_1;
$L__BB117_6:
	ld.u32 	%r3, [%SP+8];
	and.b32  	%r4, %r3, 1;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	__popcounthi2
.visible .func  (.param .b32 func_retval0) __popcounthi2(
	.param .b32 __popcounthi2_param_0
)
{
	.local .align 4 .b8 	__local_depot118[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<12>;

	mov.u64 	%SPL, __local_depot118;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u16 	%rs1, [__popcounthi2_param_0];
	st.u16 	[%SP+0], %rs1;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+8], %r1;
	st.u32 	[%SP+4], %r1;
	bra.uni 	$L__BB118_1;
$L__BB118_1:
	ld.u32 	%r2, [%SP+4];
	setp.gt.s32 	%p1, %r2, 15;
	@%p1 bra 	$L__BB118_6;
	bra.uni 	$L__BB118_2;
$L__BB118_2:
	ld.u16 	%r4, [%SP+0];
	ld.u32 	%r5, [%SP+4];
	shr.u32 	%r6, %r4, %r5;
	and.b32  	%r7, %r6, 1;
	setp.eq.b32 	%p2, %r7, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB118_4;
	bra.uni 	$L__BB118_3;
$L__BB118_3:
	ld.u32 	%r8, [%SP+8];
	add.s32 	%r9, %r8, 1;
	st.u32 	[%SP+8], %r9;
	bra.uni 	$L__BB118_4;
$L__BB118_4:
	bra.uni 	$L__BB118_5;
$L__BB118_5:
	ld.u32 	%r10, [%SP+4];
	add.s32 	%r11, %r10, 1;
	st.u32 	[%SP+4], %r11;
	bra.uni 	$L__BB118_1;
$L__BB118_6:
	ld.u32 	%r3, [%SP+8];
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	__mulsi3_iq2000
.visible .func  (.param .b32 func_retval0) __mulsi3_iq2000(
	.param .b32 __mulsi3_iq2000_param_0,
	.param .b32 __mulsi3_iq2000_param_1
)
{
	.local .align 4 .b8 	__local_depot119[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<15>;

	mov.u64 	%SPL, __local_depot119;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [__mulsi3_iq2000_param_1];
	ld.param.u32 	%r1, [__mulsi3_iq2000_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	mov.b32 	%r3, 0;
	st.u32 	[%SP+8], %r3;
	bra.uni 	$L__BB119_1;
$L__BB119_1:
	ld.u32 	%r4, [%SP+0];
	setp.eq.s32 	%p1, %r4, 0;
	@%p1 bra 	$L__BB119_5;
	bra.uni 	$L__BB119_2;
$L__BB119_2:
	ld.u32 	%r6, [%SP+0];
	and.b32  	%r7, %r6, 1;
	setp.eq.b32 	%p2, %r7, 1;
	mov.pred 	%p3, 0;
	xor.pred  	%p4, %p2, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	$L__BB119_4;
	bra.uni 	$L__BB119_3;
$L__BB119_3:
	ld.u32 	%r8, [%SP+4];
	ld.u32 	%r9, [%SP+8];
	add.s32 	%r10, %r9, %r8;
	st.u32 	[%SP+8], %r10;
	bra.uni 	$L__BB119_4;
$L__BB119_4:
	ld.u32 	%r11, [%SP+0];
	shr.u32 	%r12, %r11, 1;
	st.u32 	[%SP+0], %r12;
	ld.u32 	%r13, [%SP+4];
	shl.b32 	%r14, %r13, 1;
	st.u32 	[%SP+4], %r14;
	bra.uni 	$L__BB119_1;
$L__BB119_5:
	ld.u32 	%r5, [%SP+8];
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	__mulsi3_lm32
.visible .func  (.param .b32 func_retval0) __mulsi3_lm32(
	.param .b32 __mulsi3_lm32_param_0,
	.param .b32 __mulsi3_lm32_param_1
)
{
	.local .align 4 .b8 	__local_depot120[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<18>;

	mov.u64 	%SPL, __local_depot120;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [__mulsi3_lm32_param_1];
	ld.param.u32 	%r1, [__mulsi3_lm32_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	mov.b32 	%r3, 0;
	st.u32 	[%SP+12], %r3;
	ld.u32 	%r4, [%SP+4];
	setp.ne.s32 	%p1, %r4, 0;
	@%p1 bra 	$L__BB120_2;
	bra.uni 	$L__BB120_1;
$L__BB120_1:
	mov.b32 	%r16, 0;
	st.u32 	[%SP+0], %r16;
	bra.uni 	$L__BB120_8;
$L__BB120_2:
	bra.uni 	$L__BB120_3;
$L__BB120_3:
	ld.u32 	%r5, [%SP+8];
	setp.eq.s32 	%p2, %r5, 0;
	@%p2 bra 	$L__BB120_7;
	bra.uni 	$L__BB120_4;
$L__BB120_4:
	ld.u32 	%r7, [%SP+8];
	and.b32  	%r8, %r7, 1;
	setp.eq.b32 	%p3, %r8, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	not.pred 	%p6, %p5;
	@%p6 bra 	$L__BB120_6;
	bra.uni 	$L__BB120_5;
$L__BB120_5:
	ld.u32 	%r9, [%SP+4];
	ld.u32 	%r10, [%SP+12];
	add.s32 	%r11, %r10, %r9;
	st.u32 	[%SP+12], %r11;
	bra.uni 	$L__BB120_6;
$L__BB120_6:
	ld.u32 	%r12, [%SP+4];
	shl.b32 	%r13, %r12, 1;
	st.u32 	[%SP+4], %r13;
	ld.u32 	%r14, [%SP+8];
	shr.u32 	%r15, %r14, 1;
	st.u32 	[%SP+8], %r15;
	bra.uni 	$L__BB120_3;
$L__BB120_7:
	ld.u32 	%r6, [%SP+12];
	st.u32 	[%SP+0], %r6;
	bra.uni 	$L__BB120_8;
$L__BB120_8:
	ld.u32 	%r17, [%SP+0];
	st.param.b32 	[func_retval0+0], %r17;
	ret;

}
	// .globl	__udivmodsi4
.visible .func  (.param .b32 func_retval0) __udivmodsi4(
	.param .b32 __udivmodsi4_param_0,
	.param .b32 __udivmodsi4_param_1,
	.param .b32 __udivmodsi4_param_2
)
{
	.local .align 4 .b8 	__local_depot121[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<3>;

	mov.u64 	%SPL, __local_depot121;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r3, [__udivmodsi4_param_2];
	ld.param.u32 	%r2, [__udivmodsi4_param_1];
	ld.param.u32 	%r1, [__udivmodsi4_param_0];
	st.u32 	[%SP+4], %r1;
	st.u32 	[%SP+8], %r2;
	st.u32 	[%SP+12], %r3;
	mov.b32 	%r4, 1;
	st.u32 	[%SP+16], %r4;
	mov.b32 	%r5, 0;
	st.u32 	[%SP+20], %r5;
	bra.uni 	$L__BB121_1;
$L__BB121_1:
	ld.u32 	%r6, [%SP+8];
	ld.u32 	%r7, [%SP+4];
	setp.ge.u32 	%p4, %r6, %r7;
	mov.pred 	%p3, 0;
	mov.pred 	%p10, %p3;
	@%p4 bra 	$L__BB121_4;
	bra.uni 	$L__BB121_2;
$L__BB121_2:
	ld.u32 	%r8, [%SP+16];
	setp.eq.s32 	%p6, %r8, 0;
	mov.pred 	%p5, 0;
	mov.pred 	%p10, %p5;
	@%p6 bra 	$L__BB121_4;
	bra.uni 	$L__BB121_3;
$L__BB121_3:
	ld.u32 	%rd1, [%SP+8];
	and.b64  	%rd2, %rd1, 2147483648;
	setp.eq.s64 	%p1, %rd2, 0;
	mov.pred 	%p10, %p1;
	bra.uni 	$L__BB121_4;
$L__BB121_4:
	mov.pred 	%p2, %p10;
	@!%p2 bra 	$L__BB121_6;
	bra.uni 	$L__BB121_5;
$L__BB121_5:
	ld.u32 	%r26, [%SP+8];
	shl.b32 	%r27, %r26, 1;
	st.u32 	[%SP+8], %r27;
	ld.u32 	%r28, [%SP+16];
	shl.b32 	%r29, %r28, 1;
	st.u32 	[%SP+16], %r29;
	bra.uni 	$L__BB121_1;
$L__BB121_6:
	bra.uni 	$L__BB121_7;
$L__BB121_7:
	ld.u32 	%r9, [%SP+16];
	setp.eq.s32 	%p7, %r9, 0;
	@%p7 bra 	$L__BB121_11;
	bra.uni 	$L__BB121_8;
$L__BB121_8:
	ld.u32 	%r14, [%SP+4];
	ld.u32 	%r15, [%SP+8];
	setp.lt.u32 	%p9, %r14, %r15;
	@%p9 bra 	$L__BB121_10;
	bra.uni 	$L__BB121_9;
$L__BB121_9:
	ld.u32 	%r16, [%SP+8];
	ld.u32 	%r17, [%SP+4];
	sub.s32 	%r18, %r17, %r16;
	st.u32 	[%SP+4], %r18;
	ld.u32 	%r19, [%SP+16];
	ld.u32 	%r20, [%SP+20];
	or.b32  	%r21, %r20, %r19;
	st.u32 	[%SP+20], %r21;
	bra.uni 	$L__BB121_10;
$L__BB121_10:
	ld.u32 	%r22, [%SP+16];
	shr.u32 	%r23, %r22, 1;
	st.u32 	[%SP+16], %r23;
	ld.u32 	%r24, [%SP+8];
	shr.u32 	%r25, %r24, 1;
	st.u32 	[%SP+8], %r25;
	bra.uni 	$L__BB121_7;
$L__BB121_11:
	ld.u32 	%r10, [%SP+12];
	setp.eq.s32 	%p8, %r10, 0;
	@%p8 bra 	$L__BB121_13;
	bra.uni 	$L__BB121_12;
$L__BB121_12:
	ld.u32 	%r12, [%SP+4];
	st.u32 	[%SP+0], %r12;
	bra.uni 	$L__BB121_14;
$L__BB121_13:
	ld.u32 	%r11, [%SP+20];
	st.u32 	[%SP+0], %r11;
	bra.uni 	$L__BB121_14;
$L__BB121_14:
	ld.u32 	%r13, [%SP+0];
	st.param.b32 	[func_retval0+0], %r13;
	ret;

}
	// .globl	__mspabi_cmpf
.visible .func  (.param .b32 func_retval0) __mspabi_cmpf(
	.param .b32 __mspabi_cmpf_param_0,
	.param .b32 __mspabi_cmpf_param_1
)
{
	.local .align 4 .b8 	__local_depot122[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b32 	%r<5>;
	.reg .f32 	%f<7>;

	mov.u64 	%SPL, __local_depot122;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f2, [__mspabi_cmpf_param_1];
	ld.param.f32 	%f1, [__mspabi_cmpf_param_0];
	st.f32 	[%SP+4], %f1;
	st.f32 	[%SP+8], %f2;
	ld.f32 	%f3, [%SP+4];
	ld.f32 	%f4, [%SP+8];
	setp.geu.f32 	%p1, %f3, %f4;
	@%p1 bra 	$L__BB122_2;
	bra.uni 	$L__BB122_1;
$L__BB122_1:
	mov.b32 	%r3, -1;
	st.u32 	[%SP+0], %r3;
	bra.uni 	$L__BB122_5;
$L__BB122_2:
	ld.f32 	%f5, [%SP+4];
	ld.f32 	%f6, [%SP+8];
	setp.leu.f32 	%p2, %f5, %f6;
	@%p2 bra 	$L__BB122_4;
	bra.uni 	$L__BB122_3;
$L__BB122_3:
	mov.b32 	%r2, 1;
	st.u32 	[%SP+0], %r2;
	bra.uni 	$L__BB122_5;
$L__BB122_4:
	mov.b32 	%r1, 0;
	st.u32 	[%SP+0], %r1;
	bra.uni 	$L__BB122_5;
$L__BB122_5:
	ld.u32 	%r4, [%SP+0];
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	__mspabi_cmpd
.visible .func  (.param .b32 func_retval0) __mspabi_cmpd(
	.param .b64 __mspabi_cmpd_param_0,
	.param .b64 __mspabi_cmpd_param_1
)
{
	.local .align 8 .b8 	__local_depot123[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<7>;

	mov.u64 	%SPL, __local_depot123;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd2, [__mspabi_cmpd_param_1];
	ld.param.f64 	%fd1, [__mspabi_cmpd_param_0];
	st.f64 	[%SP+8], %fd1;
	st.f64 	[%SP+16], %fd2;
	ld.f64 	%fd3, [%SP+8];
	ld.f64 	%fd4, [%SP+16];
	setp.geu.f64 	%p1, %fd3, %fd4;
	@%p1 bra 	$L__BB123_2;
	bra.uni 	$L__BB123_1;
$L__BB123_1:
	mov.b32 	%r3, -1;
	st.u32 	[%SP+0], %r3;
	bra.uni 	$L__BB123_5;
$L__BB123_2:
	ld.f64 	%fd5, [%SP+8];
	ld.f64 	%fd6, [%SP+16];
	setp.leu.f64 	%p2, %fd5, %fd6;
	@%p2 bra 	$L__BB123_4;
	bra.uni 	$L__BB123_3;
$L__BB123_3:
	mov.b32 	%r2, 1;
	st.u32 	[%SP+0], %r2;
	bra.uni 	$L__BB123_5;
$L__BB123_4:
	mov.b32 	%r1, 0;
	st.u32 	[%SP+0], %r1;
	bra.uni 	$L__BB123_5;
$L__BB123_5:
	ld.u32 	%r4, [%SP+0];
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	__mspabi_mpysll
.visible .func  (.param .b64 func_retval0) __mspabi_mpysll(
	.param .b64 __mspabi_mpysll_param_0,
	.param .b64 __mspabi_mpysll_param_1
)
{
	.local .align 8 .b8 	__local_depot124[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<6>;

	mov.u64 	%SPL, __local_depot124;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [__mspabi_mpysll_param_1];
	ld.param.u64 	%rd1, [__mspabi_mpysll_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	ld.u64 	%rd4, [%SP+8];
	mul.lo.s64 	%rd5, %rd3, %rd4;
	st.param.b64 	[func_retval0+0], %rd5;
	ret;

}
	// .globl	__mspabi_mpyull
.visible .func  (.param .b64 func_retval0) __mspabi_mpyull(
	.param .b64 __mspabi_mpyull_param_0,
	.param .b64 __mspabi_mpyull_param_1
)
{
	.local .align 8 .b8 	__local_depot125[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<6>;

	mov.u64 	%SPL, __local_depot125;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [__mspabi_mpyull_param_1];
	ld.param.u64 	%rd1, [__mspabi_mpyull_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	ld.u64 	%rd4, [%SP+8];
	mul.lo.s64 	%rd5, %rd3, %rd4;
	st.param.b64 	[func_retval0+0], %rd5;
	ret;

}
	// .globl	__mulhi3
.visible .func  (.param .b32 func_retval0) __mulhi3(
	.param .b32 __mulhi3_param_0,
	.param .b32 __mulhi3_param_1
)
{
	.local .align 4 .b8 	__local_depot126[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<2>;

	mov.u64 	%SPL, __local_depot126;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r5, [__mulhi3_param_1];
	ld.param.u32 	%r4, [__mulhi3_param_0];
	st.u32 	[%SP+0], %r4;
	st.u32 	[%SP+4], %r5;
	mov.b32 	%r6, 0;
	st.u32 	[%SP+12], %r6;
	st.u32 	[%SP+16], %r6;
	ld.u32 	%r7, [%SP+4];
	setp.gt.s32 	%p3, %r7, -1;
	@%p3 bra 	$L__BB126_2;
	bra.uni 	$L__BB126_1;
$L__BB126_1:
	ld.u32 	%r8, [%SP+4];
	neg.s32 	%r9, %r8;
	st.u32 	[%SP+4], %r9;
	mov.b32 	%r10, 1;
	st.u32 	[%SP+12], %r10;
	bra.uni 	$L__BB126_2;
$L__BB126_2:
	mov.u16 	%rs1, 0;
	st.u8 	[%SP+8], %rs1;
	bra.uni 	$L__BB126_3;
$L__BB126_3:
	ld.u32 	%r11, [%SP+4];
	setp.eq.s32 	%p5, %r11, 0;
	mov.pred 	%p4, 0;
	mov.pred 	%p11, %p4;
	@%p5 bra 	$L__BB126_5;
	bra.uni 	$L__BB126_4;
$L__BB126_4:
	ld.s8 	%rd1, [%SP+8];
	setp.lt.u64 	%p1, %rd1, 32;
	mov.pred 	%p11, %p1;
	bra.uni 	$L__BB126_5;
$L__BB126_5:
	mov.pred 	%p2, %p11;
	@!%p2 bra 	$L__BB126_10;
	bra.uni 	$L__BB126_6;
$L__BB126_6:
	ld.u32 	%r14, [%SP+4];
	and.b32  	%r15, %r14, 1;
	setp.eq.b32 	%p7, %r15, 1;
	mov.pred 	%p8, 0;
	xor.pred  	%p9, %p7, %p8;
	not.pred 	%p10, %p9;
	@%p10 bra 	$L__BB126_8;
	bra.uni 	$L__BB126_7;
$L__BB126_7:
	ld.u32 	%r16, [%SP+0];
	ld.u32 	%r17, [%SP+16];
	add.s32 	%r18, %r17, %r16;
	st.u32 	[%SP+16], %r18;
	bra.uni 	$L__BB126_8;
$L__BB126_8:
	ld.u32 	%r19, [%SP+0];
	shl.b32 	%r20, %r19, 1;
	st.u32 	[%SP+0], %r20;
	ld.u32 	%r21, [%SP+4];
	shr.s32 	%r22, %r21, 1;
	st.u32 	[%SP+4], %r22;
	bra.uni 	$L__BB126_9;
$L__BB126_9:
	ld.u8 	%rs2, [%SP+8];
	add.s16 	%rs3, %rs2, 1;
	st.u8 	[%SP+8], %rs3;
	bra.uni 	$L__BB126_3;
$L__BB126_10:
	ld.u32 	%r12, [%SP+12];
	setp.eq.s32 	%p6, %r12, 0;
	@%p6 bra 	$L__BB126_12;
	bra.uni 	$L__BB126_11;
$L__BB126_11:
	ld.u32 	%r13, [%SP+16];
	neg.s32 	%r1, %r13;
	mov.u32 	%r23, %r1;
	bra.uni 	$L__BB126_13;
$L__BB126_12:
	ld.u32 	%r2, [%SP+16];
	mov.u32 	%r23, %r2;
	bra.uni 	$L__BB126_13;
$L__BB126_13:
	mov.u32 	%r3, %r23;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	__divsi3
.visible .func  (.param .b64 func_retval0) __divsi3(
	.param .b64 __divsi3_param_0,
	.param .b64 __divsi3_param_1
)
{
	.local .align 8 .b8 	__local_depot127[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<13>;

	mov.u64 	%SPL, __local_depot127;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [__divsi3_param_1];
	ld.param.u64 	%rd1, [__divsi3_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+16], %r1;
	ld.u64 	%rd3, [%SP+0];
	setp.gt.s64 	%p1, %rd3, -1;
	@%p1 bra 	$L__BB127_2;
	bra.uni 	$L__BB127_1;
$L__BB127_1:
	ld.u64 	%rd4, [%SP+0];
	neg.s64 	%rd5, %rd4;
	st.u64 	[%SP+0], %rd5;
	ld.u32 	%r2, [%SP+16];
	setp.eq.s32 	%p2, %r2, 0;
	selp.u32 	%r3, 1, 0, %p2;
	st.u32 	[%SP+16], %r3;
	bra.uni 	$L__BB127_2;
$L__BB127_2:
	ld.u64 	%rd6, [%SP+8];
	setp.gt.s64 	%p3, %rd6, -1;
	@%p3 bra 	$L__BB127_4;
	bra.uni 	$L__BB127_3;
$L__BB127_3:
	ld.u64 	%rd7, [%SP+8];
	neg.s64 	%rd8, %rd7;
	st.u64 	[%SP+8], %rd8;
	ld.u32 	%r4, [%SP+16];
	setp.eq.s32 	%p4, %r4, 0;
	selp.u32 	%r5, 1, 0, %p4;
	st.u32 	[%SP+16], %r5;
	bra.uni 	$L__BB127_4;
$L__BB127_4:
	ld.u32 	%r6, [%SP+0];
	ld.u32 	%r7, [%SP+8];
	{ // callseq 13, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r6;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r7;
	.param .b32 param2;
	st.param.b32 	[param2+0], 0;
	.param .b32 retval0;
	call.uni (retval0), 
	__udivmodsi4, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r8, [retval0+0];
	} // callseq 13
	cvt.u64.u32 	%rd9, %r8;
	st.u64 	[%SP+24], %rd9;
	ld.u32 	%r10, [%SP+16];
	setp.eq.s32 	%p5, %r10, 0;
	@%p5 bra 	$L__BB127_6;
	bra.uni 	$L__BB127_5;
$L__BB127_5:
	ld.u64 	%rd10, [%SP+24];
	neg.s64 	%rd11, %rd10;
	st.u64 	[%SP+24], %rd11;
	bra.uni 	$L__BB127_6;
$L__BB127_6:
	ld.u64 	%rd12, [%SP+24];
	st.param.b64 	[func_retval0+0], %rd12;
	ret;

}
	// .globl	__modsi3
.visible .func  (.param .b64 func_retval0) __modsi3(
	.param .b64 __modsi3_param_0,
	.param .b64 __modsi3_param_1
)
{
	.local .align 8 .b8 	__local_depot128[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<13>;

	mov.u64 	%SPL, __local_depot128;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [__modsi3_param_1];
	ld.param.u64 	%rd1, [__modsi3_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	mov.b32 	%r1, 0;
	st.u32 	[%SP+16], %r1;
	ld.u64 	%rd3, [%SP+0];
	setp.gt.s64 	%p1, %rd3, -1;
	@%p1 bra 	$L__BB128_2;
	bra.uni 	$L__BB128_1;
$L__BB128_1:
	ld.u64 	%rd4, [%SP+0];
	neg.s64 	%rd5, %rd4;
	st.u64 	[%SP+0], %rd5;
	mov.b32 	%r2, 1;
	st.u32 	[%SP+16], %r2;
	bra.uni 	$L__BB128_2;
$L__BB128_2:
	ld.u64 	%rd6, [%SP+8];
	setp.gt.s64 	%p2, %rd6, -1;
	@%p2 bra 	$L__BB128_4;
	bra.uni 	$L__BB128_3;
$L__BB128_3:
	ld.u64 	%rd7, [%SP+8];
	neg.s64 	%rd8, %rd7;
	st.u64 	[%SP+8], %rd8;
	bra.uni 	$L__BB128_4;
$L__BB128_4:
	ld.u32 	%r3, [%SP+0];
	ld.u32 	%r4, [%SP+8];
	{ // callseq 14, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r3;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r4;
	.param .b32 param2;
	st.param.b32 	[param2+0], 1;
	.param .b32 retval0;
	call.uni (retval0), 
	__udivmodsi4, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r5, [retval0+0];
	} // callseq 14
	cvt.u64.u32 	%rd9, %r5;
	st.u64 	[%SP+24], %rd9;
	ld.u32 	%r7, [%SP+16];
	setp.eq.s32 	%p3, %r7, 0;
	@%p3 bra 	$L__BB128_6;
	bra.uni 	$L__BB128_5;
$L__BB128_5:
	ld.u64 	%rd10, [%SP+24];
	neg.s64 	%rd11, %rd10;
	st.u64 	[%SP+24], %rd11;
	bra.uni 	$L__BB128_6;
$L__BB128_6:
	ld.u64 	%rd12, [%SP+24];
	st.param.b64 	[func_retval0+0], %rd12;
	ret;

}
	// .globl	__udivmodhi4
.visible .func  (.param .b32 func_retval0) __udivmodhi4(
	.param .b32 __udivmodhi4_param_0,
	.param .b32 __udivmodhi4_param_1,
	.param .b32 __udivmodhi4_param_2
)
{
	.local .align 4 .b8 	__local_depot129[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<13>;
	.reg .b32 	%r<20>;

	mov.u64 	%SPL, __local_depot129;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__udivmodhi4_param_2];
	ld.param.u16 	%rs2, [__udivmodhi4_param_1];
	ld.param.u16 	%rs1, [__udivmodhi4_param_0];
	st.u16 	[%SP+2], %rs1;
	st.u16 	[%SP+4], %rs2;
	st.u32 	[%SP+8], %r1;
	mov.u16 	%rs3, 1;
	st.u16 	[%SP+12], %rs3;
	mov.u16 	%rs4, 0;
	st.u16 	[%SP+14], %rs4;
	bra.uni 	$L__BB129_1;
$L__BB129_1:
	ld.u16 	%r2, [%SP+4];
	ld.u16 	%r3, [%SP+2];
	setp.ge.s32 	%p4, %r2, %r3;
	mov.pred 	%p3, 0;
	mov.pred 	%p10, %p3;
	@%p4 bra 	$L__BB129_4;
	bra.uni 	$L__BB129_2;
$L__BB129_2:
	ld.u16 	%rs5, [%SP+12];
	setp.eq.s16 	%p6, %rs5, 0;
	mov.pred 	%p5, 0;
	mov.pred 	%p10, %p5;
	@%p6 bra 	$L__BB129_4;
	bra.uni 	$L__BB129_3;
$L__BB129_3:
	ld.u16 	%r4, [%SP+4];
	and.b32  	%r5, %r4, 32768;
	setp.eq.s32 	%p1, %r5, 0;
	mov.pred 	%p10, %p1;
	bra.uni 	$L__BB129_4;
$L__BB129_4:
	mov.pred 	%p2, %p10;
	@!%p2 bra 	$L__BB129_6;
	bra.uni 	$L__BB129_5;
$L__BB129_5:
	ld.u16 	%rs9, [%SP+4];
	shl.b16 	%rs10, %rs9, 1;
	st.u16 	[%SP+4], %rs10;
	ld.u16 	%rs11, [%SP+12];
	shl.b16 	%rs12, %rs11, 1;
	st.u16 	[%SP+12], %rs12;
	bra.uni 	$L__BB129_1;
$L__BB129_6:
	bra.uni 	$L__BB129_7;
$L__BB129_7:
	ld.u16 	%rs6, [%SP+12];
	setp.eq.s16 	%p7, %rs6, 0;
	@%p7 bra 	$L__BB129_11;
	bra.uni 	$L__BB129_8;
$L__BB129_8:
	ld.u16 	%r8, [%SP+2];
	ld.u16 	%r9, [%SP+4];
	setp.lt.s32 	%p9, %r8, %r9;
	@%p9 bra 	$L__BB129_10;
	bra.uni 	$L__BB129_9;
$L__BB129_9:
	ld.u16 	%r10, [%SP+4];
	ld.u16 	%r11, [%SP+2];
	sub.s32 	%r12, %r11, %r10;
	st.u16 	[%SP+2], %r12;
	ld.u16 	%r13, [%SP+12];
	ld.u16 	%r14, [%SP+14];
	or.b32  	%r15, %r14, %r13;
	st.u16 	[%SP+14], %r15;
	bra.uni 	$L__BB129_10;
$L__BB129_10:
	ld.u16 	%r16, [%SP+12];
	shr.u32 	%r17, %r16, 1;
	st.u16 	[%SP+12], %r17;
	ld.u16 	%r18, [%SP+4];
	shr.u32 	%r19, %r18, 1;
	st.u16 	[%SP+4], %r19;
	bra.uni 	$L__BB129_7;
$L__BB129_11:
	ld.u32 	%r6, [%SP+8];
	setp.eq.s32 	%p8, %r6, 0;
	@%p8 bra 	$L__BB129_13;
	bra.uni 	$L__BB129_12;
$L__BB129_12:
	ld.u16 	%rs8, [%SP+2];
	st.u16 	[%SP+0], %rs8;
	bra.uni 	$L__BB129_14;
$L__BB129_13:
	ld.u16 	%rs7, [%SP+14];
	st.u16 	[%SP+0], %rs7;
	bra.uni 	$L__BB129_14;
$L__BB129_14:
	ld.u16 	%r7, [%SP+0];
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	__udivmodsi4_libgcc
.visible .func  (.param .b64 func_retval0) __udivmodsi4_libgcc(
	.param .b64 __udivmodsi4_libgcc_param_0,
	.param .b64 __udivmodsi4_libgcc_param_1,
	.param .b32 __udivmodsi4_libgcc_param_2
)
{
	.local .align 8 .b8 	__local_depot130[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<30>;

	mov.u64 	%SPL, __local_depot130;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__udivmodsi4_libgcc_param_2];
	ld.param.u64 	%rd2, [__udivmodsi4_libgcc_param_1];
	ld.param.u64 	%rd1, [__udivmodsi4_libgcc_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	st.u32 	[%SP+24], %r1;
	mov.u64 	%rd3, 1;
	st.u64 	[%SP+32], %rd3;
	mov.u64 	%rd4, 0;
	st.u64 	[%SP+40], %rd4;
	bra.uni 	$L__BB130_1;
$L__BB130_1:
	ld.u64 	%rd5, [%SP+16];
	ld.u64 	%rd6, [%SP+8];
	setp.ge.u64 	%p4, %rd5, %rd6;
	mov.pred 	%p3, 0;
	mov.pred 	%p10, %p3;
	@%p4 bra 	$L__BB130_4;
	bra.uni 	$L__BB130_2;
$L__BB130_2:
	ld.u64 	%rd7, [%SP+32];
	setp.eq.s64 	%p6, %rd7, 0;
	mov.pred 	%p5, 0;
	mov.pred 	%p10, %p5;
	@%p6 bra 	$L__BB130_4;
	bra.uni 	$L__BB130_3;
$L__BB130_3:
	add.u64 	%rd8, %SP, 16;
	or.b64  	%rd9, %rd8, 3;
	ld.u8 	%rs1, [%rd9];
	and.b16  	%rs2, %rs1, 128;
	setp.eq.s16 	%p1, %rs2, 0;
	mov.pred 	%p10, %p1;
	bra.uni 	$L__BB130_4;
$L__BB130_4:
	mov.pred 	%p2, %p10;
	@!%p2 bra 	$L__BB130_6;
	bra.uni 	$L__BB130_5;
$L__BB130_5:
	ld.u64 	%rd26, [%SP+16];
	shl.b64 	%rd27, %rd26, 1;
	st.u64 	[%SP+16], %rd27;
	ld.u64 	%rd28, [%SP+32];
	shl.b64 	%rd29, %rd28, 1;
	st.u64 	[%SP+32], %rd29;
	bra.uni 	$L__BB130_1;
$L__BB130_6:
	bra.uni 	$L__BB130_7;
$L__BB130_7:
	ld.u64 	%rd10, [%SP+32];
	setp.eq.s64 	%p7, %rd10, 0;
	@%p7 bra 	$L__BB130_11;
	bra.uni 	$L__BB130_8;
$L__BB130_8:
	ld.u64 	%rd14, [%SP+8];
	ld.u64 	%rd15, [%SP+16];
	setp.lt.u64 	%p9, %rd14, %rd15;
	@%p9 bra 	$L__BB130_10;
	bra.uni 	$L__BB130_9;
$L__BB130_9:
	ld.u64 	%rd16, [%SP+16];
	ld.u64 	%rd17, [%SP+8];
	sub.s64 	%rd18, %rd17, %rd16;
	st.u64 	[%SP+8], %rd18;
	ld.u64 	%rd19, [%SP+32];
	ld.u64 	%rd20, [%SP+40];
	or.b64  	%rd21, %rd20, %rd19;
	st.u64 	[%SP+40], %rd21;
	bra.uni 	$L__BB130_10;
$L__BB130_10:
	ld.u64 	%rd22, [%SP+32];
	shr.u64 	%rd23, %rd22, 1;
	st.u64 	[%SP+32], %rd23;
	ld.u64 	%rd24, [%SP+16];
	shr.u64 	%rd25, %rd24, 1;
	st.u64 	[%SP+16], %rd25;
	bra.uni 	$L__BB130_7;
$L__BB130_11:
	ld.u32 	%r2, [%SP+24];
	setp.eq.s32 	%p8, %r2, 0;
	@%p8 bra 	$L__BB130_13;
	bra.uni 	$L__BB130_12;
$L__BB130_12:
	ld.u64 	%rd12, [%SP+8];
	st.u64 	[%SP+0], %rd12;
	bra.uni 	$L__BB130_14;
$L__BB130_13:
	ld.u64 	%rd11, [%SP+40];
	st.u64 	[%SP+0], %rd11;
	bra.uni 	$L__BB130_14;
$L__BB130_14:
	ld.u64 	%rd13, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd13;
	ret;

}
	// .globl	__ashldi3
.visible .func  (.param .b64 func_retval0) __ashldi3(
	.param .b64 __ashldi3_param_0,
	.param .b32 __ashldi3_param_1
)
{
	.local .align 8 .b8 	__local_depot131[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<12>;

	mov.u64 	%SPL, __local_depot131;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__ashldi3_param_1];
	ld.param.u64 	%rd1, [__ashldi3_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u32 	[%SP+16], %r1;
	mov.b32 	%r2, 32;
	st.u32 	[%SP+20], %r2;
	ld.u64 	%rd2, [%SP+8];
	st.u64 	[%SP+24], %rd2;
	ld.u8 	%rs1, [%SP+16];
	and.b16  	%rs2, %rs1, 32;
	setp.eq.s16 	%p1, %rs2, 0;
	@%p1 bra 	$L__BB131_2;
	bra.uni 	$L__BB131_1;
$L__BB131_1:
	mov.b32 	%r15, 0;
	st.u32 	[%SP+32], %r15;
	ld.u32 	%r16, [%SP+24];
	ld.u32 	%r17, [%SP+16];
	add.s32 	%r18, %r17, -32;
	shl.b32 	%r19, %r16, %r18;
	add.u64 	%rd8, %SP, 32;
	or.b64  	%rd9, %rd8, 4;
	st.u32 	[%rd9], %r19;
	bra.uni 	$L__BB131_5;
$L__BB131_2:
	ld.u32 	%r3, [%SP+16];
	setp.ne.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB131_4;
	bra.uni 	$L__BB131_3;
$L__BB131_3:
	ld.u64 	%rd7, [%SP+8];
	st.u64 	[%SP+0], %rd7;
	bra.uni 	$L__BB131_6;
$L__BB131_4:
	ld.u32 	%r4, [%SP+24];
	ld.u32 	%r5, [%SP+16];
	shl.b32 	%r6, %r4, %r5;
	st.u32 	[%SP+32], %r6;
	add.u64 	%rd3, %SP, 24;
	or.b64  	%rd4, %rd3, 4;
	ld.u32 	%r7, [%rd4];
	ld.u32 	%r8, [%SP+16];
	shl.b32 	%r9, %r7, %r8;
	ld.u32 	%r10, [%SP+24];
	mov.b32 	%r11, 32;
	sub.s32 	%r12, %r11, %r8;
	shr.u32 	%r13, %r10, %r12;
	or.b32  	%r14, %r9, %r13;
	add.u64 	%rd5, %SP, 32;
	or.b64  	%rd6, %rd5, 4;
	st.u32 	[%rd6], %r14;
	bra.uni 	$L__BB131_5;
$L__BB131_5:
	ld.u64 	%rd10, [%SP+32];
	st.u64 	[%SP+0], %rd10;
	bra.uni 	$L__BB131_6;
$L__BB131_6:
	ld.u64 	%rd11, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd11;
	ret;

}
	// .globl	__ashlti3
.visible .func  (.param .align 16 .b8 func_retval0[16]) __ashlti3(
	.param .align 16 .b8 __ashlti3_param_0[16],
	.param .b32 __ashlti3_param_1
)
{
	.local .align 16 .b8 	__local_depot132[80];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<41>;

	mov.u64 	%SPL, __local_depot132;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd1, %rd2}, [__ashlti3_param_0];
	ld.param.u32 	%r1, [__ashlti3_param_1];
	add.u64 	%rd3, %SP, 16;
	or.b64  	%rd4, %rd3, 8;
	st.u64 	[%rd4], %rd2;
	st.u64 	[%SP+16], %rd1;
	st.u32 	[%SP+32], %r1;
	mov.b32 	%r2, 64;
	st.u32 	[%SP+36], %r2;
	ld.u64 	%rd5, [%rd4];
	ld.u64 	%rd6, [%SP+16];
	add.u64 	%rd7, %SP, 48;
	or.b64  	%rd8, %rd7, 8;
	st.u64 	[%rd8], %rd5;
	st.u64 	[%SP+48], %rd6;
	ld.u8 	%rs1, [%SP+32];
	and.b16  	%rs2, %rs1, 64;
	setp.eq.s16 	%p1, %rs2, 0;
	@%p1 bra 	$L__BB132_2;
	bra.uni 	$L__BB132_1;
$L__BB132_1:
	mov.u64 	%rd26, 0;
	st.u64 	[%SP+64], %rd26;
	ld.u64 	%rd27, [%SP+48];
	ld.u32 	%r8, [%SP+32];
	add.s32 	%r9, %r8, -64;
	shl.b64 	%rd28, %rd27, %r9;
	add.u64 	%rd29, %SP, 64;
	or.b64  	%rd30, %rd29, 8;
	st.u64 	[%rd30], %rd28;
	bra.uni 	$L__BB132_5;
$L__BB132_2:
	ld.u32 	%r3, [%SP+32];
	setp.ne.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB132_4;
	bra.uni 	$L__BB132_3;
$L__BB132_3:
	add.u64 	%rd20, %SP, 16;
	or.b64  	%rd21, %rd20, 8;
	ld.u64 	%rd22, [%rd21];
	ld.u64 	%rd23, [%SP+16];
	add.u64 	%rd24, %SP, 0;
	or.b64  	%rd25, %rd24, 8;
	st.u64 	[%rd25], %rd22;
	st.u64 	[%SP+0], %rd23;
	bra.uni 	$L__BB132_6;
$L__BB132_4:
	ld.u64 	%rd9, [%SP+48];
	ld.u32 	%r4, [%SP+32];
	shl.b64 	%rd10, %rd9, %r4;
	st.u64 	[%SP+64], %rd10;
	add.u64 	%rd11, %SP, 48;
	or.b64  	%rd12, %rd11, 8;
	ld.u64 	%rd13, [%rd12];
	ld.u32 	%r5, [%SP+32];
	shl.b64 	%rd14, %rd13, %r5;
	ld.u64 	%rd15, [%SP+48];
	mov.b32 	%r6, 64;
	sub.s32 	%r7, %r6, %r5;
	shr.u64 	%rd16, %rd15, %r7;
	or.b64  	%rd17, %rd14, %rd16;
	add.u64 	%rd18, %SP, 64;
	or.b64  	%rd19, %rd18, 8;
	st.u64 	[%rd19], %rd17;
	bra.uni 	$L__BB132_5;
$L__BB132_5:
	add.u64 	%rd31, %SP, 64;
	or.b64  	%rd32, %rd31, 8;
	ld.u64 	%rd33, [%rd32];
	ld.u64 	%rd34, [%SP+64];
	add.u64 	%rd35, %SP, 0;
	or.b64  	%rd36, %rd35, 8;
	st.u64 	[%rd36], %rd33;
	st.u64 	[%SP+0], %rd34;
	bra.uni 	$L__BB132_6;
$L__BB132_6:
	add.u64 	%rd37, %SP, 0;
	or.b64  	%rd38, %rd37, 8;
	ld.u64 	%rd39, [%rd38];
	ld.u64 	%rd40, [%SP+0];
	st.param.v2.b64 	[func_retval0+0], {%rd40, %rd39};
	ret;

}
	// .globl	__ashrdi3
.visible .func  (.param .b64 func_retval0) __ashrdi3(
	.param .b64 __ashrdi3_param_0,
	.param .b32 __ashrdi3_param_1
)
{
	.local .align 8 .b8 	__local_depot133[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<14>;

	mov.u64 	%SPL, __local_depot133;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__ashrdi3_param_1];
	ld.param.u64 	%rd1, [__ashrdi3_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u32 	[%SP+16], %r1;
	mov.b32 	%r2, 32;
	st.u32 	[%SP+20], %r2;
	ld.u64 	%rd2, [%SP+8];
	st.u64 	[%SP+24], %rd2;
	ld.u8 	%rs1, [%SP+16];
	and.b16  	%rs2, %rs1, 32;
	setp.eq.s16 	%p1, %rs2, 0;
	@%p1 bra 	$L__BB133_2;
	bra.uni 	$L__BB133_1;
$L__BB133_1:
	add.u64 	%rd8, %SP, 24;
	or.b64  	%rd9, %rd8, 4;
	ld.u32 	%r15, [%rd9];
	shr.s32 	%r16, %r15, 31;
	add.u64 	%rd10, %SP, 32;
	or.b64  	%rd11, %rd10, 4;
	st.u32 	[%rd11], %r16;
	ld.u32 	%r17, [%rd9];
	ld.u32 	%r18, [%SP+16];
	add.s32 	%r19, %r18, -32;
	shr.s32 	%r20, %r17, %r19;
	st.u32 	[%SP+32], %r20;
	bra.uni 	$L__BB133_5;
$L__BB133_2:
	ld.u32 	%r3, [%SP+16];
	setp.ne.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB133_4;
	bra.uni 	$L__BB133_3;
$L__BB133_3:
	ld.u64 	%rd7, [%SP+8];
	st.u64 	[%SP+0], %rd7;
	bra.uni 	$L__BB133_6;
$L__BB133_4:
	add.u64 	%rd3, %SP, 24;
	or.b64  	%rd4, %rd3, 4;
	ld.u32 	%r4, [%rd4];
	ld.u32 	%r5, [%SP+16];
	shr.s32 	%r6, %r4, %r5;
	add.u64 	%rd5, %SP, 32;
	or.b64  	%rd6, %rd5, 4;
	st.u32 	[%rd6], %r6;
	ld.u32 	%r7, [%rd4];
	ld.u32 	%r8, [%SP+16];
	mov.b32 	%r9, 32;
	sub.s32 	%r10, %r9, %r8;
	shl.b32 	%r11, %r7, %r10;
	ld.u32 	%r12, [%SP+24];
	shr.u32 	%r13, %r12, %r8;
	or.b32  	%r14, %r11, %r13;
	st.u32 	[%SP+32], %r14;
	bra.uni 	$L__BB133_5;
$L__BB133_5:
	ld.u64 	%rd12, [%SP+32];
	st.u64 	[%SP+0], %rd12;
	bra.uni 	$L__BB133_6;
$L__BB133_6:
	ld.u64 	%rd13, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd13;
	ret;

}
	// .globl	__ashrti3
.visible .func  (.param .align 16 .b8 func_retval0[16]) __ashrti3(
	.param .align 16 .b8 __ashrti3_param_0[16],
	.param .b32 __ashrti3_param_1
)
{
	.local .align 16 .b8 	__local_depot134[80];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<44>;

	mov.u64 	%SPL, __local_depot134;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd1, %rd2}, [__ashrti3_param_0];
	ld.param.u32 	%r1, [__ashrti3_param_1];
	add.u64 	%rd3, %SP, 16;
	or.b64  	%rd4, %rd3, 8;
	st.u64 	[%rd4], %rd2;
	st.u64 	[%SP+16], %rd1;
	st.u32 	[%SP+32], %r1;
	mov.b32 	%r2, 64;
	st.u32 	[%SP+36], %r2;
	ld.u64 	%rd5, [%rd4];
	ld.u64 	%rd6, [%SP+16];
	add.u64 	%rd7, %SP, 48;
	or.b64  	%rd8, %rd7, 8;
	st.u64 	[%rd8], %rd5;
	st.u64 	[%SP+48], %rd6;
	ld.u8 	%rs1, [%SP+32];
	and.b16  	%rs2, %rs1, 64;
	setp.eq.s16 	%p1, %rs2, 0;
	@%p1 bra 	$L__BB134_2;
	bra.uni 	$L__BB134_1;
$L__BB134_1:
	add.u64 	%rd26, %SP, 48;
	or.b64  	%rd27, %rd26, 8;
	ld.u64 	%rd28, [%rd27];
	shr.s64 	%rd29, %rd28, 63;
	add.u64 	%rd30, %SP, 64;
	or.b64  	%rd31, %rd30, 8;
	st.u64 	[%rd31], %rd29;
	ld.u64 	%rd32, [%rd27];
	ld.u32 	%r8, [%SP+32];
	add.s32 	%r9, %r8, -64;
	shr.s64 	%rd33, %rd32, %r9;
	st.u64 	[%SP+64], %rd33;
	bra.uni 	$L__BB134_5;
$L__BB134_2:
	ld.u32 	%r3, [%SP+32];
	setp.ne.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB134_4;
	bra.uni 	$L__BB134_3;
$L__BB134_3:
	add.u64 	%rd20, %SP, 16;
	or.b64  	%rd21, %rd20, 8;
	ld.u64 	%rd22, [%rd21];
	ld.u64 	%rd23, [%SP+16];
	add.u64 	%rd24, %SP, 0;
	or.b64  	%rd25, %rd24, 8;
	st.u64 	[%rd25], %rd22;
	st.u64 	[%SP+0], %rd23;
	bra.uni 	$L__BB134_6;
$L__BB134_4:
	add.u64 	%rd9, %SP, 48;
	or.b64  	%rd10, %rd9, 8;
	ld.u64 	%rd11, [%rd10];
	ld.u32 	%r4, [%SP+32];
	shr.s64 	%rd12, %rd11, %r4;
	add.u64 	%rd13, %SP, 64;
	or.b64  	%rd14, %rd13, 8;
	st.u64 	[%rd14], %rd12;
	ld.u64 	%rd15, [%rd10];
	ld.u32 	%r5, [%SP+32];
	mov.b32 	%r6, 64;
	sub.s32 	%r7, %r6, %r5;
	shl.b64 	%rd16, %rd15, %r7;
	ld.u64 	%rd17, [%SP+48];
	shr.u64 	%rd18, %rd17, %r5;
	or.b64  	%rd19, %rd16, %rd18;
	st.u64 	[%SP+64], %rd19;
	bra.uni 	$L__BB134_5;
$L__BB134_5:
	add.u64 	%rd34, %SP, 64;
	or.b64  	%rd35, %rd34, 8;
	ld.u64 	%rd36, [%rd35];
	ld.u64 	%rd37, [%SP+64];
	add.u64 	%rd38, %SP, 0;
	or.b64  	%rd39, %rd38, 8;
	st.u64 	[%rd39], %rd36;
	st.u64 	[%SP+0], %rd37;
	bra.uni 	$L__BB134_6;
$L__BB134_6:
	add.u64 	%rd40, %SP, 0;
	or.b64  	%rd41, %rd40, 8;
	ld.u64 	%rd42, [%rd41];
	ld.u64 	%rd43, [%SP+0];
	st.param.v2.b64 	[func_retval0+0], {%rd43, %rd42};
	ret;

}
	// .globl	__bswapdi2
.visible .func  (.param .b64 func_retval0) __bswapdi2(
	.param .b64 __bswapdi2_param_0
)
{
	.local .align 8 .b8 	__local_depot135[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<24>;

	mov.u64 	%SPL, __local_depot135;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [__bswapdi2_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	shr.u64 	%rd3, %rd2, 56;
	and.b64  	%rd4, %rd2, 71776119061217280;
	shr.u64 	%rd5, %rd4, 40;
	or.b64  	%rd6, %rd3, %rd5;
	and.b64  	%rd7, %rd2, 280375465082880;
	shr.u64 	%rd8, %rd7, 24;
	or.b64  	%rd9, %rd6, %rd8;
	and.b64  	%rd10, %rd2, 1095216660480;
	shr.u64 	%rd11, %rd10, 8;
	or.b64  	%rd12, %rd9, %rd11;
	and.b64  	%rd13, %rd2, 4278190080;
	shl.b64 	%rd14, %rd13, 8;
	or.b64  	%rd15, %rd12, %rd14;
	and.b64  	%rd16, %rd2, 16711680;
	shl.b64 	%rd17, %rd16, 24;
	or.b64  	%rd18, %rd15, %rd17;
	and.b64  	%rd19, %rd2, 65280;
	shl.b64 	%rd20, %rd19, 40;
	or.b64  	%rd21, %rd18, %rd20;
	shl.b64 	%rd22, %rd2, 56;
	or.b64  	%rd23, %rd21, %rd22;
	st.param.b64 	[func_retval0+0], %rd23;
	ret;

}
	// .globl	__bswapsi2
.visible .func  (.param .b32 func_retval0) __bswapsi2(
	.param .b32 __bswapsi2_param_0
)
{
	.local .align 4 .b8 	__local_depot136[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<12>;

	mov.u64 	%SPL, __local_depot136;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__bswapsi2_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	shr.u32 	%r3, %r2, 24;
	and.b32  	%r4, %r2, 16711680;
	shr.u32 	%r5, %r4, 8;
	or.b32  	%r6, %r3, %r5;
	and.b32  	%r7, %r2, 65280;
	shl.b32 	%r8, %r7, 8;
	or.b32  	%r9, %r6, %r8;
	shl.b32 	%r10, %r2, 24;
	or.b32  	%r11, %r9, %r10;
	st.param.b32 	[func_retval0+0], %r11;
	ret;

}
	// .globl	__clzsi2
.visible .func  (.param .b32 func_retval0) __clzsi2(
	.param .b32 __clzsi2_param_0
)
{
	.local .align 4 .b8 	__local_depot137[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<4>;

	mov.u64 	%SPL, __local_depot137;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__clzsi2_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+4], %r2;
	add.u64 	%rd1, %SP, 4;
	or.b64  	%rd2, %rd1, 2;
	ld.u16 	%r3, [%rd2];
	shl.b32 	%r4, %r3, 16;
	setp.eq.s32 	%p1, %r4, 0;
	selp.u32 	%r5, 1, 0, %p1;
	shl.b32 	%r6, %r5, 4;
	st.u32 	[%SP+8], %r6;
	ld.u32 	%r7, [%SP+8];
	mov.b32 	%r8, 16;
	sub.s32 	%r9, %r8, %r7;
	ld.u32 	%r10, [%SP+4];
	shr.u32 	%r11, %r10, %r9;
	st.u32 	[%SP+4], %r11;
	ld.u32 	%r12, [%SP+8];
	st.u32 	[%SP+12], %r12;
	or.b64  	%rd3, %rd1, 1;
	ld.u8 	%r13, [%rd3];
	shl.b32 	%r14, %r13, 8;
	setp.eq.s32 	%p2, %r14, 0;
	selp.u32 	%r15, 1, 0, %p2;
	shl.b32 	%r16, %r15, 3;
	st.u32 	[%SP+8], %r16;
	ld.u32 	%r17, [%SP+8];
	mov.b32 	%r18, 8;
	sub.s32 	%r19, %r18, %r17;
	ld.u32 	%r20, [%SP+4];
	shr.u32 	%r21, %r20, %r19;
	st.u32 	[%SP+4], %r21;
	ld.u32 	%r22, [%SP+8];
	ld.u32 	%r23, [%SP+12];
	add.s32 	%r24, %r23, %r22;
	st.u32 	[%SP+12], %r24;
	ld.u32 	%r25, [%SP+4];
	and.b32  	%r26, %r25, 240;
	setp.eq.s32 	%p3, %r26, 0;
	selp.u32 	%r27, 1, 0, %p3;
	shl.b32 	%r28, %r27, 2;
	st.u32 	[%SP+8], %r28;
	ld.u32 	%r29, [%SP+8];
	mov.b32 	%r30, 4;
	sub.s32 	%r31, %r30, %r29;
	ld.u32 	%r32, [%SP+4];
	shr.u32 	%r33, %r32, %r31;
	st.u32 	[%SP+4], %r33;
	ld.u32 	%r34, [%SP+8];
	ld.u32 	%r35, [%SP+12];
	add.s32 	%r36, %r35, %r34;
	st.u32 	[%SP+12], %r36;
	ld.u32 	%r37, [%SP+4];
	and.b32  	%r38, %r37, 12;
	setp.eq.s32 	%p4, %r38, 0;
	selp.u32 	%r39, 1, 0, %p4;
	shl.b32 	%r40, %r39, 1;
	st.u32 	[%SP+8], %r40;
	ld.u32 	%r41, [%SP+8];
	mov.b32 	%r42, 2;
	sub.s32 	%r43, %r42, %r41;
	ld.u32 	%r44, [%SP+4];
	shr.u32 	%r45, %r44, %r43;
	st.u32 	[%SP+4], %r45;
	ld.u32 	%r46, [%SP+8];
	ld.u32 	%r47, [%SP+12];
	add.s32 	%r48, %r47, %r46;
	st.u32 	[%SP+12], %r48;
	ld.u32 	%r49, [%SP+12];
	ld.u32 	%r50, [%SP+4];
	sub.s32 	%r51, %r42, %r50;
	and.b32  	%r52, %r50, 2;
	setp.eq.s32 	%p5, %r52, 0;
	selp.s32 	%r53, -1, 0, %p5;
	and.b32  	%r54, %r51, %r53;
	add.s32 	%r55, %r49, %r54;
	st.param.b32 	[func_retval0+0], %r55;
	ret;

}
	// .globl	__clzti2
.visible .func  (.param .b32 func_retval0) __clzti2(
	.param .align 16 .b8 __clzti2_param_0[16]
)
{
	.local .align 16 .b8 	__local_depot138[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<18>;

	mov.u64 	%SPL, __local_depot138;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd1, %rd2}, [__clzti2_param_0];
	add.u64 	%rd3, %SP, 0;
	or.b64  	%rd4, %rd3, 8;
	st.u64 	[%rd4], %rd2;
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd5, [%rd4];
	ld.u64 	%rd6, [%SP+0];
	add.u64 	%rd7, %SP, 16;
	or.b64  	%rd8, %rd7, 8;
	st.u64 	[%rd8], %rd5;
	st.u64 	[%SP+16], %rd6;
	ld.u64 	%rd9, [%rd8];
	setp.eq.s64 	%p1, %rd9, 0;
	selp.s64 	%rd10, -1, 0, %p1;
	st.u64 	[%SP+32], %rd10;
	ld.u64 	%rd11, [%rd8];
	ld.u64 	%rd12, [%SP+32];
	not.b64 	%rd13, %rd12;
	and.b64  	%rd14, %rd11, %rd13;
	ld.u64 	%rd15, [%SP+16];
	and.b64  	%rd16, %rd15, %rd12;
	or.b64  	%rd17, %rd14, %rd16;
	clz.b64 	%r1, %rd17;
	cvt.u32.u64 	%r2, %rd12;
	and.b32  	%r3, %r2, 64;
	add.s32 	%r4, %r1, %r3;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	__cmpdi2
.visible .func  (.param .b32 func_retval0) __cmpdi2(
	.param .b64 __cmpdi2_param_0,
	.param .b64 __cmpdi2_param_1
)
{
	.local .align 8 .b8 	__local_depot139[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<13>;

	mov.u64 	%SPL, __local_depot139;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [__cmpdi2_param_1];
	ld.param.u64 	%rd1, [__cmpdi2_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	ld.u64 	%rd3, [%SP+8];
	st.u64 	[%SP+24], %rd3;
	ld.u64 	%rd4, [%SP+16];
	st.u64 	[%SP+32], %rd4;
	add.u64 	%rd5, %SP, 24;
	or.b64  	%rd6, %rd5, 4;
	ld.u32 	%r1, [%rd6];
	add.u64 	%rd7, %SP, 32;
	or.b64  	%rd8, %rd7, 4;
	ld.u32 	%r2, [%rd8];
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB139_2;
	bra.uni 	$L__BB139_1;
$L__BB139_1:
	mov.b32 	%r13, 0;
	st.u32 	[%SP+0], %r13;
	bra.uni 	$L__BB139_9;
$L__BB139_2:
	add.u64 	%rd9, %SP, 24;
	or.b64  	%rd10, %rd9, 4;
	ld.u32 	%r3, [%rd10];
	add.u64 	%rd11, %SP, 32;
	or.b64  	%rd12, %rd11, 4;
	ld.u32 	%r4, [%rd12];
	setp.le.s32 	%p2, %r3, %r4;
	@%p2 bra 	$L__BB139_4;
	bra.uni 	$L__BB139_3;
$L__BB139_3:
	mov.b32 	%r12, 2;
	st.u32 	[%SP+0], %r12;
	bra.uni 	$L__BB139_9;
$L__BB139_4:
	ld.u32 	%r5, [%SP+24];
	ld.u32 	%r6, [%SP+32];
	setp.ge.u32 	%p3, %r5, %r6;
	@%p3 bra 	$L__BB139_6;
	bra.uni 	$L__BB139_5;
$L__BB139_5:
	mov.b32 	%r11, 0;
	st.u32 	[%SP+0], %r11;
	bra.uni 	$L__BB139_9;
$L__BB139_6:
	ld.u32 	%r7, [%SP+24];
	ld.u32 	%r8, [%SP+32];
	setp.le.u32 	%p4, %r7, %r8;
	@%p4 bra 	$L__BB139_8;
	bra.uni 	$L__BB139_7;
$L__BB139_7:
	mov.b32 	%r10, 2;
	st.u32 	[%SP+0], %r10;
	bra.uni 	$L__BB139_9;
$L__BB139_8:
	mov.b32 	%r9, 1;
	st.u32 	[%SP+0], %r9;
	bra.uni 	$L__BB139_9;
$L__BB139_9:
	ld.u32 	%r14, [%SP+0];
	st.param.b32 	[func_retval0+0], %r14;
	ret;

}
	// .globl	__aeabi_lcmp
.visible .func  (.param .b32 func_retval0) __aeabi_lcmp(
	.param .b64 __aeabi_lcmp_param_0,
	.param .b64 __aeabi_lcmp_param_1
)
{
	.local .align 8 .b8 	__local_depot140[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<5>;

	mov.u64 	%SPL, __local_depot140;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [__aeabi_lcmp_param_1];
	ld.param.u64 	%rd1, [__aeabi_lcmp_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	ld.u64 	%rd4, [%SP+8];
	{ // callseq 15, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd3;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd4;
	.param .b32 retval0;
	call.uni (retval0), 
	__cmpdi2, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1, [retval0+0];
	} // callseq 15
	add.s32 	%r3, %r1, -1;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	__cmpti2
.visible .func  (.param .b32 func_retval0) __cmpti2(
	.param .align 16 .b8 __cmpti2_param_0[16],
	.param .align 16 .b8 __cmpti2_param_1[16]
)
{
	.local .align 16 .b8 	__local_depot141[80];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<29>;

	mov.u64 	%SPL, __local_depot141;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd3, %rd4}, [__cmpti2_param_1];
	ld.param.v2.u64 	{%rd1, %rd2}, [__cmpti2_param_0];
	add.u64 	%rd5, %SP, 16;
	or.b64  	%rd6, %rd5, 8;
	st.u64 	[%rd6], %rd2;
	st.u64 	[%SP+16], %rd1;
	add.u64 	%rd7, %SP, 32;
	or.b64  	%rd8, %rd7, 8;
	st.u64 	[%rd8], %rd4;
	st.u64 	[%SP+32], %rd3;
	ld.u64 	%rd9, [%rd6];
	ld.u64 	%rd10, [%SP+16];
	add.u64 	%rd11, %SP, 48;
	or.b64  	%rd12, %rd11, 8;
	st.u64 	[%rd12], %rd9;
	st.u64 	[%SP+48], %rd10;
	ld.u64 	%rd13, [%rd8];
	ld.u64 	%rd14, [%SP+32];
	add.u64 	%rd15, %SP, 64;
	or.b64  	%rd16, %rd15, 8;
	st.u64 	[%rd16], %rd13;
	st.u64 	[%SP+64], %rd14;
	ld.u64 	%rd17, [%rd12];
	ld.u64 	%rd18, [%rd16];
	setp.ge.s64 	%p1, %rd17, %rd18;
	@%p1 bra 	$L__BB141_2;
	bra.uni 	$L__BB141_1;
$L__BB141_1:
	mov.b32 	%r5, 0;
	st.u32 	[%SP+0], %r5;
	bra.uni 	$L__BB141_9;
$L__BB141_2:
	add.u64 	%rd19, %SP, 48;
	or.b64  	%rd20, %rd19, 8;
	ld.u64 	%rd21, [%rd20];
	add.u64 	%rd22, %SP, 64;
	or.b64  	%rd23, %rd22, 8;
	ld.u64 	%rd24, [%rd23];
	setp.le.s64 	%p2, %rd21, %rd24;
	@%p2 bra 	$L__BB141_4;
	bra.uni 	$L__BB141_3;
$L__BB141_3:
	mov.b32 	%r4, 2;
	st.u32 	[%SP+0], %r4;
	bra.uni 	$L__BB141_9;
$L__BB141_4:
	ld.u64 	%rd25, [%SP+48];
	ld.u64 	%rd26, [%SP+64];
	setp.ge.u64 	%p3, %rd25, %rd26;
	@%p3 bra 	$L__BB141_6;
	bra.uni 	$L__BB141_5;
$L__BB141_5:
	mov.b32 	%r3, 0;
	st.u32 	[%SP+0], %r3;
	bra.uni 	$L__BB141_9;
$L__BB141_6:
	ld.u64 	%rd27, [%SP+48];
	ld.u64 	%rd28, [%SP+64];
	setp.le.u64 	%p4, %rd27, %rd28;
	@%p4 bra 	$L__BB141_8;
	bra.uni 	$L__BB141_7;
$L__BB141_7:
	mov.b32 	%r2, 2;
	st.u32 	[%SP+0], %r2;
	bra.uni 	$L__BB141_9;
$L__BB141_8:
	mov.b32 	%r1, 1;
	st.u32 	[%SP+0], %r1;
	bra.uni 	$L__BB141_9;
$L__BB141_9:
	ld.u32 	%r6, [%SP+0];
	st.param.b32 	[func_retval0+0], %r6;
	ret;

}
	// .globl	__ctzsi2
.visible .func  (.param .b32 func_retval0) __ctzsi2(
	.param .b32 __ctzsi2_param_0
)
{
	.local .align 4 .b8 	__local_depot142[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<49>;

	mov.u64 	%SPL, __local_depot142;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__ctzsi2_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+4], %r2;
	ld.u16 	%rs1, [%SP+4];
	setp.eq.s16 	%p1, %rs1, 0;
	selp.u32 	%r3, 1, 0, %p1;
	shl.b32 	%r4, %r3, 4;
	st.u32 	[%SP+8], %r4;
	ld.u32 	%r5, [%SP+8];
	ld.u32 	%r6, [%SP+4];
	shr.u32 	%r7, %r6, %r5;
	st.u32 	[%SP+4], %r7;
	ld.u32 	%r8, [%SP+8];
	st.u32 	[%SP+12], %r8;
	ld.u8 	%r9, [%SP+4];
	setp.eq.s32 	%p2, %r9, 0;
	selp.u32 	%r10, 1, 0, %p2;
	shl.b32 	%r11, %r10, 3;
	st.u32 	[%SP+8], %r11;
	ld.u32 	%r12, [%SP+8];
	ld.u32 	%r13, [%SP+4];
	shr.u32 	%r14, %r13, %r12;
	st.u32 	[%SP+4], %r14;
	ld.u32 	%r15, [%SP+8];
	ld.u32 	%r16, [%SP+12];
	add.s32 	%r17, %r16, %r15;
	st.u32 	[%SP+12], %r17;
	ld.u32 	%r18, [%SP+4];
	and.b32  	%r19, %r18, 15;
	setp.eq.s32 	%p3, %r19, 0;
	selp.u32 	%r20, 1, 0, %p3;
	shl.b32 	%r21, %r20, 2;
	st.u32 	[%SP+8], %r21;
	ld.u32 	%r22, [%SP+8];
	ld.u32 	%r23, [%SP+4];
	shr.u32 	%r24, %r23, %r22;
	st.u32 	[%SP+4], %r24;
	ld.u32 	%r25, [%SP+8];
	ld.u32 	%r26, [%SP+12];
	add.s32 	%r27, %r26, %r25;
	st.u32 	[%SP+12], %r27;
	ld.u32 	%r28, [%SP+4];
	and.b32  	%r29, %r28, 3;
	setp.eq.s32 	%p4, %r29, 0;
	selp.u32 	%r30, 1, 0, %p4;
	shl.b32 	%r31, %r30, 1;
	st.u32 	[%SP+8], %r31;
	ld.u32 	%r32, [%SP+8];
	ld.u32 	%r33, [%SP+4];
	shr.u32 	%r34, %r33, %r32;
	st.u32 	[%SP+4], %r34;
	ld.u32 	%r35, [%SP+4];
	and.b32  	%r36, %r35, 3;
	st.u32 	[%SP+4], %r36;
	ld.u32 	%r37, [%SP+8];
	ld.u32 	%r38, [%SP+12];
	add.s32 	%r39, %r38, %r37;
	st.u32 	[%SP+12], %r39;
	ld.u32 	%r40, [%SP+12];
	ld.u32 	%r41, [%SP+4];
	shr.u32 	%r42, %r41, 1;
	mov.b32 	%r43, 2;
	sub.s32 	%r44, %r43, %r42;
	and.b32  	%r45, %r41, 1;
	add.s32 	%r46, %r45, -1;
	and.b32  	%r47, %r44, %r46;
	add.s32 	%r48, %r40, %r47;
	st.param.b32 	[func_retval0+0], %r48;
	ret;

}
	// .globl	__ctzti2
.visible .func  (.param .b32 func_retval0) __ctzti2(
	.param .align 16 .b8 __ctzti2_param_0[16]
)
{
	.local .align 16 .b8 	__local_depot143[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<21>;

	mov.u64 	%SPL, __local_depot143;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd1, %rd2}, [__ctzti2_param_0];
	add.u64 	%rd3, %SP, 0;
	or.b64  	%rd4, %rd3, 8;
	st.u64 	[%rd4], %rd2;
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd5, [%rd4];
	ld.u64 	%rd6, [%SP+0];
	add.u64 	%rd7, %SP, 16;
	or.b64  	%rd8, %rd7, 8;
	st.u64 	[%rd8], %rd5;
	st.u64 	[%SP+16], %rd6;
	ld.u64 	%rd9, [%SP+16];
	setp.eq.s64 	%p1, %rd9, 0;
	selp.s64 	%rd10, -1, 0, %p1;
	st.u64 	[%SP+32], %rd10;
	ld.u64 	%rd11, [%rd8];
	ld.u64 	%rd12, [%SP+32];
	and.b64  	%rd13, %rd11, %rd12;
	ld.u64 	%rd14, [%SP+16];
	not.b64 	%rd15, %rd12;
	and.b64  	%rd16, %rd14, %rd15;
	or.b64  	%rd17, %rd13, %rd16;
	add.s64 	%rd18, %rd17, -1;
	not.b64 	%rd19, %rd17;
	and.b64  	%rd20, %rd19, %rd18;
	popc.b64 	%r1, %rd20;
	cvt.u32.u64 	%r2, %rd12;
	and.b32  	%r3, %r2, 64;
	add.s32 	%r4, %r1, %r3;
	st.param.b32 	[func_retval0+0], %r4;
	ret;

}
	// .globl	__ffsti2
.visible .func  (.param .b32 func_retval0) __ffsti2(
	.param .align 16 .b8 __ffsti2_param_0[16]
)
{
	.local .align 16 .b8 	__local_depot144[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<25>;

	mov.u64 	%SPL, __local_depot144;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd1, %rd2}, [__ffsti2_param_0];
	add.u64 	%rd3, %SP, 16;
	or.b64  	%rd4, %rd3, 8;
	st.u64 	[%rd4], %rd2;
	st.u64 	[%SP+16], %rd1;
	ld.u64 	%rd5, [%rd4];
	ld.u64 	%rd6, [%SP+16];
	add.u64 	%rd7, %SP, 32;
	or.b64  	%rd8, %rd7, 8;
	st.u64 	[%rd8], %rd5;
	st.u64 	[%SP+32], %rd6;
	ld.u64 	%rd9, [%SP+32];
	setp.ne.s64 	%p1, %rd9, 0;
	@%p1 bra 	$L__BB144_4;
	bra.uni 	$L__BB144_1;
$L__BB144_1:
	add.u64 	%rd14, %SP, 32;
	or.b64  	%rd15, %rd14, 8;
	ld.u64 	%rd16, [%rd15];
	setp.ne.s64 	%p2, %rd16, 0;
	@%p2 bra 	$L__BB144_3;
	bra.uni 	$L__BB144_2;
$L__BB144_2:
	mov.b32 	%r4, 0;
	st.u32 	[%SP+0], %r4;
	bra.uni 	$L__BB144_5;
$L__BB144_3:
	add.u64 	%rd17, %SP, 32;
	or.b64  	%rd18, %rd17, 8;
	ld.u64 	%rd19, [%rd18];
	add.s64 	%rd20, %rd19, -1;
	not.b64 	%rd21, %rd19;
	and.b64  	%rd22, %rd21, %rd20;
	popc.b64 	%r3, %rd22;
	cvt.u64.u32 	%rd23, %r3;
	add.s64 	%rd24, %rd23, 65;
	st.u32 	[%SP+0], %rd24;
	bra.uni 	$L__BB144_5;
$L__BB144_4:
	ld.u64 	%rd10, [%SP+32];
	add.s64 	%rd11, %rd10, -1;
	not.b64 	%rd12, %rd10;
	and.b64  	%rd13, %rd12, %rd11;
	popc.b64 	%r1, %rd13;
	add.s32 	%r2, %r1, 1;
	st.u32 	[%SP+0], %r2;
	bra.uni 	$L__BB144_5;
$L__BB144_5:
	ld.u32 	%r5, [%SP+0];
	st.param.b32 	[func_retval0+0], %r5;
	ret;

}
	// .globl	__lshrdi3
.visible .func  (.param .b64 func_retval0) __lshrdi3(
	.param .b64 __lshrdi3_param_0,
	.param .b32 __lshrdi3_param_1
)
{
	.local .align 8 .b8 	__local_depot145[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<14>;

	mov.u64 	%SPL, __local_depot145;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__lshrdi3_param_1];
	ld.param.u64 	%rd1, [__lshrdi3_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u32 	[%SP+16], %r1;
	mov.b32 	%r2, 32;
	st.u32 	[%SP+20], %r2;
	ld.u64 	%rd2, [%SP+8];
	st.u64 	[%SP+24], %rd2;
	ld.u8 	%rs1, [%SP+16];
	and.b16  	%rs2, %rs1, 32;
	setp.eq.s16 	%p1, %rs2, 0;
	@%p1 bra 	$L__BB145_2;
	bra.uni 	$L__BB145_1;
$L__BB145_1:
	add.u64 	%rd8, %SP, 32;
	or.b64  	%rd9, %rd8, 4;
	mov.b32 	%r15, 0;
	st.u32 	[%rd9], %r15;
	add.u64 	%rd10, %SP, 24;
	or.b64  	%rd11, %rd10, 4;
	ld.u32 	%r16, [%rd11];
	ld.u32 	%r17, [%SP+16];
	add.s32 	%r18, %r17, -32;
	shr.u32 	%r19, %r16, %r18;
	st.u32 	[%SP+32], %r19;
	bra.uni 	$L__BB145_5;
$L__BB145_2:
	ld.u32 	%r3, [%SP+16];
	setp.ne.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB145_4;
	bra.uni 	$L__BB145_3;
$L__BB145_3:
	ld.u64 	%rd7, [%SP+8];
	st.u64 	[%SP+0], %rd7;
	bra.uni 	$L__BB145_6;
$L__BB145_4:
	add.u64 	%rd3, %SP, 24;
	or.b64  	%rd4, %rd3, 4;
	ld.u32 	%r4, [%rd4];
	ld.u32 	%r5, [%SP+16];
	shr.u32 	%r6, %r4, %r5;
	add.u64 	%rd5, %SP, 32;
	or.b64  	%rd6, %rd5, 4;
	st.u32 	[%rd6], %r6;
	ld.u32 	%r7, [%rd4];
	ld.u32 	%r8, [%SP+16];
	mov.b32 	%r9, 32;
	sub.s32 	%r10, %r9, %r8;
	shl.b32 	%r11, %r7, %r10;
	ld.u32 	%r12, [%SP+24];
	shr.u32 	%r13, %r12, %r8;
	or.b32  	%r14, %r11, %r13;
	st.u32 	[%SP+32], %r14;
	bra.uni 	$L__BB145_5;
$L__BB145_5:
	ld.u64 	%rd12, [%SP+32];
	st.u64 	[%SP+0], %rd12;
	bra.uni 	$L__BB145_6;
$L__BB145_6:
	ld.u64 	%rd13, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd13;
	ret;

}
	// .globl	__lshrti3
.visible .func  (.param .align 16 .b8 func_retval0[16]) __lshrti3(
	.param .align 16 .b8 __lshrti3_param_0[16],
	.param .b32 __lshrti3_param_1
)
{
	.local .align 16 .b8 	__local_depot146[80];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<43>;

	mov.u64 	%SPL, __local_depot146;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd1, %rd2}, [__lshrti3_param_0];
	ld.param.u32 	%r1, [__lshrti3_param_1];
	add.u64 	%rd3, %SP, 16;
	or.b64  	%rd4, %rd3, 8;
	st.u64 	[%rd4], %rd2;
	st.u64 	[%SP+16], %rd1;
	st.u32 	[%SP+32], %r1;
	mov.b32 	%r2, 64;
	st.u32 	[%SP+36], %r2;
	ld.u64 	%rd5, [%rd4];
	ld.u64 	%rd6, [%SP+16];
	add.u64 	%rd7, %SP, 48;
	or.b64  	%rd8, %rd7, 8;
	st.u64 	[%rd8], %rd5;
	st.u64 	[%SP+48], %rd6;
	ld.u8 	%rs1, [%SP+32];
	and.b16  	%rs2, %rs1, 64;
	setp.eq.s16 	%p1, %rs2, 0;
	@%p1 bra 	$L__BB146_2;
	bra.uni 	$L__BB146_1;
$L__BB146_1:
	add.u64 	%rd26, %SP, 64;
	or.b64  	%rd27, %rd26, 8;
	mov.u64 	%rd28, 0;
	st.u64 	[%rd27], %rd28;
	add.u64 	%rd29, %SP, 48;
	or.b64  	%rd30, %rd29, 8;
	ld.u64 	%rd31, [%rd30];
	ld.u32 	%r8, [%SP+32];
	add.s32 	%r9, %r8, -64;
	shr.u64 	%rd32, %rd31, %r9;
	st.u64 	[%SP+64], %rd32;
	bra.uni 	$L__BB146_5;
$L__BB146_2:
	ld.u32 	%r3, [%SP+32];
	setp.ne.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB146_4;
	bra.uni 	$L__BB146_3;
$L__BB146_3:
	add.u64 	%rd20, %SP, 16;
	or.b64  	%rd21, %rd20, 8;
	ld.u64 	%rd22, [%rd21];
	ld.u64 	%rd23, [%SP+16];
	add.u64 	%rd24, %SP, 0;
	or.b64  	%rd25, %rd24, 8;
	st.u64 	[%rd25], %rd22;
	st.u64 	[%SP+0], %rd23;
	bra.uni 	$L__BB146_6;
$L__BB146_4:
	add.u64 	%rd9, %SP, 48;
	or.b64  	%rd10, %rd9, 8;
	ld.u64 	%rd11, [%rd10];
	ld.u32 	%r4, [%SP+32];
	shr.u64 	%rd12, %rd11, %r4;
	add.u64 	%rd13, %SP, 64;
	or.b64  	%rd14, %rd13, 8;
	st.u64 	[%rd14], %rd12;
	ld.u64 	%rd15, [%rd10];
	ld.u32 	%r5, [%SP+32];
	mov.b32 	%r6, 64;
	sub.s32 	%r7, %r6, %r5;
	shl.b64 	%rd16, %rd15, %r7;
	ld.u64 	%rd17, [%SP+48];
	shr.u64 	%rd18, %rd17, %r5;
	or.b64  	%rd19, %rd16, %rd18;
	st.u64 	[%SP+64], %rd19;
	bra.uni 	$L__BB146_5;
$L__BB146_5:
	add.u64 	%rd33, %SP, 64;
	or.b64  	%rd34, %rd33, 8;
	ld.u64 	%rd35, [%rd34];
	ld.u64 	%rd36, [%SP+64];
	add.u64 	%rd37, %SP, 0;
	or.b64  	%rd38, %rd37, 8;
	st.u64 	[%rd38], %rd35;
	st.u64 	[%SP+0], %rd36;
	bra.uni 	$L__BB146_6;
$L__BB146_6:
	add.u64 	%rd39, %SP, 0;
	or.b64  	%rd40, %rd39, 8;
	ld.u64 	%rd41, [%rd40];
	ld.u64 	%rd42, [%SP+0];
	st.param.v2.b64 	[func_retval0+0], {%rd42, %rd41};
	ret;

}
	// .globl	__muldsi3
.visible .func  (.param .b64 func_retval0) __muldsi3(
	.param .b32 __muldsi3_param_0,
	.param .b32 __muldsi3_param_1
)
{
	.local .align 8 .b8 	__local_depot147[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<11>;

	mov.u64 	%SPL, __local_depot147;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [__muldsi3_param_1];
	ld.param.u32 	%r1, [__muldsi3_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	mov.b32 	%r3, 16;
	st.u32 	[%SP+16], %r3;
	mov.b32 	%r4, 65535;
	st.u32 	[%SP+20], %r4;
	ld.u16 	%r5, [%SP+0];
	ld.u16 	%r6, [%SP+4];
	mul.lo.s32 	%r7, %r5, %r6;
	st.u32 	[%SP+8], %r7;
	add.u64 	%rd1, %SP, 8;
	or.b64  	%rd2, %rd1, 2;
	ld.u16 	%r8, [%rd2];
	st.u32 	[%SP+24], %r8;
	ld.u16 	%r9, [%SP+8];
	st.u32 	[%SP+8], %r9;
	add.u64 	%rd3, %SP, 0;
	or.b64  	%rd4, %rd3, 2;
	ld.u16 	%r10, [%rd4];
	ld.u16 	%r11, [%SP+4];
	mul.lo.s32 	%r12, %r10, %r11;
	ld.u32 	%r13, [%SP+24];
	add.s32 	%r14, %r13, %r12;
	st.u32 	[%SP+24], %r14;
	ld.u32 	%r15, [%SP+24];
	shl.b32 	%r16, %r15, 16;
	ld.u32 	%r17, [%SP+8];
	add.s32 	%r18, %r17, %r16;
	st.u32 	[%SP+8], %r18;
	add.u64 	%rd5, %SP, 24;
	or.b64  	%rd6, %rd5, 2;
	ld.u16 	%r19, [%rd6];
	or.b64  	%rd7, %rd1, 4;
	st.u32 	[%rd7], %r19;
	ld.u16 	%r20, [%rd2];
	st.u32 	[%SP+24], %r20;
	ld.u16 	%r21, [%SP+8];
	st.u32 	[%SP+8], %r21;
	add.u64 	%rd8, %SP, 4;
	or.b64  	%rd9, %rd8, 2;
	ld.u16 	%r22, [%rd9];
	ld.u16 	%r23, [%SP+0];
	mul.lo.s32 	%r24, %r22, %r23;
	ld.u32 	%r25, [%SP+24];
	add.s32 	%r26, %r25, %r24;
	st.u32 	[%SP+24], %r26;
	ld.u32 	%r27, [%SP+24];
	shl.b32 	%r28, %r27, 16;
	ld.u32 	%r29, [%SP+8];
	add.s32 	%r30, %r29, %r28;
	st.u32 	[%SP+8], %r30;
	ld.u16 	%r31, [%rd6];
	ld.u32 	%r32, [%rd7];
	add.s32 	%r33, %r32, %r31;
	st.u32 	[%rd7], %r33;
	ld.u16 	%r34, [%rd4];
	ld.u16 	%r35, [%rd9];
	mul.lo.s32 	%r36, %r34, %r35;
	ld.u32 	%r37, [%rd7];
	add.s32 	%r38, %r37, %r36;
	st.u32 	[%rd7], %r38;
	ld.u64 	%rd10, [%SP+8];
	st.param.b64 	[func_retval0+0], %rd10;
	ret;

}
	// .globl	__muldi3_compiler_rt
.visible .func  (.param .b64 func_retval0) __muldi3_compiler_rt(
	.param .b64 __muldi3_compiler_rt_param_0,
	.param .b64 __muldi3_compiler_rt_param_1
)
{
	.local .align 8 .b8 	__local_depot148[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<14>;

	mov.u64 	%SPL, __local_depot148;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [__muldi3_compiler_rt_param_1];
	ld.param.u64 	%rd1, [__muldi3_compiler_rt_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd4, [%SP+8];
	st.u64 	[%SP+24], %rd4;
	ld.u32 	%r1, [%SP+16];
	ld.u32 	%r2, [%SP+24];
	{ // callseq 16, 0
	.param .b32 param0;
	st.param.b32 	[param0+0], %r1;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r2;
	.param .b64 retval0;
	call.uni (retval0), 
	__muldsi3, 
	(
	param0, 
	param1
	);
	ld.param.b64 	%rd5, [retval0+0];
	} // callseq 16
	st.u64 	[%SP+32], %rd5;
	add.u64 	%rd7, %SP, 16;
	or.b64  	%rd8, %rd7, 4;
	ld.u32 	%r3, [%rd8];
	ld.u32 	%r4, [%SP+24];
	mul.lo.s32 	%r5, %r3, %r4;
	ld.u32 	%r6, [%SP+16];
	add.u64 	%rd9, %SP, 24;
	or.b64  	%rd10, %rd9, 4;
	ld.u32 	%r7, [%rd10];
	mul.lo.s32 	%r8, %r6, %r7;
	add.s32 	%r9, %r5, %r8;
	add.u64 	%rd11, %SP, 32;
	or.b64  	%rd12, %rd11, 4;
	ld.u32 	%r10, [%rd12];
	add.s32 	%r11, %r10, %r9;
	st.u32 	[%rd12], %r11;
	ld.u64 	%rd13, [%SP+32];
	st.param.b64 	[func_retval0+0], %rd13;
	ret;

}
	// .globl	__mulddi3
.visible .func  (.param .align 16 .b8 func_retval0[16]) __mulddi3(
	.param .b64 __mulddi3_param_0,
	.param .b64 __mulddi3_param_1
)
{
	.local .align 16 .b8 	__local_depot149[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<49>;

	mov.u64 	%SPL, __local_depot149;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [__mulddi3_param_1];
	ld.param.u64 	%rd1, [__mulddi3_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	mov.b32 	%r1, 32;
	st.u32 	[%SP+32], %r1;
	mov.u64 	%rd3, 4294967295;
	st.u64 	[%SP+40], %rd3;
	ld.u32 	%rd4, [%SP+0];
	ld.u32 	%rd5, [%SP+8];
	mul.lo.s64 	%rd6, %rd4, %rd5;
	st.u64 	[%SP+16], %rd6;
	add.u64 	%rd7, %SP, 16;
	or.b64  	%rd8, %rd7, 4;
	ld.u32 	%rd9, [%rd8];
	st.u64 	[%SP+48], %rd9;
	ld.u32 	%rd10, [%SP+16];
	st.u64 	[%SP+16], %rd10;
	add.u64 	%rd11, %SP, 0;
	or.b64  	%rd12, %rd11, 4;
	ld.u32 	%rd13, [%rd12];
	ld.u32 	%rd14, [%SP+8];
	mul.lo.s64 	%rd15, %rd13, %rd14;
	ld.u64 	%rd16, [%SP+48];
	add.s64 	%rd17, %rd16, %rd15;
	st.u64 	[%SP+48], %rd17;
	ld.u64 	%rd18, [%SP+48];
	shl.b64 	%rd19, %rd18, 32;
	ld.u64 	%rd20, [%SP+16];
	add.s64 	%rd21, %rd20, %rd19;
	st.u64 	[%SP+16], %rd21;
	add.u64 	%rd22, %SP, 48;
	or.b64  	%rd23, %rd22, 4;
	ld.u32 	%rd24, [%rd23];
	or.b64  	%rd25, %rd7, 8;
	st.u64 	[%rd25], %rd24;
	ld.u32 	%rd26, [%rd8];
	st.u64 	[%SP+48], %rd26;
	ld.u32 	%rd27, [%SP+16];
	st.u64 	[%SP+16], %rd27;
	add.u64 	%rd28, %SP, 8;
	or.b64  	%rd29, %rd28, 4;
	ld.u32 	%rd30, [%rd29];
	ld.u32 	%rd31, [%SP+0];
	mul.lo.s64 	%rd32, %rd30, %rd31;
	ld.u64 	%rd33, [%SP+48];
	add.s64 	%rd34, %rd33, %rd32;
	st.u64 	[%SP+48], %rd34;
	ld.u64 	%rd35, [%SP+48];
	shl.b64 	%rd36, %rd35, 32;
	ld.u64 	%rd37, [%SP+16];
	add.s64 	%rd38, %rd37, %rd36;
	st.u64 	[%SP+16], %rd38;
	ld.u32 	%rd39, [%rd23];
	ld.u64 	%rd40, [%rd25];
	add.s64 	%rd41, %rd40, %rd39;
	st.u64 	[%rd25], %rd41;
	ld.u32 	%rd42, [%rd12];
	ld.u32 	%rd43, [%rd29];
	mul.lo.s64 	%rd44, %rd42, %rd43;
	ld.u64 	%rd45, [%rd25];
	add.s64 	%rd46, %rd45, %rd44;
	st.u64 	[%rd25], %rd46;
	ld.u64 	%rd47, [%rd25];
	ld.u64 	%rd48, [%SP+16];
	st.param.v2.b64 	[func_retval0+0], {%rd48, %rd47};
	ret;

}
	// .globl	__multi3
.visible .func  (.param .align 16 .b8 func_retval0[16]) __multi3(
	.param .align 16 .b8 __multi3_param_0[16],
	.param .align 16 .b8 __multi3_param_1[16]
)
{
	.local .align 16 .b8 	__local_depot150[80];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<36>;

	mov.u64 	%SPL, __local_depot150;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd3, %rd4}, [__multi3_param_1];
	ld.param.v2.u64 	{%rd1, %rd2}, [__multi3_param_0];
	add.u64 	%rd5, %SP, 0;
	or.b64  	%rd6, %rd5, 8;
	st.u64 	[%rd6], %rd2;
	st.u64 	[%SP+0], %rd1;
	add.u64 	%rd7, %SP, 16;
	or.b64  	%rd8, %rd7, 8;
	st.u64 	[%rd8], %rd4;
	st.u64 	[%SP+16], %rd3;
	ld.u64 	%rd9, [%rd6];
	ld.u64 	%rd10, [%SP+0];
	add.u64 	%rd11, %SP, 32;
	or.b64  	%rd12, %rd11, 8;
	st.u64 	[%rd12], %rd9;
	st.u64 	[%SP+32], %rd10;
	ld.u64 	%rd13, [%rd8];
	ld.u64 	%rd14, [%SP+16];
	add.u64 	%rd15, %SP, 48;
	or.b64  	%rd16, %rd15, 8;
	st.u64 	[%rd16], %rd13;
	st.u64 	[%SP+48], %rd14;
	ld.u64 	%rd17, [%SP+32];
	ld.u64 	%rd18, [%SP+48];
	{ // callseq 17, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd17;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd18;
	.param .align 16 .b8 retval0[16];
	call.uni (retval0), 
	__mulddi3, 
	(
	param0, 
	param1
	);
	ld.param.v2.b64 	{%rd19, %rd20}, [retval0+0];
	} // callseq 17
	add.u64 	%rd23, %SP, 64;
	or.b64  	%rd24, %rd23, 8;
	st.u64 	[%rd24], %rd20;
	st.u64 	[%SP+64], %rd19;
	ld.u64 	%rd25, [%rd12];
	ld.u64 	%rd26, [%SP+48];
	mul.lo.s64 	%rd27, %rd25, %rd26;
	ld.u64 	%rd28, [%SP+32];
	ld.u64 	%rd29, [%rd16];
	mul.lo.s64 	%rd30, %rd28, %rd29;
	add.s64 	%rd31, %rd27, %rd30;
	ld.u64 	%rd32, [%rd24];
	add.s64 	%rd33, %rd32, %rd31;
	st.u64 	[%rd24], %rd33;
	ld.u64 	%rd34, [%rd24];
	ld.u64 	%rd35, [%SP+64];
	st.param.v2.b64 	[func_retval0+0], {%rd35, %rd34};
	ret;

}
	// .globl	__negdi2
.visible .func  (.param .b64 func_retval0) __negdi2(
	.param .b64 __negdi2_param_0
)
{
	.local .align 8 .b8 	__local_depot151[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<4>;

	mov.u64 	%SPL, __local_depot151;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [__negdi2_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	neg.s64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .globl	__negti2
.visible .func  (.param .align 16 .b8 func_retval0[16]) __negti2(
	.param .align 16 .b8 __negti2_param_0[16]
)
{
	.local .align 16 .b8 	__local_depot152[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b64 	%rd<11>;

	mov.u64 	%SPL, __local_depot152;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd1, %rd2}, [__negti2_param_0];
	add.u64 	%rd3, %SP, 0;
	or.b64  	%rd4, %rd3, 8;
	st.u64 	[%rd4], %rd2;
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd5, [%rd4];
	ld.u64 	%rd6, [%SP+0];
	setp.ne.s64 	%p1, %rd6, 0;
	selp.u64 	%rd7, 1, 0, %p1;
	add.s64 	%rd8, %rd5, %rd7;
	neg.s64 	%rd9, %rd8;
	neg.s64 	%rd10, %rd6;
	st.param.v2.b64 	[func_retval0+0], {%rd10, %rd9};
	ret;

}
	// .globl	__paritydi2
.visible .func  (.param .b32 func_retval0) __paritydi2(
	.param .b64 __paritydi2_param_0
)
{
	.local .align 8 .b8 	__local_depot153[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<5>;

	mov.u64 	%SPL, __local_depot153;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [__paritydi2_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	st.u64 	[%SP+8], %rd2;
	add.u64 	%rd3, %SP, 8;
	or.b64  	%rd4, %rd3, 4;
	ld.u32 	%r1, [%rd4];
	ld.u32 	%r2, [%SP+8];
	xor.b32  	%r3, %r1, %r2;
	st.u32 	[%SP+16], %r3;
	ld.u32 	%r4, [%SP+16];
	shr.u32 	%r5, %r4, 16;
	xor.b32  	%r6, %r4, %r5;
	st.u32 	[%SP+16], %r6;
	ld.u32 	%r7, [%SP+16];
	shr.u32 	%r8, %r7, 8;
	xor.b32  	%r9, %r7, %r8;
	st.u32 	[%SP+16], %r9;
	ld.u32 	%r10, [%SP+16];
	shr.u32 	%r11, %r10, 4;
	xor.b32  	%r12, %r10, %r11;
	st.u32 	[%SP+16], %r12;
	ld.u32 	%r13, [%SP+16];
	and.b32  	%r14, %r13, 15;
	mov.b32 	%r15, 27030;
	shr.u32 	%r16, %r15, %r14;
	and.b32  	%r17, %r16, 1;
	st.param.b32 	[func_retval0+0], %r17;
	ret;

}
	// .globl	__parityti2
.visible .func  (.param .b32 func_retval0) __parityti2(
	.param .align 16 .b8 __parityti2_param_0[16]
)
{
	.local .align 16 .b8 	__local_depot154[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<14>;

	mov.u64 	%SPL, __local_depot154;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd1, %rd2}, [__parityti2_param_0];
	add.u64 	%rd3, %SP, 0;
	or.b64  	%rd4, %rd3, 8;
	st.u64 	[%rd4], %rd2;
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd5, [%rd4];
	ld.u64 	%rd6, [%SP+0];
	add.u64 	%rd7, %SP, 16;
	or.b64  	%rd8, %rd7, 8;
	st.u64 	[%rd8], %rd5;
	st.u64 	[%SP+16], %rd6;
	ld.u64 	%rd9, [%rd8];
	ld.u64 	%rd10, [%SP+16];
	xor.b64  	%rd11, %rd9, %rd10;
	st.u64 	[%SP+32], %rd11;
	add.u64 	%rd12, %SP, 32;
	or.b64  	%rd13, %rd12, 4;
	ld.u32 	%r1, [%rd13];
	ld.u32 	%r2, [%SP+32];
	xor.b32  	%r3, %r1, %r2;
	st.u32 	[%SP+40], %r3;
	ld.u32 	%r4, [%SP+40];
	shr.u32 	%r5, %r4, 16;
	xor.b32  	%r6, %r4, %r5;
	st.u32 	[%SP+40], %r6;
	ld.u32 	%r7, [%SP+40];
	shr.u32 	%r8, %r7, 8;
	xor.b32  	%r9, %r7, %r8;
	st.u32 	[%SP+40], %r9;
	ld.u32 	%r10, [%SP+40];
	shr.u32 	%r11, %r10, 4;
	xor.b32  	%r12, %r10, %r11;
	st.u32 	[%SP+40], %r12;
	ld.u32 	%r13, [%SP+40];
	and.b32  	%r14, %r13, 15;
	mov.b32 	%r15, 27030;
	shr.u32 	%r16, %r15, %r14;
	and.b32  	%r17, %r16, 1;
	st.param.b32 	[func_retval0+0], %r17;
	ret;

}
	// .globl	__paritysi2
.visible .func  (.param .b32 func_retval0) __paritysi2(
	.param .b32 __paritysi2_param_0
)
{
	.local .align 4 .b8 	__local_depot155[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<17>;

	mov.u64 	%SPL, __local_depot155;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__paritysi2_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+4];
	shr.u32 	%r4, %r3, 16;
	xor.b32  	%r5, %r3, %r4;
	st.u32 	[%SP+4], %r5;
	ld.u32 	%r6, [%SP+4];
	shr.u32 	%r7, %r6, 8;
	xor.b32  	%r8, %r6, %r7;
	st.u32 	[%SP+4], %r8;
	ld.u32 	%r9, [%SP+4];
	shr.u32 	%r10, %r9, 4;
	xor.b32  	%r11, %r9, %r10;
	st.u32 	[%SP+4], %r11;
	ld.u32 	%r12, [%SP+4];
	and.b32  	%r13, %r12, 15;
	mov.b32 	%r14, 27030;
	shr.u32 	%r15, %r14, %r13;
	and.b32  	%r16, %r15, 1;
	st.param.b32 	[func_retval0+0], %r16;
	ret;

}
	// .globl	__popcountdi2
.visible .func  (.param .b32 func_retval0) __popcountdi2(
	.param .b64 __popcountdi2_param_0
)
{
	.local .align 8 .b8 	__local_depot156[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<19>;

	mov.u64 	%SPL, __local_depot156;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [__popcountdi2_param_0];
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd2, [%SP+0];
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+8];
	shr.u64 	%rd4, %rd3, 1;
	and.b64  	%rd5, %rd4, 6148914691236517205;
	sub.s64 	%rd6, %rd3, %rd5;
	st.u64 	[%SP+8], %rd6;
	ld.u64 	%rd7, [%SP+8];
	shr.u64 	%rd8, %rd7, 2;
	and.b64  	%rd9, %rd8, 3689348814741910323;
	and.b64  	%rd10, %rd7, 3689348814741910323;
	add.s64 	%rd11, %rd9, %rd10;
	st.u64 	[%SP+8], %rd11;
	ld.u64 	%rd12, [%SP+8];
	shr.u64 	%rd13, %rd12, 4;
	add.s64 	%rd14, %rd12, %rd13;
	and.b64  	%rd15, %rd14, 1085102592571150095;
	st.u64 	[%SP+8], %rd15;
	ld.u64 	%rd16, [%SP+8];
	shr.u64 	%rd17, %rd16, 32;
	add.s64 	%rd18, %rd16, %rd17;
	st.u32 	[%SP+16], %rd18;
	ld.u32 	%r1, [%SP+16];
	shr.u32 	%r2, %r1, 16;
	add.s32 	%r3, %r1, %r2;
	st.u32 	[%SP+16], %r3;
	ld.u32 	%r4, [%SP+16];
	shr.u32 	%r5, %r4, 8;
	add.s32 	%r6, %r4, %r5;
	and.b32  	%r7, %r6, 127;
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	__popcountsi2
.visible .func  (.param .b32 func_retval0) __popcountsi2(
	.param .b32 __popcountsi2_param_0
)
{
	.local .align 4 .b8 	__local_depot157[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<23>;

	mov.u64 	%SPL, __local_depot157;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__popcountsi2_param_0];
	st.u32 	[%SP+0], %r1;
	ld.u32 	%r2, [%SP+0];
	st.u32 	[%SP+4], %r2;
	ld.u32 	%r3, [%SP+4];
	shr.u32 	%r4, %r3, 1;
	and.b32  	%r5, %r4, 1431655765;
	sub.s32 	%r6, %r3, %r5;
	st.u32 	[%SP+4], %r6;
	ld.u32 	%r7, [%SP+4];
	shr.u32 	%r8, %r7, 2;
	and.b32  	%r9, %r8, 858993459;
	and.b32  	%r10, %r7, 858993459;
	add.s32 	%r11, %r9, %r10;
	st.u32 	[%SP+4], %r11;
	ld.u32 	%r12, [%SP+4];
	shr.u32 	%r13, %r12, 4;
	add.s32 	%r14, %r12, %r13;
	and.b32  	%r15, %r14, 252645135;
	st.u32 	[%SP+4], %r15;
	ld.u32 	%r16, [%SP+4];
	shr.u32 	%r17, %r16, 16;
	add.s32 	%r18, %r16, %r17;
	st.u32 	[%SP+4], %r18;
	ld.u32 	%r19, [%SP+4];
	shr.u32 	%r20, %r19, 8;
	add.s32 	%r21, %r19, %r20;
	and.b32  	%r22, %r21, 63;
	st.param.b32 	[func_retval0+0], %r22;
	ret;

}
	// .globl	__popcountti2
.visible .func  (.param .b32 func_retval0) __popcountti2(
	.param .align 16 .b8 __popcountti2_param_0[16]
)
{
	.local .align 16 .b8 	__local_depot158[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<49>;

	mov.u64 	%SPL, __local_depot158;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd1, %rd2}, [__popcountti2_param_0];
	add.u64 	%rd3, %SP, 0;
	or.b64  	%rd4, %rd3, 8;
	st.u64 	[%rd4], %rd2;
	st.u64 	[%SP+0], %rd1;
	ld.u64 	%rd5, [%rd4];
	ld.u64 	%rd6, [%SP+0];
	add.u64 	%rd7, %SP, 16;
	or.b64  	%rd8, %rd7, 8;
	st.u64 	[%rd8], %rd5;
	st.u64 	[%SP+16], %rd6;
	ld.u64 	%rd9, [%rd8];
	ld.u64 	%rd10, [%SP+16];
	shr.u64 	%rd11, %rd9, 1;
	shr.u64 	%rd12, %rd10, 1;
	and.b64  	%rd13, %rd12, 6148914691236517205;
	and.b64  	%rd14, %rd11, 6148914691236517205;
	sub.s64 	%rd15, %rd9, %rd14;
	setp.lt.u64 	%p1, %rd10, %rd13;
	selp.s64 	%rd16, -1, 0, %p1;
	add.s64 	%rd17, %rd15, %rd16;
	sub.s64 	%rd18, %rd10, %rd13;
	st.u64 	[%rd8], %rd17;
	st.u64 	[%SP+16], %rd18;
	ld.u64 	%rd19, [%rd8];
	ld.u64 	%rd20, [%SP+16];
	shr.u64 	%rd21, %rd19, 2;
	shr.u64 	%rd22, %rd20, 2;
	and.b64  	%rd23, %rd22, 3689348814741910323;
	and.b64  	%rd24, %rd21, 3689348814741910323;
	and.b64  	%rd25, %rd20, 3689348814741910323;
	and.b64  	%rd26, %rd19, 3689348814741910323;
	add.s64 	%rd27, %rd24, %rd26;
	add.s64 	%rd28, %rd23, %rd25;
	setp.lt.u64 	%p2, %rd28, %rd23;
	selp.u64 	%rd29, 1, 0, %p2;
	add.s64 	%rd30, %rd27, %rd29;
	st.u64 	[%rd8], %rd30;
	st.u64 	[%SP+16], %rd28;
	ld.u64 	%rd31, [%rd8];
	ld.u64 	%rd32, [%SP+16];
	shl.b64 	%rd33, %rd31, 60;
	shr.u64 	%rd34, %rd32, 4;
	or.b64  	%rd35, %rd34, %rd33;
	shr.u64 	%rd36, %rd31, 4;
	add.s64 	%rd37, %rd31, %rd36;
	add.s64 	%rd38, %rd32, %rd35;
	setp.lt.u64 	%p3, %rd38, %rd32;
	selp.u64 	%rd39, 1, 0, %p3;
	add.s64 	%rd40, %rd37, %rd39;
	and.b64  	%rd41, %rd40, 1085102592571150095;
	and.b64  	%rd42, %rd38, 1085102592571150095;
	st.u64 	[%rd8], %rd41;
	st.u64 	[%SP+16], %rd42;
	ld.u64 	%rd43, [%rd8];
	ld.u64 	%rd44, [%SP+16];
	add.s64 	%rd45, %rd44, %rd43;
	st.u64 	[%SP+32], %rd45;
	ld.u64 	%rd46, [%SP+32];
	shr.u64 	%rd47, %rd46, 32;
	add.s64 	%rd48, %rd46, %rd47;
	st.u32 	[%SP+40], %rd48;
	ld.u32 	%r1, [%SP+40];
	shr.u32 	%r2, %r1, 16;
	add.s32 	%r3, %r1, %r2;
	st.u32 	[%SP+40], %r3;
	ld.u32 	%r4, [%SP+40];
	shr.u32 	%r5, %r4, 8;
	add.s32 	%r6, %r4, %r5;
	and.b32  	%r7, %r6, 255;
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	__powidf2
.visible .func  (.param .b64 func_retval0) __powidf2(
	.param .b64 __powidf2_param_0,
	.param .b32 __powidf2_param_1
)
{
	.local .align 8 .b8 	__local_depot159[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<2>;
	.reg .f64 	%fd<12>;

	mov.u64 	%SPL, __local_depot159;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__powidf2_param_1];
	ld.param.f64 	%fd4, [__powidf2_param_0];
	st.f64 	[%SP+0], %fd4;
	st.u32 	[%SP+8], %r1;
	ld.u32 	%r2, [%SP+8];
	shr.u32 	%r3, %r2, 31;
	st.u32 	[%SP+12], %r3;
	mov.u64 	%rd1, 4607182418800017408;
	st.u64 	[%SP+16], %rd1;
	bra.uni 	$L__BB159_1;
$L__BB159_1:
	ld.u32 	%r4, [%SP+8];
	and.b32  	%r5, %r4, 1;
	setp.eq.b32 	%p1, %r5, 1;
	mov.pred 	%p2, 0;
	xor.pred  	%p3, %p1, %p2;
	not.pred 	%p4, %p3;
	@%p4 bra 	$L__BB159_3;
	bra.uni 	$L__BB159_2;
$L__BB159_2:
	ld.f64 	%fd5, [%SP+0];
	ld.f64 	%fd6, [%SP+16];
	mul.rn.f64 	%fd7, %fd6, %fd5;
	st.f64 	[%SP+16], %fd7;
	bra.uni 	$L__BB159_3;
$L__BB159_3:
	ld.u32 	%r6, [%SP+8];
	shr.u32 	%r7, %r6, 31;
	add.s32 	%r8, %r6, %r7;
	shr.s32 	%r9, %r8, 1;
	st.u32 	[%SP+8], %r9;
	ld.u32 	%r10, [%SP+8];
	setp.ne.s32 	%p5, %r10, 0;
	@%p5 bra 	$L__BB159_5;
	bra.uni 	$L__BB159_4;
$L__BB159_4:
	bra.uni 	$L__BB159_6;
$L__BB159_5:
	ld.f64 	%fd8, [%SP+0];
	mul.rn.f64 	%fd9, %fd8, %fd8;
	st.f64 	[%SP+0], %fd9;
	bra.uni 	$L__BB159_1;
$L__BB159_6:
	ld.u32 	%r11, [%SP+12];
	setp.eq.s32 	%p6, %r11, 0;
	@%p6 bra 	$L__BB159_8;
	bra.uni 	$L__BB159_7;
$L__BB159_7:
	ld.f64 	%fd10, [%SP+16];
	rcp.rn.f64 	%fd1, %fd10;
	mov.f64 	%fd11, %fd1;
	bra.uni 	$L__BB159_9;
$L__BB159_8:
	ld.f64 	%fd2, [%SP+16];
	mov.f64 	%fd11, %fd2;
	bra.uni 	$L__BB159_9;
$L__BB159_9:
	mov.f64 	%fd3, %fd11;
	st.param.f64 	[func_retval0+0], %fd3;
	ret;

}
	// .globl	__powisf2
.visible .func  (.param .b32 func_retval0) __powisf2(
	.param .b32 __powisf2_param_0,
	.param .b32 __powisf2_param_1
)
{
	.local .align 4 .b8 	__local_depot160[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<12>;

	mov.u64 	%SPL, __local_depot160;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [__powisf2_param_1];
	ld.param.f32 	%f4, [__powisf2_param_0];
	st.f32 	[%SP+0], %f4;
	st.u32 	[%SP+4], %r1;
	ld.u32 	%r2, [%SP+4];
	shr.u32 	%r3, %r2, 31;
	st.u32 	[%SP+8], %r3;
	mov.b32 	%r4, 1065353216;
	st.u32 	[%SP+12], %r4;
	bra.uni 	$L__BB160_1;
$L__BB160_1:
	ld.u32 	%r5, [%SP+4];
	and.b32  	%r6, %r5, 1;
	setp.eq.b32 	%p1, %r6, 1;
	mov.pred 	%p2, 0;
	xor.pred  	%p3, %p1, %p2;
	not.pred 	%p4, %p3;
	@%p4 bra 	$L__BB160_3;
	bra.uni 	$L__BB160_2;
$L__BB160_2:
	ld.f32 	%f5, [%SP+0];
	ld.f32 	%f6, [%SP+12];
	mul.rn.f32 	%f7, %f6, %f5;
	st.f32 	[%SP+12], %f7;
	bra.uni 	$L__BB160_3;
$L__BB160_3:
	ld.u32 	%r7, [%SP+4];
	shr.u32 	%r8, %r7, 31;
	add.s32 	%r9, %r7, %r8;
	shr.s32 	%r10, %r9, 1;
	st.u32 	[%SP+4], %r10;
	ld.u32 	%r11, [%SP+4];
	setp.ne.s32 	%p5, %r11, 0;
	@%p5 bra 	$L__BB160_5;
	bra.uni 	$L__BB160_4;
$L__BB160_4:
	bra.uni 	$L__BB160_6;
$L__BB160_5:
	ld.f32 	%f8, [%SP+0];
	mul.rn.f32 	%f9, %f8, %f8;
	st.f32 	[%SP+0], %f9;
	bra.uni 	$L__BB160_1;
$L__BB160_6:
	ld.u32 	%r12, [%SP+8];
	setp.eq.s32 	%p6, %r12, 0;
	@%p6 bra 	$L__BB160_8;
	bra.uni 	$L__BB160_7;
$L__BB160_7:
	ld.f32 	%f10, [%SP+12];
	rcp.rn.f32 	%f1, %f10;
	mov.f32 	%f11, %f1;
	bra.uni 	$L__BB160_9;
$L__BB160_8:
	ld.f32 	%f2, [%SP+12];
	mov.f32 	%f11, %f2;
	bra.uni 	$L__BB160_9;
$L__BB160_9:
	mov.f32 	%f3, %f11;
	st.param.f32 	[func_retval0+0], %f3;
	ret;

}
	// .globl	__ucmpdi2
.visible .func  (.param .b32 func_retval0) __ucmpdi2(
	.param .b64 __ucmpdi2_param_0,
	.param .b64 __ucmpdi2_param_1
)
{
	.local .align 8 .b8 	__local_depot161[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<13>;

	mov.u64 	%SPL, __local_depot161;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [__ucmpdi2_param_1];
	ld.param.u64 	%rd1, [__ucmpdi2_param_0];
	st.u64 	[%SP+8], %rd1;
	st.u64 	[%SP+16], %rd2;
	ld.u64 	%rd3, [%SP+8];
	st.u64 	[%SP+24], %rd3;
	ld.u64 	%rd4, [%SP+16];
	st.u64 	[%SP+32], %rd4;
	add.u64 	%rd5, %SP, 24;
	or.b64  	%rd6, %rd5, 4;
	ld.u32 	%r1, [%rd6];
	add.u64 	%rd7, %SP, 32;
	or.b64  	%rd8, %rd7, 4;
	ld.u32 	%r2, [%rd8];
	setp.ge.u32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB161_2;
	bra.uni 	$L__BB161_1;
$L__BB161_1:
	mov.b32 	%r13, 0;
	st.u32 	[%SP+0], %r13;
	bra.uni 	$L__BB161_9;
$L__BB161_2:
	add.u64 	%rd9, %SP, 24;
	or.b64  	%rd10, %rd9, 4;
	ld.u32 	%r3, [%rd10];
	add.u64 	%rd11, %SP, 32;
	or.b64  	%rd12, %rd11, 4;
	ld.u32 	%r4, [%rd12];
	setp.le.u32 	%p2, %r3, %r4;
	@%p2 bra 	$L__BB161_4;
	bra.uni 	$L__BB161_3;
$L__BB161_3:
	mov.b32 	%r12, 2;
	st.u32 	[%SP+0], %r12;
	bra.uni 	$L__BB161_9;
$L__BB161_4:
	ld.u32 	%r5, [%SP+24];
	ld.u32 	%r6, [%SP+32];
	setp.ge.u32 	%p3, %r5, %r6;
	@%p3 bra 	$L__BB161_6;
	bra.uni 	$L__BB161_5;
$L__BB161_5:
	mov.b32 	%r11, 0;
	st.u32 	[%SP+0], %r11;
	bra.uni 	$L__BB161_9;
$L__BB161_6:
	ld.u32 	%r7, [%SP+24];
	ld.u32 	%r8, [%SP+32];
	setp.le.u32 	%p4, %r7, %r8;
	@%p4 bra 	$L__BB161_8;
	bra.uni 	$L__BB161_7;
$L__BB161_7:
	mov.b32 	%r10, 2;
	st.u32 	[%SP+0], %r10;
	bra.uni 	$L__BB161_9;
$L__BB161_8:
	mov.b32 	%r9, 1;
	st.u32 	[%SP+0], %r9;
	bra.uni 	$L__BB161_9;
$L__BB161_9:
	ld.u32 	%r14, [%SP+0];
	st.param.b32 	[func_retval0+0], %r14;
	ret;

}
	// .globl	__aeabi_ulcmp
.visible .func  (.param .b32 func_retval0) __aeabi_ulcmp(
	.param .b64 __aeabi_ulcmp_param_0,
	.param .b64 __aeabi_ulcmp_param_1
)
{
	.local .align 8 .b8 	__local_depot162[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<5>;

	mov.u64 	%SPL, __local_depot162;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [__aeabi_ulcmp_param_1];
	ld.param.u64 	%rd1, [__aeabi_ulcmp_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	ld.u64 	%rd3, [%SP+0];
	ld.u64 	%rd4, [%SP+8];
	{ // callseq 18, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd3;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd4;
	.param .b32 retval0;
	call.uni (retval0), 
	__ucmpdi2, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1, [retval0+0];
	} // callseq 18
	add.s32 	%r3, %r1, -1;
	st.param.b32 	[func_retval0+0], %r3;
	ret;

}
	// .globl	__ucmpti2
.visible .func  (.param .b32 func_retval0) __ucmpti2(
	.param .align 16 .b8 __ucmpti2_param_0[16],
	.param .align 16 .b8 __ucmpti2_param_1[16]
)
{
	.local .align 16 .b8 	__local_depot163[80];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<29>;

	mov.u64 	%SPL, __local_depot163;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u64 	{%rd3, %rd4}, [__ucmpti2_param_1];
	ld.param.v2.u64 	{%rd1, %rd2}, [__ucmpti2_param_0];
	add.u64 	%rd5, %SP, 16;
	or.b64  	%rd6, %rd5, 8;
	st.u64 	[%rd6], %rd2;
	st.u64 	[%SP+16], %rd1;
	add.u64 	%rd7, %SP, 32;
	or.b64  	%rd8, %rd7, 8;
	st.u64 	[%rd8], %rd4;
	st.u64 	[%SP+32], %rd3;
	ld.u64 	%rd9, [%rd6];
	ld.u64 	%rd10, [%SP+16];
	add.u64 	%rd11, %SP, 48;
	or.b64  	%rd12, %rd11, 8;
	st.u64 	[%rd12], %rd9;
	st.u64 	[%SP+48], %rd10;
	ld.u64 	%rd13, [%rd8];
	ld.u64 	%rd14, [%SP+32];
	add.u64 	%rd15, %SP, 64;
	or.b64  	%rd16, %rd15, 8;
	st.u64 	[%rd16], %rd13;
	st.u64 	[%SP+64], %rd14;
	ld.u64 	%rd17, [%rd12];
	ld.u64 	%rd18, [%rd16];
	setp.ge.u64 	%p1, %rd17, %rd18;
	@%p1 bra 	$L__BB163_2;
	bra.uni 	$L__BB163_1;
$L__BB163_1:
	mov.b32 	%r5, 0;
	st.u32 	[%SP+0], %r5;
	bra.uni 	$L__BB163_9;
$L__BB163_2:
	add.u64 	%rd19, %SP, 48;
	or.b64  	%rd20, %rd19, 8;
	ld.u64 	%rd21, [%rd20];
	add.u64 	%rd22, %SP, 64;
	or.b64  	%rd23, %rd22, 8;
	ld.u64 	%rd24, [%rd23];
	setp.le.u64 	%p2, %rd21, %rd24;
	@%p2 bra 	$L__BB163_4;
	bra.uni 	$L__BB163_3;
$L__BB163_3:
	mov.b32 	%r4, 2;
	st.u32 	[%SP+0], %r4;
	bra.uni 	$L__BB163_9;
$L__BB163_4:
	ld.u64 	%rd25, [%SP+48];
	ld.u64 	%rd26, [%SP+64];
	setp.ge.u64 	%p3, %rd25, %rd26;
	@%p3 bra 	$L__BB163_6;
	bra.uni 	$L__BB163_5;
$L__BB163_5:
	mov.b32 	%r3, 0;
	st.u32 	[%SP+0], %r3;
	bra.uni 	$L__BB163_9;
$L__BB163_6:
	ld.u64 	%rd27, [%SP+48];
	ld.u64 	%rd28, [%SP+64];
	setp.le.u64 	%p4, %rd27, %rd28;
	@%p4 bra 	$L__BB163_8;
	bra.uni 	$L__BB163_7;
$L__BB163_7:
	mov.b32 	%r2, 2;
	st.u32 	[%SP+0], %r2;
	bra.uni 	$L__BB163_9;
$L__BB163_8:
	mov.b32 	%r1, 1;
	st.u32 	[%SP+0], %r1;
	bra.uni 	$L__BB163_9;
$L__BB163_9:
	ld.u32 	%r6, [%SP+0];
	st.param.b32 	[func_retval0+0], %r6;
	ret;

}
